# AWS Certified DevOps Engineer - Professional DOP-C02

Total Questions: 390

---

## Question 1

*Date: April 5, 2023, 1:22 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has a mobile application that makes HTTP API calls to an Application Load Balancer (ALB). The ALB routes requests to an AWS Lambda function. Many different versions of the application are in use at any given time, including versions that are in testing by a subset of users. The version of the application is defined in the user-agent header that is sent with all requests to the API.
After a series of recent changes to the API, the company has observed issues with the application. The company needs to gather a metric for each API operation by response code for each version of the application that is in use. A DevOps engineer has modified the Lambda function to extract the API operation name, version information from the user-agent header and response code.
Which additional set of actions should the DevOps engineer take to gather the required metrics?

**Options:**
- A. Modify the Lambda function to write the API operation name, response code, and version number as a log line to an Amazon CloudWatch Logs log group. Configure a CloudWatch Logs metric filter that increments a metric for each API operation name. Specify response code and application version as dimensions for the metric.
- B. Modify the Lambda function to write the API operation name, response code, and version number as a log line to an Amazon CloudWatch Logs log group. Configure a CloudWatch Logs Insights query to populate CloudWatch metrics from the log lines. Specify response code and application version as dimensions for the metric.
- C. Configure the ALB access logs to write to an Amazon CloudWatch Logs log group. Modify the Lambda function to respond to the ALB with the API operation name, response code, and version number as response metadata. Configure a CloudWatch Logs metric filter that increments a metric for each API operation name. Specify response code and application version as dimensions for the metric.
- D. Configure AWS X-Ray integration on the Lambda function. Modify the Lambda function to create an X-Ray subsegment with the API operation name, response code, and version number. Configure X-Ray insights to extract an aggregated metric for each API operation name and to publish the metric to Amazon CloudWatch. Specify response code and application version as dimensions for the metric.

> **Suggested Answer:** A
> **Community Vote:** A (87%), 8%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**c3518fc** (Sun 22 Sep 2024 14:59) - *Upvotes: 9*
The other options are either incomplete or involve unnecessary complexity:

Option B requires using CloudWatch Logs Insights queries, which may introduce additional complexity and potential performance overhead.
Option C involves modifying the ALB access logs, which may not provide the required level of granularity or flexibility for capturing the application version information.
Option D requires integrating AWS X-Ray, which is primarily designed for distributed tracing and may not be necessary for this specific use case of gathering metrics by response code and application version.

Therefore, option A is the most appropriate and straightforward solution for the given requirements.

---

**serkikikko123** (Tue 02 Sep 2025 08:07) - *Upvotes: 1*
see full details about the dop c02 exam Ref - https://lc.cx/BXle6s

---

**BaburTurk** (Thu 31 Aug 2023 23:49) - *Upvotes: 5*
Option A: This option is the most efficient way to gather the required metrics. It does not require any additional infrastructure and can be easily implemented.
Option B: This option is more complex than Option A and requires configuring a CloudWatch Logs Insights query. This can be more time-consuming to set up and can be less efficient if the query is not optimized.
Option C: This option requires configuring the ALB access logs to write to CloudWatch Logs. This can add additional latency to the requests.
Option D: This option requires configuring AWS X-Ray integration. This is a more complex solution that is not necessary in this case.

---

**bmoses** (Thu 04 Sep 2025 03:00) - *Upvotes: 1*
Option A: Is the correct Way of writing the lambda logs to CloudWatch logs group from their we can play with it with minimal effor

---

**festusbetd** (Thu 20 Feb 2025 20:58) - *Upvotes: 1*
Why Option A is Correct:
It directly addresses the requirement to gather metrics for each API operation by response code and application version.
CloudWatch Logs and metric filters are designed for this purpose and are easy to implement.
It provides a scalable and cost-effective solution.

---

**maikerusukofyirudo** (Thu 28 Nov 2024 08:28) - *Upvotes: 1*
That’s right

---

**chan123** (Mon 25 Nov 2024 03:41) - *Upvotes: 1*
dsfdsf

---

**nothinmuch** (Sun 25 Feb 2024 19:57) - *Upvotes: 3*
The answer is b based on the scenario.

When to Choose Which

Choose A (Metric Filters) if you need basic metrics with straightforward patterns (e.g., counting occurrences of specific API operations and response codes).

Choose B (Insights Queries) if you require more complex metric calculations, such as:
Aggregations (averages, sums, etc.) over time
Filtering metrics based on conditions within the logs
Creating custom metrics not directly defined in log lines

---

**Gowtham5798** (Wed 31 Jan 2024 11:54) - *Upvotes: 1*
The correct answer is A.

---

**thanhnv142** (Sun 28 Jan 2024 03:50) - *Upvotes: 1*
A is correct: Because only need to gather metric but not parse log or advanced analyzing the log.

---


<br/>

## Question 2

*Date: April 5, 2023, 1:29 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company provides an application to customers. The application has an Amazon API Gateway REST API that invokes an AWS Lambda function. On initialization, the Lambda function loads a large amount of data from an Amazon DynamoDB table. The data load process results in long cold-start times of 8-10 seconds. The DynamoDB table has DynamoDB Accelerator (DAX) configured.
Customers report that the application intermittently takes a long time to respond to requests. The application receives thousands of requests throughout the day. In the middle of the day, the application experiences 10 times more requests than at any other time of the day. Near the end of the day, the application's request volume decreases to 10% of its normal total.
A DevOps engineer needs to reduce the latency of the Lambda function at all times of the day.
Which solution will meet these requirements?

**Options:**
- A. Configure provisioned concurrency on the Lambda function with a concurrency value of 1. Delete the DAX cluster for the DynamoDB table.
- B. Configure reserved concurrency on the Lambda function with a concurrency value of 0.
- C. Configure provisioned concurrency on the Lambda function. Configure AWS Application Auto Scaling on the Lambda function with provisioned concurrency values set to a minimum of 1 and a maximum of 100.
- D. Configure reserved concurrency on the Lambda function. Configure AWS Application Auto Scaling on the API Gateway API with a reserved concurrency maximum value of 100.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**5aga** (Tue 24 Sep 2024 10:08) - *Upvotes: 11*
To reduce the latency of the Lambda function at all times of the day, the best solution is to configure provisioned concurrency on the Lambda function with a concurrency value of 1 and also configure AWS Application Auto Scaling on the Lambda function with provisioned concurrency values set to a minimum of 1 and a maximum of 100 (Option C).

Provisioned concurrency will ensure that the Lambda function has a set number of instances always available, which will reduce the cold start time. By setting the provisioned concurrency values to a minimum of 1 and a maximum of 100, the Lambda function can handle sudden spikes in traffic and can scale down during low-traffic periods, thus minimizing costs.

---

**9675557** (Tue 22 Jul 2025 14:53) - *Upvotes: 1*
Great! Answer is C: Beacuase Provisioned Concurrency Keeps a pre-initialized number of instances ready to avoid cold starts.

---

**ele** (Tue 24 Sep 2024 10:09) - *Upvotes: 2*
Lambda integrates with Application Auto Scaling, allowing you to manage provisioned concurrency on a schedule or based on utilization.
https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html
https://docs.aws.amazon.com/autoscaling/application/userguide/services-that-can-integrate-lambda.html

---

**c3518fc** (Tue 24 Sep 2024 10:09) - *Upvotes: 2*
By implementing provisioned concurrency with auto-scaling and retaining the DynamoDB DAX cluster, the DevOps engineer can effectively reduce the latency of the Lambda function at all times of the day while ensuring that the application can handle varying request volumes.

---

**Gomer** (Tue 24 Sep 2024 10:08) - *Upvotes: 1*
Answer is to is to use application autoscaling to create a Lambda scaling policy for Provisioned Concurrency based on a re-occuring schedule
Here is reference that explains exactly how to do it CLI: https://aws.amazon.com/blogs/compute/scheduling-aws-lambda-provisioned-concurrency-for-recurring-peak-usage/
Here are truncated command examples from the reference:
aws application-autoscaling register-scalable-target --service-namespace lambda [...] --min-capacity 1 --max-capacity 100 --scalable-dimension lambda:function:ProvisionedConcurrency
aws application-autoscaling put-scheduled-action --service-namespace lambda --scalable-dimension lambda:function:ProvisionedConcurrency --scalable-target-action MinCapacity=100 [...]

---

**omankoman** (Tue 20 Aug 2024 16:59) - *Upvotes: 1*
C is right answer.

---

**NagaoShingo** (Wed 14 Aug 2024 13:26) - *Upvotes: 1*
C is right answer. Omamko.

---

**thanhnv142** (Sat 27 Jan 2024 11:55) - *Upvotes: 1*
C definitely

---

**yuliaqwerty** (Sun 07 Jan 2024 15:10) - *Upvotes: 1*
Agree answer C. Provisioned concurrency – This is the number of pre-initialized execution environments allocated to your function. These execution environments are ready to respond immediately to incoming function requests.

---

**zijo** (Tue 28 Nov 2023 17:35) - *Upvotes: 1*
Auto Scaling makes it easy to dynamically adjust the provisioned concurrency based on metrics and hence option C is a good choice to dynamically adjust based on the changing demand levels of the lambda function throughout the day.

---


<br/>

## Question 3

*Date: April 7, 2023, 10:41 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company is adopting AWS CodeDeploy to automate its application deployments for a Java-Apache Tomcat application with an Apache Webserver. The development team started with a proof of concept, created a deployment group for a developer environment, and performed functional tests within the application. After completion, the team will create additional deployment groups for staging and production.
The current log level is configured within the Apache settings, but the team wants to change this configuration dynamically when the deployment occurs, so that they can set different log level configurations depending on the deployment group without having a different application revision for each group.
How can these requirements be met with the LEAST management overhead and without requiring different script versions for each deployment group?

**Options:**
- A. Tag the Amazon EC2 instances depending on the deployment group. Then place a script into the application revision that calls the metadata service and the EC2 API to identify which deployment group the instance is part of. Use this information to configure the log level settings. Reference the script as part of the AfterInstall lifecycle hook in the appspec.yml file.
- B. Create a script that uses the CodeDeploy environment variable DEPLOYMENT_GROUP_ NAME to identify which deployment group the instance is part of. Use this information to configure the log level settings. Reference this script as part of the BeforeInstall lifecycle hook in the appspec.yml file.
- C. Create a CodeDeploy custom environment variable for each environment. Then place a script into the application revision that checks this environment variable to identify which deployment group the instance is part of. Use this information to configure the log level settings. Reference this script as part of the ValidateService lifecycle hook in the appspec.yml file.
- D. Create a script that uses the CodeDeploy environment variable DEPLOYMENT_GROUP_ID to identify which deployment group the instance is part of to configure the log level settings. Reference this script as part of the Install lifecycle hook in the appspec.yml file.

> **Suggested Answer:** B
> **Community Vote:** B (73%), D (27%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Schubibubi** (Tue 18 Apr 2023 08:18) - *Upvotes: 18*
B. In the docs: https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html you'll find a Note: "The Start, Install, TestTraffic, AllowTraffic, and End events in the deployment cannot be scripted, which is why they appear in gray in this diagram." Thats why its not D.

---

**y0eri** (Tue 24 Sep 2024 10:09) - *Upvotes: 12*
Answer: B
Read this blog.
https://aws.amazon.com/blogs/devops/using-codedeploy-environment-variables/

if [ "$DEPLOYMENT_GROUP_NAME" == "Staging" ]
then
sed -i -e 's/LogLevel warn/LogLevel debug/g' /etc/httpd/conf/httpd.conf
fi

---

**steli0** (Sat 23 Nov 2024 23:18) - *Upvotes: 1*
On top of what others wrote the other difference between B and D is the BeforeInstall, since you need to configure log level before deploying the code/serivce.

---

**Sazeka** (Tue 24 Sep 2024 10:09) - *Upvotes: 2*
B is correct.
DEPLOYMENT_ID : This variables contains the deployment ID of the current deployment.

DEPLOYMENT_GROUP_NAME : This variable contains the name of the deployment group. A deployment group is a set of instances associated with an application that you target for a deployment.

---

**zijo** (Tue 24 Sep 2024 10:09) - *Upvotes: 3*
Answer B. You only need to consider options B and D which are the least complex ones. The option B gives the CodeDeploy environment variable DEPLOYMENT_GROUP_ NAME that points to different instances and gives the option to set different log-level configurations in the same script depending on the deployment group without having a different application revision for each group. Also, The BeforeInstall lifecycle hook in the appspec.yml file refers to a script that will run on the instance before the application revision files are installed.

---

**c3518fc** (Tue 24 Sep 2024 10:09) - *Upvotes: 1*
version: 0.0
os: linux
files:
- source: /
destination: /var/www/html/
hooks:
BeforeInstall:
- location: scripts/update_log_level.sh
timeout: 300
runas: root

---

**Gomer** (Tue 24 Sep 2024 10:09) - *Upvotes: 1*
This reference specifies the exact scenario described in "B", so I have to go with that
"Set the log level according to the deployment group."
https://aws.amazon.com/blogs/devops/using-codedeploy-environment-variables/

cat install_dependencies.sh
[...]
if [ "$DEPLOYMENT_GROUP_NAME" == "Staging" ]
[...]

cat appspec.yml
hooks:
BeforeInstall:
- location: scripts/install_dependencies

---

**thanhnv142** (Sat 27 Jan 2024 12:15) - *Upvotes: 2*
B is correct

---

**thanhnv142** (Fri 09 Feb 2024 07:01) - *Upvotes: 1*
B is correct: <without having a different application revision for each group> means A and C is incorrect.
A and C: <place a script into the application revision> both mention this, indicating a revision of the app, which is contradicted to the question
D: there is no Install lifecycle hook

---

**hoakhanh281** (Mon 01 Jan 2024 04:28) - *Upvotes: 3*
Answer D.
You only need to consider options B and D which are the least complex ones. The option B gives the CodeDeploy environment variable DEPLOYMENT_GROUP_ NAME, but DEPLOYMENT_GROUP_ID is recommended because it's more reliable and less prone to changes or inconsistencies. Answer D with settings DEPLOYMENT_GROUP_ID environment variable, which contains the unique identifier for the deployment group. This allows you to identify the deployment group without relying on custom scripts or metadata services.

---


<br/>

## Question 4

*Date: April 5, 2023, 9:45 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company requires its developers to tag all Amazon Elastic Block Store (Amazon EBS) volumes in an account to indicate a desired backup frequency. This requirement Includes EBS volumes that do not require backups. The company uses custom tags named Backup_Frequency that have values of none, dally, or weekly that correspond to the desired backup frequency. An audit finds that developers are occasionally not tagging the EBS volumes.
A DevOps engineer needs to ensure that all EBS volumes always have the Backup_Frequency tag so that the company can perform backups at least weekly unless a different value is specified.
Which solution will meet these requirements?

**Options:**
- A. Set up AWS Config in the account. Create a custom rule that returns a compliance failure for all Amazon EC2 resources that do not have a Backup Frequency tag applied. Configure a remediation action that uses a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly.
- B. Set up AWS Config in the account. Use a managed rule that returns a compliance failure for EC2::Volume resources that do not have a Backup Frequency tag applied. Configure a remediation action that uses a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly.
- C. Turn on AWS CloudTrail in the account. Create an Amazon EventBridge rule that reacts to EBS CreateVolume events. Configure a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly. Specify the runbook as the target of the rule.
- D. Turn on AWS CloudTrail in the account. Create an Amazon EventBridge rule that reacts to EBS CreateVolume events or EBS ModifyVolume events. Configure a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly. Specify the runbook as the target of the rule.

> **Suggested Answer:** B
> **Community Vote:** B (95%), 5%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**CristianoRosa** (Tue 24 Sep 2024 10:11) - *Upvotes: 6*
A: It works, but it uses a custom rule.
B: It is simpler than option A as it uses a managed rule which already exists.
C: It only applies to new volumes and does not address existing resources.
D: It is better than C but still does not fully meet the requirement to check all EBS volumes and enforce compliance.
Best Answer is B.

---

**arinaho_muleba** (Thu 24 Apr 2025 09:10) - *Upvotes: 1*
[A] will enforce the rule on all EC2 resources, including EC2 instance which isn't a requirement and will unnecessary be tagged

---

**bb4f13b** (Fri 03 Jan 2025 16:54) - *Upvotes: 1*
Option D:
Real-Time Enforcement: Ensures tagging compliance as soon as a volume is created or modified.
Comprehensive Coverage: Captures both CreateVolume and ModifyVolume events.
Minimal Overhead: Automates tagging without requiring manual audits or remediation actions

---

**dark4igi** (Wed 26 Feb 2025 21:59) - *Upvotes: 1*
what will happen if dev decide to change tag to dally?
AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly

---

**lunt** (Tue 24 Sep 2024 10:11) - *Upvotes: 4*
Only takes few minutes to login > Config > Managed rulename = BACKUP_PLAN_MIN_FREQUENCY_AND_MIN_RETENTION_CHECK
A = tags everything in EC2, thats EC2::* which includes ELB/EIP/etc. Nope.
Option B you can specify the tags to match & expected values = answer.

---

**c3518fc** (Tue 24 Sep 2024 10:11) - *Upvotes: 1*
By leveraging the AWS Config managed rule and automated remediation action, the DevOps engineer can ensure that all EBS volumes in the account always have the required Backup_Frequency tag, enabling the company to perform backups at least weekly unless a different value is explicitly specified. This solution provides continuous monitoring and automated remediation, reducing the risk of human error and ensuring compliance with the company's backup policy.

---

**ajeeshb** (Tue 24 Sep 2024 10:11) - *Upvotes: 2*
Option B --> AWS config managed rule on EC2::Volume resource + custom SSM automation document
Not Option A --> because it says custom config rule on all EC2::Instance + Managed SSM automation document
Not options C & D --> As it says cloudtrail which is for logging API actions

---

**ajeeshb** (Sun 30 Jun 2024 22:19) - *Upvotes: 1*
sorry, a typo.. Option A also says custom SSM automation document, but it is wrong where it says custom config rule on all Ec2::Instance

---

**Diego1414** (Tue 13 Feb 2024 22:43) - *Upvotes: 1*
Answer is A.
Checks if your resources have the tags that you specify. For example, you can check whether your Amazon EC2 instances have the CostCenter tag, while also checking if all your RDS instance have one set of Keys tag. Separate multiple values with commas. You can check up to 6 tags at a time.

The AWS-managed AWS Systems Manager automation document AWS-SetRequiredTags does not work as a remediation with this rule. You will need to create your own custom Systems Manager automation documentation for remediation

https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html

---

**Hizumi** (Sun 18 Feb 2024 14:07) - *Upvotes: 1*
We don't need to create a custom AWS Config rule, we can utilize the managed rule to detect for non-compliance on the EBS volumes. Otherwise the options indicate to use a custom runbook for AWS Systems Manager to remediate the missing tags.

---


<br/>

## Question 5

*Date: April 6, 2023, 9:24 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is using an Amazon Aurora cluster as the data store for its application. The Aurora cluster is configured with a single DB instance. The application performs read and write operations on the database by using the cluster's instance endpoint.
The company has scheduled an update to be applied to the cluster during an upcoming maintenance window. The cluster must remain available with the least possible interruption during the maintenance window.
What should a DevOps engineer do to meet these requirements?

**Options:**
- A. Add a reader instance to the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster's reader endpoint for reads.
- B. Add a reader instance to the Aurora cluster. Create a custom ANY endpoint for the cluster. Update the application to use the Aurora cluster's custom ANY endpoint for read and write operations.
- C. Turn on the Multi-AZ option on the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster’s reader endpoint for reads.
- D. Turn on the Multi-AZ option on the Aurora cluster. Create a custom ANY endpoint for the cluster. Update the application to use the Aurora cluster's custom ANY endpoint for read and write operations

> **Suggested Answer:** A
> **Community Vote:** A (75%), C (16%), 5%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**junrun3** (Tue 04 Jul 2023 17:41) - *Upvotes: 16*
B and D are incorrect because Aurora cluster provides cluster and read endpoints, but does not support creating custom ANY endpoints.



C and D are incorrect because Amazon Aurora's multi-AZ option must be set when the DB instance is created.



Therefore, A is correct.

---

**Just_Ninja** (Wed 05 Jul 2023 13:17) - *Upvotes: 13*
Option A is the right choise for an existing Cluster without Multi-AZ!
Refer to: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html

#Read the Tip Box#
"You can set up a Multi-AZ cluster by making a simple choice when you create the cluster. The choice is simple whether you use the AWS Management Console, the AWS CLI, or the Amazon RDS API. You can also make an existing Aurora cluster into a Multi-AZ cluster by adding a new reader instance and specifying a different Availability Zone."

---

**life1991** (Thu 17 Apr 2025 16:53) - *Upvotes: 1*
aurora cluster turn on multi az by default. for limit downtime, we create a new read instance. When an update window in the primary instance, the read instance become a new primary instance

---

**MarcosSantos** (Wed 16 Apr 2025 00:57) - *Upvotes: 1*
For me option C is the correct!

---

**haazybanj** (Tue 24 Sep 2024 10:13) - *Upvotes: 4*
C. Turn on the Multi-AZ option on the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster’s reader endpoint for reads.

Enabling Multi-AZ ensures that the data in the Aurora cluster is replicated across multiple Availability Zones (AZs), providing high availability and durability. During maintenance, the update will be applied to one AZ at a time, allowing the cluster to remain available. Updating the application to use the cluster endpoint for write operations ensures that writes will continue to be directed to the primary instance in the cluster, while updating the reader endpoint for reads allows read traffic to be routed to the appropriate instance. Adding a reader instance or creating a custom ANY endpoint are not necessary for meeting the requirement of minimizing interruption during maintenance.

---

**koenigParas2324** (Tue 24 Sep 2024 10:13) - *Upvotes: 2*
To meet the requirement of keeping the Aurora cluster available with the least possible interruption during the maintenance window, the DevOps engineer should choose option C: Turn on the Multi-AZ option on the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster’s reader endpoint for reads.

Option A is incorrect because adding a reader instance alone does not provide high availability during maintenance, and updating the application to use the reader endpoint for reads is unnecessary when the Multi-AZ option is enabled

---

**alexleely** (Tue 24 Sep 2024 10:12) - *Upvotes: 2*
Option A is the correct choice, adding a reader instance after provisioning is the same as setting a Multi-AZ during creation. In the event that the primary fails, the reader instance will be promoted to do both reading and writing automatically.

Additionally, reader instance can also be used for read activity if you use the cluster reader endpoint which you can serve to user/application closer to the region for better performance.

---

**vietnguyen2** (Tue 24 Sep 2024 10:12) - *Upvotes: 1*
"You can set up a Multi-AZ DB cluster by making a simple choice when you create the cluster. You can use the AWS Management Console, the AWS CLI, or the Amazon RDS API. You can also convert an existing Aurora DB cluster into a Multi-AZ DB cluster by adding a new reader DB instance and specifying a different Availability Zone."

---

**Rizwan_Shaukat** (Tue 24 Sep 2024 10:12) - *Upvotes: 2*
To meet the requirements of the given scenario, the DevOps engineer should do the following:

Add a reader instance to the Aurora cluster:

This will allow the application to offload read operations to the reader instance, reducing the load on the primary instance.
The application should be updated to use the Aurora cluster endpoint for write operations and the reader endpoint for read operations.
The engineer should not turn on the Multi-AZ option on the Aurora cluster.

Multi-AZ is used to provide high availability and failover capabilities, but it does not necessarily minimize interruption during a maintenance window.
Adding a reader instance is a more appropriate solution to maintain availability and distribute the read workload.
Therefore, the correct option is A. Add a reader instance to the Aurora cluster and update the application to use the appropriate endpoints for read and write operations.

---

**Rahul369** (Thu 20 Jun 2024 10:08) - *Upvotes: 1*
You cannot change the az option after creation but can deploy a reader instance in another az and use it for reading and writing in your main instance.

---


<br/>

## Question 6

*Date: April 5, 2023, 12:27 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company must encrypt all AMIs that the company shares across accounts. A DevOps engineer has access to a source account where an unencrypted custom AMI has been built. The DevOps engineer also has access to a target account where an Amazon EC2 Auto Scaling group will launch EC2 instances from the AMI. The DevOps engineer must share the AMI with the target account.
The company has created an AWS Key Management Service (AWS KMS) key in the source account.
Which additional steps should the DevOps engineer perform to meet the requirements? (Choose three.)

**Options:**
- A. In the source account, copy the unencrypted AMI to an encrypted AMI. Specify the KMS key in the copy action.
- B. In the source account, copy the unencrypted AMI to an encrypted AMI. Specify the default Amazon Elastic Block Store (Amazon EBS) encryption key in the copy action.
- C. In the source account, create a KMS grant that delegates permissions to the Auto Scaling group service-linked role in the target account.
- D. In the source account, modify the key policy to give the target account permissions to create a grant. In the target account, create a KMS grant that delegates permissions to the Auto Scaling group service-linked role.
- E. In the source account, share the unencrypted AMI with the target account.
- F. In the source account, share the encrypted AMI with the target account.

> **Suggested Answer:** ADF
> **Community Vote:** ADF (96%), 2%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**kacsabacsi78** (Mon 12 Jun 2023 13:31) - *Upvotes: 11*
ADF seems to be the correct answer

---

**Dimidrol** (Wed 05 Apr 2023 12:27) - *Upvotes: 9*
A D F for me. https://jackiechen.blog/2020/01/29/share-encrypted-ami-across-aws-accounts/

---

**life1991** (Thu 17 Apr 2025 16:57) - *Upvotes: 2*
i think so

---

**namtp** (Sun 04 Aug 2024 19:22) - *Upvotes: 1*
ADF for me,

---

**martinarg2024** (Fri 01 Mar 2024 17:54) - *Upvotes: 1*
ADF is correct

---

**Vitalydt** (Wed 28 Feb 2024 12:26) - *Upvotes: 1*
A D F for me

---

**thanhnv142** (Sun 28 Jan 2024 03:54) - *Upvotes: 2*
ADF:
A: cannot be B because using KMS
D: Must share with the account because grant is only temp
F: share the AMI with the target

---

**thanhnv142** (Sat 27 Jan 2024 15:42) - *Upvotes: 1*
AFD seem about right

---

**Jonalb** (Sun 21 Jan 2024 06:56) - *Upvotes: 1*
ADF the correct answer

---

**khchan123** (Sat 13 Jan 2024 03:43) - *Upvotes: 1*
ACF. For autoscaling to work a KMS grant is needed

---


<br/>

## Question 7

*Date: April 7, 2023, 2:12 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses AWS CodePipeline pipelines to automate releases of its application A typical pipeline consists of three stages build, test, and deployment. The company has been using a separate AWS CodeBuild project to run scripts for each stage. However, the company now wants to use AWS CodeDeploy to handle the deployment stage of the pipelines.
The company has packaged the application as an RPM package and must deploy the application to a fleet of Amazon EC2 instances. The EC2 instances are in an EC2 Auto Scaling group and are launched from a common AMI.
Which combination of steps should a DevOps engineer perform to meet these requirements? (Choose two.)

**Options:**
- A. Create a new version of the common AMI with the CodeDeploy agent installed. Update the IAM role of the EC2 instances to allow access to CodeDeploy.
- B. Create a new version of the common AMI with the CodeDeploy agent installed. Create an AppSpec file that contains application deployment scripts and grants access to CodeDeploy.
- C. Create an application in CodeDeploy. Configure an in-place deployment type. Specify the Auto Scaling group as the deployment target. Add a step to the CodePipeline pipeline to use EC2 Image Builder to create a new AMI. Configure CodeDeploy to deploy the newly created AMI.
- D. Create an application in CodeDeploy. Configure an in-place deployment type. Specify the Auto Scaling group as the deployment target. Update the CodePipeline pipeline to use the CodeDeploy action to deploy the application.
- E. Create an application in CodeDeploy. Configure an in-place deployment type. Specify the EC2 instances that are launched from the common AMI as the deployment target. Update the CodePipeline pipeline to use the CodeDeploy action to deploy the application.

> **Suggested Answer:** AD
> **Community Vote:** AD (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**bcx** (Tue 30 May 2023 15:44) - *Upvotes: 11*
A and D are the correct ones.

E is wrong because it says that the instances are on an ASG.
C is qrong. You deploy the new RPM on the AMI, you do not create a new AMI every time to install the RPM.
B is wrong, the appspec has nothing to do with permissions

---

**thanhnv142** (Tue 24 Sep 2024 10:15) - *Upvotes: 6*
A and D are correct:
A: rebuild the AMI and Update the IAM role of the EC2 instances to allow access to CodeDeploy is necessary
B: no need to grants AppSpec file access to code CodeDeploy
C: <Add a step to the CodePipeline pipeline to use EC2 Image Builder to create a new AMI> : this is unnecessary, we have already done it in option A. Addtionally, recreating AIM each time running the CICD pipiline is unnecessary
D: ok
E: <Specify the EC2 instances that are launched from the common AMI as the deployment targe>: this is time-consumming. There might by hundreds of EC2 instances and targeting them individually is time-consuming and not effective.

---

**Saudis** (Tue 12 Nov 2024 13:15) - *Upvotes: 1*
the ans is A and D

D not E because in deployment the best practise deploy in group not instance

---

**thanhnv142** (Sat 27 Jan 2024 16:18) - *Upvotes: 1*
A and D:
B is incorrect: AppSpec file does not need to be granted access to code deploy. It is code deploy that need the permission to get acess to Appspec file

---

**z_inderjot** (Tue 19 Dec 2023 05:29) - *Upvotes: 3*
A - instances need code deploy agent and role
D - target as ASG

---

**robertohyena** (Tue 07 Nov 2023 14:06) - *Upvotes: 2*
A D.
IAM role/instance profile requirement for EC2 is to allow EC2 access to S3 buckets used by CodeDeploy.

https://docs.aws.amazon.com/codedeploy/latest/userguide/getting-started-create-iam-instance-profile.html

---

**DZ_Ben** (Mon 30 Oct 2023 05:52) - *Upvotes: 2*
Should be BD. EC2 doesn't need a permission to access CodeDeploy. Instead an IAM role associated with Code Deployment Group should have an permission to launch instances in that autoscaling group.

---

**z_inderjot** (Tue 19 Dec 2023 05:28) - *Upvotes: 3*
A is right
you have to attach IAM role to EC2 instance , for them to be controlled by Code deploy . The agent running in the ec2 instance needs to talk with code deploy .

---

**sivre** (Thu 12 Oct 2023 17:15) - *Upvotes: 3*
Why EC2 instance need access to CodeDeploy??, in the doc is mentioned only S3: "Create or locate an IAM instance profile that allows the Amazon EC2 Auto Scaling group to work with Amazon S3" https://docs.aws.amazon.com/codedeploy/latest/userguide/integrations-aws-auto-scaling.html

---

**harithzainudin** (Sun 10 Dec 2023 03:15) - *Upvotes: 3*
As the codedeploy agent is being installed inside the EC2, This agent facilitates the deployment process by coordinating with the CodeDeploy service. Hence,
The EC2 instances must have an IAM role that grants them the necessary permissions to interact with CodeDeploy. This step is critical to ensure that the deployment process can be executed securely and successfully

---


<br/>

## Question 8

*Date: April 5, 2023, 12:30 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company’s security team requires that all external Application Load Balancers (ALBs) and Amazon API Gateway APIs are associated with AWS WAF web ACLs. The company has hundreds of AWS accounts, all of which are included in a single organization in AWS Organizations. The company has configured AWS Config for the organization. During an audit, the company finds some externally facing ALBs that are not associated with AWS WAF web ACLs.
Which combination of steps should a DevOps engineer take to prevent future violations? (Choose two.)

**Options:**
- A. Delegate AWS Firewall Manager to a security account.
- B. Delegate Amazon GuardDuty to a security account.
- C. Create an AWS Firewall Manager policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs.
- D. Create an Amazon GuardDuty policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs.
- E. Configure an AWS Config managed rule to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs.

> **Suggested Answer:** AC
> **Community Vote:** AC (96%), 4%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**ataince** (Sun 24 Sep 2023 18:32) - *Upvotes: 12*
If you see WAF you have to think AWS Firewall Manager.

---

**alce2020** (Sat 15 Apr 2023 19:01) - *Upvotes: 10*
A and C

---

**ele** (Tue 24 Sep 2024 10:16) - *Upvotes: 1*
If instead you want to automatically apply the policy to existing in-scope resources, choose Auto remediate any noncompliant resources. This option creates a web ACL in each applicable account within the AWS organization and associates the web ACL with the resources in the accounts.
When you choose Auto remediate any noncompliant resources, you can also choose to remove existing web ACL associations from in-scope resources, for the web ACLs that aren't managed by another active Firewall Manager policy. If you choose this option, Firewall Manager first associates the policy's web ACL with the resources, and then removes the prior associations. If a resource has an association with another web ACL that's managed by a different active Firewall Manager policy, this choice doesn't affect that association.

---

**namtp** (Sun 04 Aug 2024 19:34) - *Upvotes: 1*
I think that is best way to centralize manage firewall config

---

**jamesf** (Thu 25 Jul 2024 06:44) - *Upvotes: 1*
As my understanding, WAF related with AWS Firewall Manager.

---

**Gomer** (Thu 23 May 2024 21:59) - *Upvotes: 2*
These references indicate this can all be handled within Firewall manager (w/no references to Config or GuardDuty)
https://aws.amazon.com/blogs/security/how-to-enforce-a-security-baseline-for-an-aws-waf-acl-across-your-organization-using-aws-firewall-manager/
https://aws.amazon.com/solutions/implementations/automations-for-aws-firewall-manager/

---

**Gomer** (Thu 23 May 2024 22:07) - *Upvotes: 1*
In reading a little further, I suspect that Config may be being used in the background (since Config must be enabled to use WAF. However, I believe that is totally transparent to the Organization WAF Administrator. The administration of WAF and enforcement of WAF policies is ALL handled with the Web Application Firewall service.

---

**01037** (Sun 05 May 2024 10:36) - *Upvotes: 1*
I think E works, but Firewall manager is designed for the purpose.

---

**GripZA** (Fri 18 Apr 2025 21:30) - *Upvotes: 1*
AWS Config can't auto remediate unless there's additional integration, eg with Lambda.

---

**Cervus18** (Tue 12 Mar 2024 11:07) - *Upvotes: 1*
A and C: AWS Config rules are primarily used for monitoring and evaluating the configurations of your AWS resources for compliance with desired configurations. However, AWS Config also supports remediation actions through AWS Systems Manager Automation documents.

---


<br/>

## Question 9

*Date: April 5, 2023, 2:32 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses AWS Key Management Service (AWS KMS) keys and manual key rotation to meet regulatory compliance requirements. The security team wants to be notified when any keys have not been rotated after 90 days.
Which solution will accomplish this?

**Options:**
- A. Configure AWS KMS to publish to an Amazon Simple Notification Service (Amazon SNS) topic when keys are more than 90 days old.
- B. Configure an Amazon EventBridge event to launch an AWS Lambda function to call the AWS Trusted Advisor API and publish to an Amazon Simple Notification Service (Amazon SNS) topic.
- C. Develop an AWS Config custom rule that publishes to an Amazon Simple Notification Service (Amazon SNS) topic when keys are more than 90 days old.
- D. Configure AWS Security Hub to publish to an Amazon Simple Notification Service (Amazon SNS) topic when keys are more than 90 days old.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Sat 27 Jul 2024 15:27) - *Upvotes: 5*
C is correct
A is not because KMS does not provide this function

---

**yuliaqwerty** (Sun 07 Jul 2024 14:50) - *Upvotes: 3*
Answer C. AWS Config

---

**habros** (Thu 04 Jan 2024 16:24) - *Upvotes: 3*
C. Config rules notifies.

---

**Toptip** (Thu 28 Dec 2023 19:55) - *Upvotes: 3*
Are these questions really came from DOP-C02?

---

**madperro** (Fri 08 Dec 2023 13:05) - *Upvotes: 2*
C makes sense. it should be a custom rule. Rule "access-keys-rotated" checks for access keys, not KMS keys.

---

**alce2020** (Sun 15 Oct 2023 19:02) - *Upvotes: 1*
C it is

---

**ele** (Sat 07 Oct 2023 14:22) - *Upvotes: 1*
custom config: C

---

**asfsdfsdf** (Fri 06 Oct 2023 09:38) - *Upvotes: 4*
Looks like C, actually there is a managed rule for this:
https://docs.aws.amazon.com/config/latest/developerguide/access-keys-rotated.html
anyway trusted advisor cannot be used as there is no such check, also KMS does not have this action, security hub is not conducting any active checks just react to events

---

**s50600822** (Sat 13 Jan 2024 18:44) - *Upvotes: 3*
access key?

---

**zijo** (Thu 22 Aug 2024 18:14) - *Upvotes: 1*
IAM Access Key & KMS key are different. The managed rule is for IAM Access key

---


<br/>

## Question 10

*Date: April 7, 2023, 2:24 p.m.
Disclaimers:
- ExamTopics website is not rel*

A security review has identified that an AWS CodeBuild project is downloading a database population script from an Amazon S3 bucket using an unauthenticated request. The security team does not allow unauthenticated requests to S3 buckets for this project.
How can this issue be corrected in the MOST secure manner?

**Options:**
- A. Add the bucket name to the AllowedBuckets section of the CodeBuild project settings. Update the build spec to use the AWS CLI to download the database population script.
- B. Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the build spec to use cURL to pass the token and download the database population script.
- C. Remove unauthenticated access from the S3 bucket with a bucket policy. Modify the service role for the CodeBuild project to include Amazon S3 access. Use the AWS CLI to download the database population script.
- D. Remove unauthenticated access from the S3 bucket with a bucket policy. Use the AWS CLI to download the database population script using an IAM access key and a secret access key.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Sat 27 Jan 2024 16:36) - *Upvotes: 6*
C is correct:
+ Remove unauthenticated access from the S3 bucket with a bucket policy
+ Modify the service role for the CodeBuild project to include Amazon S3 access.

---

**namtp** (Sun 04 Aug 2024 19:50) - *Upvotes: 1*
C is a correct answer.
Inside AWS, using of service roles is the best option.

---

**z_inderjot** (Tue 19 Dec 2023 05:48) - *Upvotes: 3*
all these questions seem fairly to be part of aws devops exam

---

**zain1258** (Thu 26 Oct 2023 18:44) - *Upvotes: 1*
C is correct

---

**Cervus18** (Wed 25 Oct 2023 13:54) - *Upvotes: 2*
Involves using a service role also, which make it the most secure manner

---

**SanChan** (Mon 12 Jun 2023 06:55) - *Upvotes: 4*
C is the correct answer because it involves removing unauthenticated access from the S3 bucket with a bucket policy, which ensures that only authorized users or services can access the bucket.

---

**madperro** (Thu 08 Jun 2023 12:06) - *Upvotes: 1*
C is the best answer.

---

**alce2020** (Fri 14 Apr 2023 22:08) - *Upvotes: 2*
c is the answer

---

**ataince** (Mon 10 Apr 2023 20:00) - *Upvotes: 1*
c is the answer.

---

**ele** (Fri 07 Apr 2023 14:24) - *Upvotes: 1*
C most secure

---


<br/>

## Question 11

*Date: April 5, 2023, 2:42 a.m.
Disclaimers:
- ExamTopics website is not rel*

An ecommerce company has chosen AWS to host its new platform. The company's DevOps team has started building an AWS Control Tower landing zone. The DevOps team has set the identity store within AWS IAM Identity Center (AWS Single Sign-On) to external identity provider (IdP) and has configured SAML 2.0.
The DevOps team wants a robust permission model that applies the principle of least privilege. The model must allow the team to build and manage only the team's own resources.
Which combination of steps will meet these requirements? (Choose three.)

**Options:**
- A. Create IAM policies that include the required permissions. Include the aws:PrincipalTag condition key.
- B. Create permission sets. Attach an inline policy that includes the required permissions and uses the aws:PrincipalTag condition key to scope the permissions.
- C. Create a group in the IdP. Place users in the group. Assign the group to accounts and the permission sets in IAM Identity Center.
- D. Create a group in the IdP. Place users in the group. Assign the group to OUs and IAM policies.
- E. Enable attributes for access control in IAM Identity Center. Apply tags to users. Map the tags as key-value pairs.
- F. Enable attributes for access control in IAM Identity Center. Map attributes from the IdP as key-value pairs.

> **Suggested Answer:** BCF
> **Community Vote:** BCF (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**bcx** (Tue 30 May 2023 15:56) - *Upvotes: 14*
I would go with BCF. I cannot make a large comment on why but manage an identity center setup at work and find that these are the correct ones IMHO. Your IdP has attributes, not tags, ou have to rely on the IdP's attributes for instance. And you work with permission sets almost always, so the three answers about the permission sets make the full answer. You do not use IAM directly or tags for this.

---

**asfsdfsdf** (Thu 06 Apr 2023 10:05) - *Upvotes: 6*
This is clearly stated here:
https://aws.amazon.com/blogs/aws/new-attributes-based-access-control-with-aws-single-sign-on/
Answers are: BCF - permissions sets + IDP attributes mapping + groups
For example a user with IDP attribute of Dep/hr will be able to delete instances with this specific tag

---

**namtp** (Tue 06 Aug 2024 17:42) - *Upvotes: 1*
BCF is correct anwers.
Permission set + group created in the IdP, and map attributes is key

---

**Gomer** (Fri 24 May 2024 11:07) - *Upvotes: 1*
While I have no great insights or expertise in this area, I do know how to read (RTFM) and quasi-solve the puzzle in my head. This reference URL (pdf) seems to touch all the steps listed in "B", "C", "F" and showed some extra steps not listed. Search and see for yourself.
https://d1.awsstatic.com/events/aws-reinforce-2022/IAM309_Designing-a-well-architected-identity-and-access-management-solution.pdf

---

**Gomer** (Fri 24 May 2024 11:17) - *Upvotes: 4*
Also, I might add, rather than just memorize the most votes answer to the question, I'd suggest actually going out to do some research and taking some long term notes you can reference later. That may take more time, but you also be more competent at work, and maybe keep your job longer. I love the fact that exam topics gives a forum to discuss and research complex questions and share findings. It's pretty lame If you come here to just memorize answers long enough to pass an exam.

---

**zijo** (Thu 22 Feb 2024 21:50) - *Upvotes: 1*
Permission sets are stored in IAM Identity Center. So you know all answers that mention about permission sets and IAM Identity Center are likely correct

---

**thanhnv142** (Sun 28 Jan 2024 04:41) - *Upvotes: 1*
B, C, E seem more accurate:
B- need to attach the policy so that it can be usable. A is not true because IAM policies is not the same as in IAM Identity Center
C- not D because cannot assign group to IAM policies. IAM policies is attached to groups. also, need permission sets in Identity Center
E- attributes is basically tagging.

---

**SafranboluLokumu** (Wed 29 Nov 2023 12:47) - *Upvotes: 1*
correct answer seen as A-B-C. but 11 people sure the correct answer is B-C-F in discussion.
What is the answer?
Can the system show the correct answer as wrong or are people mistaken?

---

**davdan99** (Fri 05 Jan 2024 18:23) - *Upvotes: 4*
The examTopics answers in most cases are wrong, please read discussions, and references that users provide

---

**ajeeshb** (Mon 01 Jul 2024 10:10) - *Upvotes: 2*
Then why do people pay the fee for access, I dont understand. If it is from a discussion the people have to understand the answer (that too not very sure), why do they charge so much for the contributor access?!

---


<br/>

## Question 12

*Date: April 7, 2023, 2:59 p.m.
Disclaimers:
- ExamTopics website is not rel*

An ecommerce company is receiving reports that its order history page is experiencing delays in reflecting the processing status of orders. The order processing system consists of an AWS Lambda function that uses reserved concurrency. The Lambda function processes order messages from an Amazon Simple Queue Service (Amazon SQS) queue and inserts processed orders into an Amazon DynamoDB table. The DynamoDB table has auto scaling enabled for read and write capacity.
Which actions should a DevOps engineer take to resolve this delay? (Choose two.)

**Options:**
- A. Check the ApproximateAgeOfOldestMessage metric for the SQS queue. Increase the Lambda function concurrency limit.
- B. Check the ApproximateAgeOfOldestMessage metnc for the SQS queue Configure a redrive policy on the SQS queue.
- C. Check the NumberOfMessagesSent metric for the SQS queue. Increase the SQS queue visibility timeout.
- D. Check the WriteThrottleEvents metric for the DynamoDB table. Increase the maximum write capacity units (WCUs) for the table's scaling policy.
- E. Check the Throttles metric for the Lambda function. Increase the Lambda function timeout.

> **Suggested Answer:** AD
> **Community Vote:** AD (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**madperro** (Thu 08 Jun 2023 12:29) - *Upvotes: 11*
AD look fine.

---

**kiwtirApp** (Sat 11 May 2024 22:23) - *Upvotes: 3*
Everyone who has commented AD has not provided any reasoning. Bunch of sheeps in the comments.

---

**tgv** (Thu 13 Jun 2024 12:01) - *Upvotes: 4*
D: This action is important because if the WriteThrottleEvents metric is high, it indicates that DynamoDB is throttling writes due to insufficient write capacity. By increasing the maximum WCUs, you ensure that the table can handle the increased write throughput required by the Lambda function, thus reducing delays in order processing.
even though DynamoDB has auto-scaling enabled, it’s still important to monitor the WriteThrottleEvents metric. Auto-scaling adjusts capacity based on the workload, but it may not always keep up with sudden spikes in demand or be configured optimally for this specific use case. Ensuring that the maximum write capacity units (WCUs) are set appropriately can help prevent throttling during peak times.

---

**thanhnv142** (Sun 28 Jan 2024 04:54) - *Upvotes: 3*
A and D is correct:
A: Check ApproximateAgeOfOldestMessage and increase concurrency accordingly
D: Check throttleevent (the number of rejected requests) and increase max write accordingly.

---

**SPRao** (Sun 19 Nov 2023 07:35) - *Upvotes: 3*
Answer should be A & C. D is wrong as DynamoDB has autoscaling enabled so Writethrottle should not be the case.

---

**harithzainudin** (Sun 10 Dec 2023 07:29) - *Upvotes: 4*
Eventhough the statement say


"The DynamoDB table has auto scaling enabled for read and write capacity."

A and D still are a good answer.

Option C looks at the NumberOfMessagesSent metric and suggests increasing the SQS queue visibility timeout. This action is more relevant when messages are not being processed before the visibility timeout expires, but it does not seem to be the primary issue in this scenario.

---

**Jonfernz** (Sun 10 Sep 2023 20:43) - *Upvotes: 4*
A: Check the ApproximateAgeOfOldestMessage metric for the SQS queue. If this age is high, increasing the Lambda function's concurrency limit would help speed up processing.

D: Check the WriteThrottleEvents metric for the DynamoDB table. If write operations are being throttled, increasing the maximum WCUs for the table’s scaling policy could help.

---

**ParagSanyashiv** (Mon 08 May 2023 07:20) - *Upvotes: 4*
AD are accurate for this scenario.

---

**alce2020** (Fri 14 Apr 2023 22:11) - *Upvotes: 1*
a and d are correct

---

**ele** (Fri 07 Apr 2023 14:59) - *Upvotes: 2*
upscale both

---


<br/>

## Question 13

*Date: April 7, 2023, 3:11 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has a single AWS account that runs hundreds of Amazon EC2 instances in a single AWS Region. New EC2 instances are launched and terminated each hour in the account. The account also includes existing EC2 instances that have been running for longer than a week.
The company's security policy requires all running EC2 instances to use an EC2 instance profile. If an EC2 instance does not have an instance profile attached, the EC2 instance must use a default instance profile that has no IAM permissions assigned.
A DevOps engineer reviews the account and discovers EC2 instances that are running without an instance profile. During the review, the DevOps engineer also observes that new EC2 instances are being launched without an instance profile.
Which solution will ensure that an instance profile is attached to all existing and future EC2 instances in the Region?

**Options:**
- A. Configure an Amazon EventBridge rule that reacts to EC2 RunInstances API calls. Configure the rule to invoke an AWS Lambda function to attach the default instance profile to the EC2 instances.
- B. Configure the ec2-instance-profile-attached AWS Config managed rule with a trigger type of configuration changes. Configure an automatic remediation action that invokes an AWS Systems Manager Automation runbook to attach the default instance profile to the EC2 instances.
- C. Configure an Amazon EventBridge rule that reacts to EC2 StartInstances API calls. Configure the rule to invoke an AWS Systems Manager Automation runbook to attach the default instance profile to the EC2 instances
- D. Configure the iam-role-managed-policy-check AWS Config managed rule with a trigger type of configuration changes. Configure an automatic remediation action that invokes an AWS Lambda function to attach the default instance profile to the EC2 instances.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**koenigParas2324** (Mon 20 May 2024 13:05) - *Upvotes: 9*
WS Config, specifically utilizing the "ec2-instance-profile-attached" managed rule with the configuration change trigger type. This rule helps monitor the attachment of instance profiles to EC2 instances. An automatic remediation action can be configured within AWS Config to respond when instances are found without an instance profile attached. The remediation action would execute an AWS Systems Manager Automation runbook to attach the default instance profile to those instances.

---

**AWSPICHI** (Sat 25 Jan 2025 12:04) - *Upvotes: 5*
How is it possible to remember the list of all the config rules?

---

**Gomer** (Mon 25 Nov 2024 00:14) - *Upvotes: 1*
I concur the best answer seems to be "B". However, I have not been able to trigger exactly what kind of "configuration change" triggers the config rule (e.g. "starting" or "running" an instance isn't a configuration change, but a state change. The real world answer (IMHO) would be to just kick off the AWS Config rule manually or on a schedule. I'd also take steps to ensure that all EC2 launch templates specify an instance profile so I'm not running around trying to fix things that shouldn't have been left broken from the start.

---

**Gillar** (Wed 09 Oct 2024 16:51) - *Upvotes: 1*
The rules AWS Config

---

**thanhnv142** (Sun 28 Jul 2024 04:05) - *Upvotes: 4*
B is correct: AWS config + runbook is the right way for remediation

---

**thanhnv142** (Sun 04 Aug 2024 03:14) - *Upvotes: 5*
B is correct: AWS Config run in combination with SSM Automation run book is the recommended way
A: this option only remediate new instances
C: this option only remedidate instances that have been stopped.
D: automatic remediation action should invoke Automation run book, not lambda

---

**DucSiu** (Thu 13 Jun 2024 14:50) - *Upvotes: 3*
B
Config: ec2-instance-profile-attached
SSM Automation: AttachedIAMtoinstances

---

**madperro** (Fri 08 Dec 2023 14:02) - *Upvotes: 2*
B is correct.
https://docs.aws.amazon.com/config/latest/developerguide/ec2-instance-profile-attached.html

---

**ParagSanyashiv** (Wed 08 Nov 2023 08:21) - *Upvotes: 1*
B is correct

---

**alce2020** (Sat 14 Oct 2023 22:13) - *Upvotes: 1*
correct answer is B

---


<br/>

## Question 14

*Date: April 7, 2023, 3:16 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer is building a continuous deployment pipeline for a serverless application that uses AWS Lambda functions. The company wants to reduce the customer impact of an unsuccessful deployment. The company also wants to monitor for issues.
Which deploy stage configuration will meet these requirements?

**Options:**
- A. Use an AWS Serverless Application Model (AWS SAM) template to define the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the Canary10Percent15Minutes Deployment Preference Type. Use Amazon CloudWatch alarms to monitor the health of the functions.
- B. Use AWS CloudFormation to publish a new stack update, and include Amazon CloudWatch alarms on all resources. Set up an AWS CodePipeline approval action for a developer to verify and approve the AWS CloudFormation change set.
- C. Use AWS CloudFormation to publish a new version on every stack update, and include Amazon CloudWatch alarms on all resources. Use the RoutingConfig property of the AWS::Lambda::Alias resource to update the traffic routing during the stack update.
- D. Use AWS CodeBuild to add sample event payloads for testing to the Lambda functions. Publish a new version of the functions, and include Amazon CloudWatch alarms. Update the production alias to point to the new version. Configure rollbacks to occur when an alarm is in the ALARM state.

> **Suggested Answer:** A
> **Community Vote:** A (85%), D (15%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**zolthar_z** (Fri 17 Nov 2023 19:31) - *Upvotes: 23*
Certification TIP: 99% of questions regarding lambda and cloudformation the answer is the one that involves SAM

---

**harithzainudin** (Sun 10 Dec 2023 07:38) - *Upvotes: 3*
i couldnt agree more with this

---

**Jonfernz** (Sun 10 Sep 2023 20:53) - *Upvotes: 7*
A

Reducing Customer Impact: AWS CodeDeploy with Canary deployments (Canary10Percent15Minutes) will incrementally roll out the new version. Initially, 10% of the traffic will be directed to the new version, and if everything goes well, the rest of the traffic will be shifted over the span of 15 minutes. This cautious rollout minimizes the risk and impact on customers.

Monitoring: Amazon CloudWatch alarms can be configured to track function errors, latency, and other important metrics. If anything goes awry, you can act promptly.

---

**jamesf** (Thu 25 Jul 2024 08:11) - *Upvotes: 4*
A
Keywords: Serverless Application related with AWS SAM

---

**zijo** (Mon 26 Feb 2024 18:45) - *Upvotes: 1*
Canary10Percent15Minutes refers to a specific type of deployment strategy used in the context of serverless applications, particularly with tools like AWS SAM (Serverless Application Model).

---

**thanhnv142** (Sun 04 Feb 2024 04:18) - *Upvotes: 4*
A is correct: <a continuous deployment pipeline for a serverless application> means AWS SAM.
B, C and D: no mention of SAM

---

**thanhnv142** (Sun 28 Jan 2024 05:08) - *Upvotes: 1*
A: use serverless code deployment is the right way

---

**z_inderjot** (Wed 20 Dec 2023 05:17) - *Upvotes: 1*
A - SAM for Lambda deployment
Reduce Custome Impact - Canary got it covered

---

**RVivek** (Mon 18 Sep 2023 03:50) - *Upvotes: 1*
A CodeDepoly is for canary deployment , cloudwatch alarm for monitoring, if aram is raised then codedeploy automatically rolls back
D- Using Codebuild for controlled deployment is not good. Codebuild is for build and testing

---

**DaddyDee** (Wed 06 Sep 2023 03:46) - *Upvotes: 2*
A is the answer: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html

---


<br/>

## Question 15

*Date: April 7, 2023, 3:19 p.m.
Disclaimers:
- ExamTopics website is not rel*

To run an application, a DevOps engineer launches an Amazon EC2 instance with public IP addresses in a public subnet. A user data script obtains the application artifacts and installs them on the instances upon launch. A change to the security classification of the application now requires the instances to run with no access to the internet. While the instances launch successfully and show as healthy, the application does not seem to be installed.
Which of the following should successfully install the application while complying with the new rule?

**Options:**
- A. Launch the instances in a public subnet with Elastic IP addresses attached. Once the application is installed and running, run a script to disassociate the Elastic IP addresses afterwards.
- B. Set up a NAT gateway. Deploy the EC2 instances to a private subnet. Update the private subnet's route table to use the NAT gateway as the default route.
- C. Publish the application artifacts to an Amazon S3 bucket and create a VPC endpoint for S3. Assign an IAM instance profile to the EC2 instances so they can read the application artifacts from the S3 bucket.
- D. Create a security group for the application instances and allow only outbound traffic to the artifact repository. Remove the security group rule once the install is complete.

> **Suggested Answer:** C
> **Community Vote:** C (91%), 9%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**z_inderjot** (Wed 20 Dec 2023 05:22) - *Upvotes: 8*
C - in the answer
Though we can use both B and C , since we only want to download to package at the time of initialization . So there is no need to have continuous access to internet . Therefore, it is cheap and optimal to use S3 .

---

**namtp** (Tue 06 Aug 2024 17:59) - *Upvotes: 1*
C is the correct answer.
no access to the internet but connect to aws services => private endpoint

---

**thanhnv142** (Sun 28 Jan 2024 05:20) - *Upvotes: 3*
C is correct: all other options utilize internet connections

---

**harithzainudin** (Sun 10 Dec 2023 07:47) - *Upvotes: 3*
C is the correct one.

all other option will allow internet access which is not compliance with the reqs

---

**zolthar_z** (Fri 17 Nov 2023 19:33) - *Upvotes: 3*
C: Can't be B because with the NAT the EC2 still has internet access

---

**robertohyena** (Sat 11 Nov 2023 15:27) - *Upvotes: 2*
C is the correct answer.

A B D are not correct.
Keywords:
- requires the instances to run with no access to the internet

---

**rowanwally** (Tue 14 Nov 2023 00:07) - *Upvotes: 1*
is the dump still valid?

---

**bosmanx** (Sun 05 Nov 2023 08:55) - *Upvotes: 2*
B is incorrect, the new policy is "no access to the internet"

---

**DevopsNoob** (Sat 28 Oct 2023 14:33) - *Upvotes: 1*
C is the answer. B would enable internet access from the instance.

---

**Ffida** (Fri 29 Sep 2023 15:12) - *Upvotes: 1*
C is correct and B, which is specifically for NAT. in question they have asked that no internet access from the instance, so If we enable NAT then from outside no one can access the instance but internet will be accessible on the instance using NAT.

---


<br/>

## Question 16

*Date: April 7, 2023, 3:39 p.m.
Disclaimers:
- ExamTopics website is not rel*

A development team is using AWS CodeCommit to version control application code and AWS CodePipeline to orchestrate software deployments. The team has decided to use a remote main branch as the trigger for the pipeline to integrate code changes. A developer has pushed code changes to the CodeCommit repository, but noticed that the pipeline had no reaction, even after 10 minutes.
Which of the following actions should be taken to troubleshoot this issue?

**Options:**
- A. Check that an Amazon EventBridge rule has been created for the main branch to trigger the pipeline.
- B. Check that the CodePipeline service role has permission to access the CodeCommit repository.
- C. Check that the developer’s IAM role has permission to push to the CodeCommit repository.
- D. Check to see if the pipeline failed to start because of CodeCommit errors in Amazon CloudWatch Logs.

> **Suggested Answer:** A
> **Community Vote:** A (61%), B (37%), 3%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**Dushank** (Sun 17 Sep 2023 05:12) - *Upvotes: 21*
A: EventBridge rules are not a requirement for CodePipeline to trigger from a CodeCommit repository. CodePipeline directly integrates with CodeCommit without needing EventBridge.

B: is a likely cause. The CodePipeline service role needs permissions to access the CodeCommit repository in order to start the pipeline execution when new code is pushed.

C:If the developer was able to push code changes to the CodeCommit repository, then their IAM role permissions with respect to CodeCommit are likely fine. This isn't the issue.

D:If the pipeline didn't start, CloudWatch Logs could give insights. However, these logs will only exist if the pipeline actually attempted to start but failed. If the pipeline never started, checking logs won't help.

Given these options, Option B: is the correct answer.

---

**a54b16f** (Mon 15 Jan 2024 14:04) - *Upvotes: 14*
B would throw out "Permission denied" error immediately, rather than no reaction for 10 minutes.

---

**ryuhei** (Wed 13 Aug 2025 07:47) - *Upvotes: 1*
The answer appears to be A.

AWS CodePipeline uses Amazon EventBridge (formerly CloudWatch Events) rules to detect changes in CodeCommit repositories.

Therefore, the solution seems to be to check whether an EventBridge rule is configured to determine the reason for the failure.

To be honest, I thought this was a rather dull problem.

---

**MarcosSantos** (Wed 16 Apr 2025 02:32) - *Upvotes: 2*
Good question! I'm used CodePipeline in a particular lab in last year and now I checked the existence of eventbridge rules created by Codepipeline, impressive.

---

**Srikantha** (Sat 12 Apr 2025 02:24) - *Upvotes: 1*
CodePipeline uses Amazon EventBridge (formerly CloudWatch Events) to trigger a pipeline when changes are pushed to a source like CodeCommit.
If no EventBridge rule is in place (or it's misconfigured), the pipeline won’t be triggered, even if code was pushed to the correct branch.
The rule must match the repository name, branch, and event type (e.g., CodeCommit Repository State Change → referenceUpdated).

---

**spring21** (Mon 23 Dec 2024 19:55) - *Upvotes: 2*
Change detection:
When you set up a CodePipeline with a CodeCommit source, the default behavior is to use an EventBridge rule to detect changes in the repository, eliminating the need for manual polling mechanisms

---

**ZinggieG87** (Wed 18 Dec 2024 20:00) - *Upvotes: 1*
The question states that the developer has pushed the change to the repository, which means CodeCommit has no issue, the pipeline doesn't rely on EvertBridge to trigger.

---

**Serial_X25** (Thu 14 Nov 2024 13:36) - *Upvotes: 1*
CodePipeline connects to third-party source providers directly using CodeConnections, https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-connections.html, but it should use EventBridge for CodeCommit, https://docs.aws.amazon.com/codepipeline/latest/userguide/triggering.html.

---

**Jonalb** (Fri 08 Nov 2024 04:48) - *Upvotes: 1*
A. Check that an Amazon EventBridge rule has been created for the main branch to trigger the pipeline.
This approach directly addresses the most likely cause: a missing or misconfigured EventBridge rule that prevents CodePipeline from starting in response to changes in the CodeCommit repository.

---

**jamesf** (Thu 25 Jul 2024 08:29) - *Upvotes: 3*
A - EventBridge rule is one of the recommended ways to configure CodePipeline to automatically trigger based on changes in a CodeCommit repository

B - if "Permission denied", the error message should prompt immediately, rather than no reaction for 10 minutes for the pipeline. Mean the pipeline not even start

Reference:
https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-about-starting.html#change-detection-methods
https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-simple-codecommit.html

---


<br/>

## Question 17

*Date: April 7, 2023, 3:48 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company's developers use Amazon EC2 instances as remote workstations. The company is concerned that users can create or modify EC2 security groups to allow unrestricted inbound access.
A DevOps engineer needs to develop a solution to detect when users create unrestricted security group rules. The solution must detect changes to security group rules in near real time, remove unrestricted rules, and send email notifications to the security team. The DevOps engineer has created an AWS Lambda function that checks for security group ID from input, removes rules that grant unrestricted access, and sends notifications through Amazon Simple Notification Service (Amazon SNS).
What should the DevOps engineer do next to meet the requirements?

**Options:**
- A. Configure the Lambda function to be invoked by the SNS topic. Create an AWS CloudTrail subscription for the SNS topic. Configure a subscription filter for security group modification events.
- B. Create an Amazon EventBridge scheduled rule to invoke the Lambda function. Define a schedule pattern that runs the Lambda function every hour.
- C. Create an Amazon EventBridge event rule that has the default event bus as the source. Define the rule’s event pattern to match EC2 security group creation and modification events. Configure the rule to invoke the Lambda function.
- D. Create an Amazon EventBridge custom event bus that subscribes to events from all AWS services. Configure the Lambda function to be invoked by the custom event bus.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Sun 28 Jul 2024 14:41) - *Upvotes: 6*
C is correct:
A: lambda should be invoked by Eventbridge
B: we need to act when there is events, not schedully
D: subscribing to events from ALL AWS services incurs a huge cost

---

**01037** (Mon 11 Nov 2024 16:39) - *Upvotes: 1*
C of course.
But A seems working, and does Aws Config work in this situation?

---

**c3518fc** (Sun 10 Nov 2024 23:49) - *Upvotes: 3*
By creating an EventBridge event rule with the appropriate event pattern and configuring it to invoke the Lambda function, the DevOps engineer can effectively detect security group rule changes in near real-time, remove unrestricted rules, and send notifications to the security team. This solution leverages the event-driven architecture of EventBridge and the serverless execution of AWS Lambda, providing a scalable and efficient way to meet the company's security requirements.

---

**meriemheni** (Mon 24 Jun 2024 22:08) - *Upvotes: 2*
selected answer:C

---

**madperro** (Fri 08 Dec 2023 19:29) - *Upvotes: 4*
C the default bus includes events from AWS services.
https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-event-bus.html

---

**bcx** (Thu 30 Nov 2023 17:37) - *Upvotes: 4*
Wrong answers:
A. SNS is used here to send a notification post-facto
B. The question requires "near real time", an hour is not "near real time"
D. AWS events come on the default event bus, you do not need a custom event bus

---

**Aja1** (Tue 06 Feb 2024 13:13) - *Upvotes: 4*
The default event bus in each account receives events from AWS services.

A custom event bus sends events to or receives events from a different account.

A custom event bus sends events to or receives events from a different Region to aggregate events in a single location.

A partner event bus receives events from a SaaS partner.

---

**haazybanj** (Thu 02 Nov 2023 03:18) - *Upvotes: 4*
To meet the requirements, the DevOps engineer should create an Amazon EventBridge event rule that has the default event bus as the source. The rule's event pattern should match EC2 security group creation and modification events, and it should be configured to invoke the Lambda function. This solution will allow for near real-time detection of security group rule changes and will trigger the Lambda function to remove any unrestricted rules and send email notifications to the security team.

---

**alce2020** (Sat 14 Oct 2023 22:23) - *Upvotes: 2*
C is the answer

---

**5aga** (Sat 14 Oct 2023 07:37) - *Upvotes: 4*
C. Create an Amazon EventBridge event rule that has the default event bus as the source. Define the rule’s event pattern to match EC2 security group creation and modification events. Configure the rule to invoke the Lambda function.

The solution requires near real-time detection of changes to security group rules and immediate action to remove unrestricted rules and send email notifications to the security team. The AWS Lambda function created by the DevOps engineer can perform these actions, but it needs to be invoked whenever a security group rule is modified.

Amazon EventBridge is a serverless event bus service that can receive and process events from various AWS services, including Amazon EC2 and Amazon SNS. An EventBridge event rule with the default event bus as the source can be created to match EC2 security group creation and modification events. This rule can then be configured to invoke the Lambda function, which can remove unrestricted rules and send email notifications to the security team.

---


<br/>

## Question 18

*Date: April 5, 2023, 4:09 a.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer is creating an AWS CloudFormation template to deploy a web service. The web service will run on Amazon EC2 instances in a private subnet behind an Application Load Balancer (ALB). The DevOps engineer must ensure that the service can accept requests from clients that have IPv6 addresses.
What should the DevOps engineer do with the CloudFormation template so that IPv6 clients can access the web service?

**Options:**
- A. Add an IPv6 CIDR block to the VPC and the private subnet for the EC2 instances. Create route table entries for the IPv6 network, use EC2 instance types that support IPv6, and assign IPv6 addresses to each EC2 instance.
- B. Assign each EC2 instance an IPv6 Elastic IP address. Create a target group, and add the EC2 instances as targets. Create a listener on port 443 of the ALB, and associate the target group with the ALB.
- C. Replace the ALB with a Network Load Balancer (NLB). Add an IPv6 CIDR block to the VPC and subnets for the NLB, and assign the NLB an IPv6 Elastic IP address.
- D. Add an IPv6 CIDR block to the VPC and subnets for the ALB. Create a listener on port 443. and specify the dualstack IP address type on the ALB. Create a target group, and add the EC2 instances as targets. Associate the target group with the ALB.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**levster** (Mon 22 May 2023 20:36) - *Upvotes: 7*
D
"To support IPv6, configure your Application Load Balancers or Network Load Balancers with the “dualstack” IP address type. This means that clients can communicate with the load balancers using both IPv4 and IPv6 addresses. In a dual-stack IP address type, the DNS name of the load balancer provides both IPv4 and IPv6 addresses, and creates A and AAAA records respectively. "

https://docs.aws.amazon.com/whitepapers/latest/ipv6-on-aws/scaling-the-dual-stack-network-design-in-aws.html

---

**01037** (Sat 11 May 2024 16:09) - *Upvotes: 3*
But why is port 443 necessary?

---

**flaacko** (Fri 16 Aug 2024 14:56) - *Upvotes: 2*
Port 443 is the TCP port for HTTPS which a secured or encrypted version of HTTP. To enable the ALB handle HTTPS traffic having a listener on port 443 is necessary.

---

**c3518fc** (Fri 10 May 2024 22:59) - *Upvotes: 1*
The correct answer is D. Add an IPv6 CIDR block to the VPC and subnets for the ALB. Create a listener on port 443. and specify the dualstack IP address type on the ALB. Create a target group, and add the EC2 instances as targets. Associate the target group with the ALB.

---

**zijo** (Wed 28 Feb 2024 17:29) - *Upvotes: 2*
Why is the need for port 443 reference on D and D has no reference to private subnet. That makes me think the answer is A, but A has no reference to ALB.

---

**thanhnv142** (Sun 28 Jan 2024 15:45) - *Upvotes: 4*
D is correct: use dual stack + listener on 443
A: no mention of the ALB
B: no mention of adding dualstack IP to ALB
C: cannot replace the ALB

---

**sksegha** (Sun 07 Jan 2024 06:34) - *Upvotes: 2*
definitely D

---

**Jamshif01** (Sun 24 Dec 2023 23:25) - *Upvotes: 2*
keyword is "Dualstack"

---

**z_inderjot** (Wed 20 Dec 2023 05:50) - *Upvotes: 3*
D is correct , To enable ALB to deal with Ipv6 requests , vpc should enable for dual stack, by configuring a ipv6 cidr , and ALB subnet should also adhere to the same , by having ipv4 and 6 cidr
B is incorrect , we can assisg any public ip to instance , since it is in private subnet .

---

**madperro** (Thu 08 Jun 2023 18:35) - *Upvotes: 1*
D is the correct answer. C is wrong, we don't need Elastic IPs for a private app.

---


<br/>

## Question 19

*Date: April 5, 2023, 4:26 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses AWS Organizations and AWS Control Tower to manage all the company's AWS accounts. The company uses the Enterprise Support plan.
A DevOps engineer is using Account Factory for Terraform (AFT) to provision new accounts. When new accounts are provisioned, the DevOps engineer notices that the support plan for the new accounts is set to the Basic Support plan. The DevOps engineer needs to implement a solution to provision the new accounts with the Enterprise Support plan.
Which solution will meet these requirements?

**Options:**
- A. Use an AWS Config conformance pack to deploy the account-part-of-organizations AWS Config rule and to automatically remediate any noncompliant accounts.
- B. Create an AWS Lambda function to create a ticket for AWS Support to add the account to the Enterprise Support plan. Grant the Lambda function the support:ResolveCase permission.
- C. Add an additional value to the control_tower_parameters input to set the AWSEnterpriseSupport parameter as the organization's management account number.
- D. Set the aft_feature_enterprise_support feature flag to True in the AFT deployment input configuration. Redeploy AFT and apply the changes.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**z_inderjot** (Thu 20 Jun 2024 05:08) - *Upvotes: 11*
D check docs
https://docs.aws.amazon.com/controltower/latest/userguide/aft-feature-options.html#enterprise-support-option

---

**5aga** (Sat 14 Oct 2023 07:39) - *Upvotes: 9*
D. Set the aft_feature_enterprise_support feature flag to True in the AFT deployment input configuration. Redeploy AFT and apply the changes.

AWS Organizations is a service that helps to manage multiple AWS accounts. AWS Control Tower is a service that makes it easy to set up and govern secure, compliant multi-account AWS environments. Account Factory for Terraform (AFT) is an AWS Control Tower feature that provisions new accounts using Terraform templates.

To provision new accounts with the Enterprise Support plan, the DevOps engineer can set the aft_feature_enterprise_support feature flag to True in the AFT deployment input configuration. This flag enables the Enterprise Support plan for newly provisioned accounts.

---

**life1991** (Fri 18 Apr 2025 06:39) - *Upvotes: 2*
if we use IaC (terraform, CDK), all update must realize in IaC instead of direct in console service

---

**thanhnv142** (Fri 09 Aug 2024 13:08) - *Upvotes: 6*
D is correct: < Account Fachttps://www.examtopics.com/exams/amazon/aws-certified-devops-engineer-professional-dop-c02/view/#tory for Terraform (AFT)> means we need to change AFT config
A: AWS Config conformance pack should be used with SSM automation document to remediate
B and C: irrelevant

---

**madperro** (Fri 08 Dec 2023 19:41) - *Upvotes: 3*
D
https://docs.aws.amazon.com/controltower/latest/userguide/aft-feature-options.html

---

**haazybanj** (Thu 02 Nov 2023 03:30) - *Upvotes: 3*
D. Set the aft_feature_enterprise_support feature flag to True in the AFT deployment input configuration, and then redeploy AFT to apply the changes. This flag is used to enable the Enterprise Support plan for new accounts provisioned by AFT. By default, AFT provisions accounts with the Basic Support plan. Therefore, enabling this flag will provision accounts with the Enterprise Support plan.

---

**alce2020** (Sun 15 Oct 2023 19:11) - *Upvotes: 2*
D it is

---

**ele** (Sat 07 Oct 2023 15:57) - *Upvotes: 1*
D: To enable the Enterprise Support option, set the following feature flag to True in your AFT deployment input configuration.
aft_feature_enterprise_support=true
https://docs.aws.amazon.com/controltower/latest/userguide/aft-feature-options.html

---

**lqpO_Oqpl** (Thu 05 Oct 2023 04:26) - *Upvotes: 1*
Why not C?

---

**bugincloud** (Tue 19 Mar 2024 14:13) - *Upvotes: 1*
check this out https://controltower.aws-management.tools/automation/aft_setup/

---


<br/>

## Question 20

*Date: April 5, 2023, 4:42 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company's DevOps engineer uses AWS Systems Manager to perform maintenance tasks during maintenance windows. The company has a few Amazon EC2 instances that require a restart after notifications from AWS Health. The DevOps engineer needs to implement an automated solution to remediate these notifications. The DevOps engineer creates an Amazon EventBridge rule.
How should the DevOps engineer configure the EventBridge rule to meet these requirements?

**Options:**
- A. Configure an event source of AWS Health, a service of EC2. and an event type that indicates instance maintenance. Target a Systems Manager document to restart the EC2 instance.
- B. Configure an event source of Systems Manager and an event type that indicates a maintenance window. Target a Systems Manager document to restart the EC2 instance.
- C. Configure an event source of AWS Health, a service of EC2, and an event type that indicates instance maintenance. Target a newly created AWS Lambda function that registers an automation task to restart the EC2 instance during a maintenance window.
- D. Configure an event source of EC2 and an event type that indicates instance maintenance. Target a newly created AWS Lambda function that registers an automation task to restart the EC2 instance during a maintenance window.

> **Suggested Answer:** A
> **Community Vote:** A (68%), C (29%), 3%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**MarDog** (Thu 29 Jun 2023 22:33) - *Upvotes: 32*
And AWS Training and Certification has A as the correct answer in the practice exam.

---

**Seoyong** (Sat 19 Aug 2023 00:27) - *Upvotes: 10*
It doesn't need to invoke Lambda.
There is a SSM document , RestartEC2Instance
https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html

---

**VerRi** (Tue 05 Nov 2024 02:27) - *Upvotes: 3*
Bad wording. The SSM document here means the automation document (Runbook), not the Command document. EventBridge + SSM Automation (automation document aka Runbook) is a good practice

---

**jamesf** (Thu 25 Jul 2024 09:00) - *Upvotes: 1*
A
Using SSM document to restart EC2 Instance. Not require to invoke Lambda.

https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html#automating-instance-actions

---

**trungtd** (Sun 07 Jul 2024 10:34) - *Upvotes: 1*
No need to invoke Lambda.

---

**xdkonorek2** (Thu 27 Jun 2024 15:59) - *Upvotes: 1*
I'm hesitant between A and C but I'm voting C
1) SSM document is not a valid target, valid targets for SSM are: Automation, Run Command, OpsItem
2) If company is already using maitenance windows devops engineer should use them instead of restarting instances immediately

---

**flaacko** (Fri 16 Aug 2024 15:19) - *Upvotes: 1*
From the question, the company is already using SSM so there is really no need to create a custom Lambda function. SSM is a valid target action for EventBridge events. You can trigger the running of the AWS-RestartEC2Instance automation document with an EventBridge event which means SSM documents are a valid target.

---

**4bc91ae** (Thu 30 May 2024 11:57) - *Upvotes: 1*
easiest way to do this

---

**Gomer** (Tue 28 May 2024 05:15) - *Upvotes: 1*
SSM Runbook: AWS-RestartEC2Instance (restart one or more EC2 instances)

---

**Gomer** (Tue 28 May 2024 05:26) - *Upvotes: 1*
In reading through some of the responses I think "maintenance windows" (plural) doesn't imply scheduling through Lambda. A DevOps engineer can disable automation during production hours. The scenario is unclear if they want this running all the time, or just enabled to run ONLY in a maintenance window. What I'm sure of is they are wanting the SSM runbook as the answer. In the real world, if productin EC2 instance has a health issue, you might just very well want to reboot it automatically if that truly fixes the problem. Nuff said.

---


<br/>

## Question 21

*Date: April 7, 2023, 5:07 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has containerized all of its in-house quality control applications. The company is running Jenkins on Amazon EC2 instances, which require patching and upgrading. The compliance officer has requested a DevOps engineer begin encrypting build artifacts since they contain company intellectual property.
What should the DevOps engineer do to accomplish this in the MOST maintainable manner?

**Options:**
- A. Automate patching and upgrading using AWS Systems Manager on EC2 instances and encrypt Amazon EBS volumes by default.
- B. Deploy Jenkins to an Amazon ECS cluster and copy build artifacts to an Amazon S3 bucket with default encryption enabled.
- C. Leverage AWS CodePipeline with a build action and encrypt the artifacts using AWS Secrets Manager.
- D. Use AWS CodeBuild with artifact encryption to replace the Jenkins instance running on EC2 instances.

> **Suggested Answer:** D
> **Community Vote:** D (82%), B (18%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**sb333** (Sun 25 Jun 2023 00:52) - *Upvotes: 26*
The question wants you to know which solution is the easiest to maintain. It's important not to get thrown by information provided about their current environment. Only the question they ask matters. The question asks which solution is the easiest to "maintain". The question did not ask whether it would be easy to transition from one solution to another or ask you to leverage containers like other parts of their environment.

As a managed service, AWS CodeBuild does not require patching and upgrading. AWS CodeBuild, using Amazon S3, provides automatic artifact encryption. So this solution is the easiest to maintain of all the solutions listed.

https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html
https://docs.aws.amazon.com/codebuild/latest/userguide/security-encryption.html

---

**madperro** (Thu 08 Jun 2023 19:34) - *Upvotes: 12*
While B will require less changes to the build process I assume AWS is promoting managed services here and expects D answer.

---

**mungdol** (Mon 04 Aug 2025 09:06) - *Upvotes: 1*
why D???????????

---

**MarcosSantos** (Wed 16 Apr 2025 03:18) - *Upvotes: 1*
Option B is the best, provides a solution that improves the maintainability of the Jenkins infrastructure by migrating it to a managed service like ECS, while also addressing the build artifact encryption requirement by utilizing the secure and default-encrypted storage of Amazon S3.

Thinking in your deployment process in jenkins and change to codebuild, can be a difficult work to do. Change only the platform for ec2 to ecs you don't need to sustain patching and updates in ECS. And preserve your artifact on S3.

---

**Ravi_Bulusu** (Sun 17 Nov 2024 16:50) - *Upvotes: 2*
The answer is B
Containerized Jenkins on ECS:
By deploying Jenkins on Amazon ECS (Elastic Container Service), you can leverage containerized environments to easily scale and manage Jenkins. This reduces the operational overhead of patching and upgrading EC2 instances running Jenkins.
Artifact Storage with Encryption:Storing build artifacts in Amazon S3 with default encryption enabled ensures that all files in the bucket are automatically encrypted at rest using either SSE-S3 or SSE-KMS. This complies with the requirement to protect intellectual property by ensuring encryption of artifacts.
This approach ensures a fully managed and scalable solution for both Jenkins (containerized) and the artifact storage, aligning with best practices for security and compliance.

---

**newpotato** (Mon 23 Sep 2024 07:20) - *Upvotes: 2*
while option D could be easier for simple projects or when starting from scratch, it may not be the most maintainable solution for a company that already has a significant investment in Jenkins. Option B provides a balanced approach, leveraging Jenkins' capabilities while improving infrastructure management and security.

---

**HarryLy** (Fri 07 Jun 2024 08:43) - *Upvotes: 1*
AWS codebuild use kms encryption key by default

---

**Gomer** (Wed 29 May 2024 01:38) - *Upvotes: 1*
"D" for me based on sb333's comments, etc.

---

**01037** (Thu 16 May 2024 01:27) - *Upvotes: 2*
D isn't cost effective, but most maintainable

---

**zijo** (Wed 28 Feb 2024 19:07) - *Upvotes: 1*
Answer is D
AWS CodeBuild can be seamlessly integrated with containerized applications deployed on Amazon ECS.
AWS CodeBuild utilizes multiple layers of encryption to safeguard your data at rest, in transit, and during execution.

---


<br/>

## Question 22

*Date: April 5, 2023, 4:53 a.m.
Disclaimers:
- ExamTopics website is not rel*

An IT team has built an AWS CloudFormation template so others in the company can quickly and reliably deploy and terminate an application. The template creates an Amazon EC2 instance with a user data script to install the application and an Amazon S3 bucket that the application uses to serve static webpages while it is running.
All resources should be removed when the CloudFormation stack is deleted. However, the team observes that CloudFormation reports an error during stack deletion, and the S3 bucket created by the stack is not deleted.
How can the team resolve the error in the MOST efficient manner to ensure that all resources are deleted without errors?

**Options:**
- A. Add a DelelionPolicy attribute to the S3 bucket resource, with the value Delete forcing the bucket to be removed when the stack is deleted.
- B. Add a custom resource with an AWS Lambda function with the DependsOn attribute specifying the S3 bucket, and an IAM role. Write the Lambda function to delete all objects from the bucket when RequestType is Delete.
- C. Identify the resource that was not deleted. Manually empty the S3 bucket and then delete it.
- D. Replace the EC2 and S3 bucket resources with a single AWS OpsWorks Stacks resource. Define a custom recipe for the stack to create and delete the EC2 instance and the S3 bucket.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**n_d1** (Fri 15 Dec 2023 22:49) - *Upvotes: 7*
B. As per the AWS DeletionPolicy Options documentation it says, "For Amazon S3 buckets, you must delete all objects in the bucket for deletion to succeed."

https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html

---

**thanhnv142** (Mon 29 Jul 2024 03:37) - *Upvotes: 7*
B is correct:
- Cant delete S3 so must check S3
- There are several DeletionPolition option in ACF: delete, retain, snapshot. For S3, even if there is delete flag, S3 can only be deleted if all objects are removed
A: wrong - add delete flag to deleteionpolicy cant forcing deletion of S3
C: should not manually do the task
D: should not swap to AWS opsworks

---

**HarryLy** (Sat 07 Dec 2024 09:44) - *Upvotes: 2*
Cloudformation does not have any behavior to force delete not empty bucket, need to invoke a custom lambda function to delete it

---

**c3518fc** (Mon 11 Nov 2024 11:00) - *Upvotes: 1*
Keyword "Custom Resource"

---

**madperro** (Fri 08 Dec 2023 20:36) - *Upvotes: 1*
B is a correct answer. A is wrong, you can't delete a bucket that has any objects.

---

**haazybanj** (Thu 02 Nov 2023 03:49) - *Upvotes: 2*
B. Add a custom resource with an AWS Lambda function with the DependsOn attribute specifying the S3 bucket, and an IAM role. Write the Lambda function to delete all objects from the bucket when RequestType is Delete.

---

**alce2020** (Sat 14 Oct 2023 22:45) - *Upvotes: 1*
B is the correct answer

---

**ele** (Sat 07 Oct 2023 17:11) - *Upvotes: 3*
Because it's B. CFN will not delete non-empty bucket. It must be emptied first. Custom resource will do it.

---

**lqpO_Oqpl** (Thu 05 Oct 2023 04:53) - *Upvotes: 1*
Why not A?

---

**tycho** (Tue 24 Oct 2023 16:12) - *Upvotes: 1*
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html
deletion policy seems fine as well ...

---


<br/>

## Question 23

*Date: April 5, 2023, 5:25 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an AWS CodePipeline pipeline that is configured with an Amazon S3 bucket in the eu-west-1 Region. The pipeline deploys an AWS Lambda application to the same Region. The pipeline consists of an AWS CodeBuild project build action and an AWS CloudFormation deploy action.
The CodeBuild project uses the aws cloudformation package AWS CLI command to build an artifact that contains the Lambda function code’s .zip file and the CloudFormation template. The CloudFormation deploy action references the CloudFormation template from the output artifact of the CodeBuild project’s build action.
The company wants to also deploy the Lambda application to the us-east-1 Region by using the pipeline in eu-west-1. A DevOps engineer has already updated the CodeBuild project to use the aws cloudformation package command to produce an additional output artifact for us-east-1.
Which combination of additional steps should the DevOps engineer take to meet these requirements? (Choose two.)

**Options:**
- A. Modify the CloudFormation template to include a parameter for the Lambda function code’s zip file location. Create a new CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to pass in the us-east-1 artifact location as a parameter override.
- B. Create a new CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to use the CloudFormation template from the us-east-1 output artifact.
- C. Create an S3 bucket in us-east-1. Configure the S3 bucket policy to allow CodePipeline to have read and write access.
- D. Create an S3 bucket in us-east-1. Configure S3 Cross-Region Replication (CRR) from the S3 bucket in eu-west-1 to the S3 bucket in us-east-1.
- E. Modify the pipeline to include the S3 bucket for us-east-1 as an artifact store. Create a new CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to use the CloudFormation template from the us-east-1 output artifact.

> **Suggested Answer:** CE
> **Community Vote:** CE (63%), AB (19%), Other, Other, A (35%), C (25%), B (20%), Other

### Discussions

**madperro** (Sat 10 Jun 2023 09:20) - *Upvotes: 19*
As below. You need S# bucket in the new region so C. You need to output artifacts to this new bucket so E.
https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-cross-region.html

---

**0ac7838** (Mon 17 Nov 2025 08:58) - *Upvotes: 1*
Option E = creates & configures the S3 artifact store (automatically).
Option B = adds the CloudFormation deployment that uses that artifact.

These two together are exactly what you need. No manual S3 bucket creation required.

---

**Jonalb** (Thu 10 Jul 2025 12:13) - *Upvotes: 1*
Alternativa A (correta):
Modificar o CloudFormation template para incluir um parâmetro para o zip da Lambda, e passar isso como parâmetro na nova ação para us-east-1.

Isso resolve o problema de apontar para o código .zip armazenado em outro bucket (em outra região).

Você precisa disso quando o template não foi criado para aceitar esses valores como parâmetros.

Altamente recomendado em casos multi-região.
✅ CORRETA

✅ Alternativa B (correta):
Criar uma nova ação de deploy no pipeline para us-east-1 e usar o template do artefato gerado para us-east-1.

Isso é necessário para qualquer deploy multirregional.

Cada região precisa de uma ação de deploy específica para ela.
✅ CORRETA

---

**Priyank1912** (Sun 23 Mar 2025 22:11) - *Upvotes: 2*
B and C are the right option

---

**steli0** (Sun 24 Nov 2024 21:20) - *Upvotes: 1*
Artifact bucket is needed in the deployment region

---

**Ravi_Bulusu** (Sun 17 Nov 2024 16:59) - *Upvotes: 1*
The Answer is : AB
To deploy to a different region, the CloudFormation template should be flexible enough to accept a parameter for the Lambda function’s zip file location. This allows the template to be reused in both regions. The new CloudFormation action in us-east-1 should reference this parameter and pass in the appropriate location of the artifact for that region
After the CodeBuild project outputs artifacts for both eu-west-1 and us-east-1, you need a separate CloudFormation deploy action in the pipeline that targets the us-east-1 region. This action should reference the CloudFormation template from the us-east-1 output artifact produced by the CodeBuild step.

---

**jamesf** (Thu 25 Jul 2024 09:32) - *Upvotes: 1*
CE
Not D because "A DevOps engineer has already updated the CodeBuild project to use the aws cloudformation package command to produce an additional output artifact for us-east-1"

https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-cross-region.html

---

**shammous** (Thu 25 Jul 2024 08:50) - *Upvotes: 1*
C and E are the right answers.

---

**shammous** (Thu 25 Jul 2024 08:51) - *Upvotes: 2*
A: It suggests pointing directly to the Lambda function but we also need the Cloudformation template. So we can rule option A out.
B: Here, we are missing the new region (us-east-1) artifact store where the new Cloudformation deploys action would store the artifacts upon completion.
E: Includes the missing part in option B and is the right answer.
D: "A DevOps engineer has already updated the CodeBuild project to use the AWS CloudFormation package command to produce an additional output artifact for us-east-1." So as we are directly producing the artifact in the S3 bucket in us-east-1, I don't see the point of having a cross-replication.
C: This option is mandatory as we should provide permissions to services (CodePipeline) to access resources (S3 bucket).

---

**ihustle** (Mon 27 May 2024 19:15) - *Upvotes: 1*
B and C are the answers.
The two important things to note here are the use of AWS CLI and artifacts from two different regions.

---


<br/>

## Question 24

*Date: April 8, 2023, 9:14 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company runs an application on one Amazon EC2 instance. Application metadata is stored in Amazon S3 and must be retrieved if the instance is restarted. The instance must restart or relaunch automatically if the instance becomes unresponsive.
Which solution will meet these requirements?

**Options:**
- A. Create an Amazon CloudWatch alarm for the StatusCheckFailed metric. Use the recover action to stop and start the instance. Use an S3 event notification to push the metadata to the instance when the instance is back up and running.
- B. Configure AWS OpsWorks, and use the auto healing feature to stop and start the instance. Use a lifecycle event in OpsWorks to pull the metadata from Amazon S3 and update it on the instance.
- C. Use EC2 Auto Recovery to automatically stop and start the instance in case of a failure. Use an S3 event notification to push the metadata to the instance when the instance is back up and running.
- D. Use AWS CloudFormation to create an EC2 instance that includes the UserData property for the EC2 resource. Add a command in UserData to retrieve the application metadata from Amazon S3.

> **Suggested Answer:** B
> **Community Vote:** B (97%), 3%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Jonfernz** (Sun 10 Sep 2023 22:28) - *Upvotes: 13*
Both Amazon CloudWatch's recover action and EC2 Auto Recovery are designed to respond to system status check failures, not instance status check failures. System status check failures indicate issues with the underlying hardware, while instance status check failures are often related to issues within your instance (like an OS-level issue).

If the requirement is to handle unresponsiveness due to both system-level and instance-level issues, neither option A nor C would fully meet the requirement. In that case, AWS OpsWorks with auto healing (Option B) could be a better fit since OpsWorks allows you to configure more complex health checks and could recover from both system-level and instance-level issues.

So, if you want to handle both types of unresponsiveness, Option B would be the most comprehensive solution.

---

**flaacko** (Fri 16 Aug 2024 15:38) - *Upvotes: 2*
May I add that AWS Opswork offers lifecycle events which you can leverage to execute custom actions on the EC2 instance for example retrieving metadata from S3 as the question requested.

---

**endian675** (Mon 09 Dec 2024 00:15) - *Upvotes: 3*
OpsWorks has now been retired, so don't expect to see this question. However, the answer appears to be B.

A: doesn't make sense because S3 notifications only happen if the S3 objects are modified.
C: same argument as A
D: impossible.

---

**BrusingWayne** (Wed 20 Nov 2024 13:20) - *Upvotes: 1*
Every options are wrong at this moment. Opsworks reached EOL. Other options do not make any sense.

---

**Ravi_Bulusu** (Sun 17 Nov 2024 17:06) - *Upvotes: 2*
The best approach is C, using EC2 Auto Recovery to monitor and recover the instance if it becomes unresponsive, combined with S3 event notifications to ensure the application metadata is properly retrieved after the instance is back online.

---

**HarryLy** (Fri 07 Jun 2024 08:52) - *Upvotes: 1*
B seem correct

---

**Gomer** (Wed 29 May 2024 23:32) - *Upvotes: 2*
Identical with Question #: 102

---

**hoazgazh** (Thu 11 Apr 2024 03:15) - *Upvotes: 1*
To automatic restart, must pull artifact for proactive

---

**thanhnv142** (Mon 29 Jan 2024 08:40) - *Upvotes: 3*
B: is correct: AWS opsworks auto healing will monitor the healthiness of EC2. If there is failure, restart EC2 and pull data from S3 to EC2
A: incorrect because no mention of method to trigger S3 and S3 will not trigger by itself
C: incorrect because no mention of method to trigger S3 and S3 will not trigger by itself
D: Cloud formation only for deploy, this task is about opswork

---

**z_inderjot** (Thu 21 Dec 2023 03:53) - *Upvotes: 3*
OpWorks is deprecated now , So will it be part of exam ? What is the point of learning of service that are not , going to use.

---


<br/>

## Question 25

*Date: April 8, 2023, 9:25 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has multiple AWS accounts. The company uses AWS IAM Identity Center (AWS Single Sign-On) that is integrated with AWS Toolkit for Microsoft Azure DevOps. The attributes for access control feature is enabled in IAM Identity Center.
The attribute mapping list contains two entries. The department key is mapped to ${path:enterprise.department}. The costCenter key is mapped to ${path:enterprise.costCenter}.
All existing Amazon EC2 instances have a department tag that corresponds to three company departments (d1, d2, d3). A DevOps engineer must create policies based on the matching attributes. The policies must minimize administrative effort and must grant each Azure AD user access to only the EC2 instances that are tagged with the user’s respective department name.
Which condition key should the DevOps engineer include in the custom permissions policies to meet these requirements?

**Options:**
- A.
- B.
- C.
- D.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Mon 29 Jan 2024 10:41) - *Upvotes: 7*
C is correct: check the EC2's department tag, if it is the same as user(principaltag)'s department tag, allow access.
A: wrong synxtax, should be StringEquals only
B: we checking the tag of Ec2, not aws.
D: if config like this, every cases will match and everyone can access every EC2, regardless of department

---

**jamesf** (Thu 25 Jul 2024 09:47) - *Upvotes: 1*
C, related with ABAC.

---

**madperro** (Sat 10 Jun 2023 09:35) - *Upvotes: 4*
C, see an example at
https://docs.aws.amazon.com/singlesignon/latest/userguide/configure-abac.html

---

**alce2020** (Fri 14 Apr 2023 23:03) - *Upvotes: 2*
C is the correct answer

---

**ele** (Sat 08 Apr 2023 09:25) - *Upvotes: 4*
https://aws.amazon.com/blogs/aws/new-attributes-based-access-control-with-aws-single-sign-on/

---


<br/>

## Question 26

*Date: April 5, 2023, 10:17 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company hosts a security auditing application in an AWS account. The auditing application uses an IAM role to access other AWS accounts. All the accounts are in the same organization in AWS Organizations.
A recent security audit revealed that users in the audited AWS accounts could modify or delete the auditing application's IAM role. The company needs to prevent any modification to the auditing application's IAM role by any entity other than a trusted administrator IAM role.
Which solution will meet these requirements?

**Options:**
- A. Create an SCP that includes a Deny statement for changes to the auditing application's IAM role. Include a condition that allows the trusted administrator IAM role to make changes. Attach the SCP to the root of the organization.
- B. Create an SCP that includes an Allow statement for changes to the auditing application's IAM role by the trusted administrator IAM role. Include a Deny statement for changes by all other IAM principals. Attach the SCP to the IAM service in each AWS account where the auditing application has an IAM role.
- C. Create an IAM permissions boundary that includes a Deny statement for changes to the auditing application's IAM role. Include a condition that allows the trusted administrator IAM role to make changes. Attach the permissions boundary to the audited AWS accounts.
- D. Create an IAM permissions boundary that includes a Deny statement for changes to the auditing application’s IAM role. Include a condition that allows the trusted administrator IAM role to make changes. Attach the permissions boundary to the auditing application's IAM role in the AWS accounts.

> **Suggested Answer:** A
> **Community Vote:** A (89%), 8%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jqso234** (Sat 14 Oct 2023 19:41) - *Upvotes: 20*
SCPs (Service Control Policies) are the best way to restrict permissions at the organizational level, which in this case would be used to restrict modifications to the IAM role used by the auditing application, while still allowing trusted administrators to make changes to it. Options C and D are not as effective because IAM permission boundaries are applied to IAM entities (users, groups, and roles), not the account itself, and must be applied to all IAM entities in the account.

---

**Serial_X25** (Mon 24 Mar 2025 13:10) - *Upvotes: 1*
A and B are wrong because "SCP never grants permissions", as stated at https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html?icmpid=docs_orgs_console#scp-effects-on-permissions.
C is wrong because you can't attach the permission boundary to an AWS account, only to IAM entities, https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html#access_policies_boundaries-eval-logic.
D is the correct option.

---

**Serial_X25** (Thu 27 Mar 2025 12:11) - *Upvotes: 2*
I'm sorry Folks!
I'm wrong the right option is A and here is the solution:
https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_general.html#example-scp-restricts-with-exception

I thought it was suggesting to add two statements one allow and another deny, but in fact, option A is suggesting to add one Deny with the condition parameter.

---

**4555894** (Sun 08 Sep 2024 13:05) - *Upvotes: 1*
https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html?icmpid=docs_orgs_console

---

**zijo** (Thu 29 Aug 2024 19:39) - *Upvotes: 1*
Service Control Policies (SCPs) in AWS Organizations can be used to enforce maximum permissions for member accounts. They don't directly grant permissions or create permission boundaries. So C & D can be ruled out.

---

**dzn** (Mon 19 Aug 2024 01:58) - *Upvotes: 3*
SCPs are applied at the account or OU level and affect all IAM entities within that organization. IAM Permission boundaries are applied individually to specific IAM roles or users.

---

**thanhnv142** (Mon 05 Aug 2024 13:02) - *Upvotes: 1*
A is correct: < prevent any modification to the auditing application's IAM role> means scp
A: <Include a condition that allows the trusted administrator IAM role> this is not the same as allow statement. So this option still valid
B: SCP does not have allow statement
C and D: These options make modification to permission boundary of the auditing application's IAM role, which is irrelavant. Other accounts may or may not assume this role.

---

**thanhnv142** (Tue 06 Aug 2024 16:07) - *Upvotes: 1*
B is not correct because can only attach scp to AWS org

---

**vn_thanhtung** (Tue 12 Nov 2024 12:43) - *Upvotes: 1*
B wrong because SCP not support principals

---

**flameme** (Wed 27 Mar 2024 15:19) - *Upvotes: 1*
AWS supports permissions boundaries for IAM entities (users or roles)

---


<br/>

## Question 27

*Date: April 8, 2023, 9:51 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an on-premises application that is written in Go. A DevOps engineer must move the application to AWS. The company's development team wants to enable blue/green deployments and perform A/B testing.
Which solution will meet these requirements?

**Options:**
- A. Deploy the application on an Amazon EC2 instance, and create an AMI of the instance. Use the AMI to create an automatic scaling launch configuration that is used in an Auto Scaling group. Use Elastic Load Balancing to distribute traffic. When changes are made to the application, a new AMI will be created, which will initiate an EC2 instance refresh.
- B. Use Amazon Lightsail to deploy the application. Store the application in a zipped format in an Amazon S3 bucket. Use this zipped version to deploy new versions of the application to Lightsail. Use Lightsail deployment options to manage the deployment.
- C. Use AWS CodeArtifact to store the application code. Use AWS CodeDeploy to deploy the application to a fleet of Amazon EC2 instances. Use Elastic Load Balancing to distribute the traffic to the EC2 instances. When making changes to the application, upload a new version to CodeArtifact and create a new CodeDeploy deployment.
- D. Use AWS Elastic Beanstalk to host the application. Store a zipped version of the application in Amazon S3. Use that location to deploy new versions of the application. Use Elastic Beanstalk to manage the deployment options.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**z_inderjot** (Fri 21 Jun 2024 04:08) - *Upvotes: 5*
D is undoubtedly is most correct one . But they shoud mention that , we are going to deploy application in different environment . Since deployming to the same envirnmont just overide the previous deployment . In order to meed the requirement of Blue / Green deployment we need two separate enviormnent . Then we have two separate version running simulateously and we can do DNS swapping to quicky shilf traffic .

---

**zijo** (Thu 29 Aug 2024 21:27) - *Upvotes: 3*
AWS Elastic Beanstalk deploy action can be used to deploy the application artifact from the S3 bucket to the green environment, which is the AWS cloud environment here.

---

**dzn** (Mon 19 Aug 2024 03:44) - *Upvotes: 2*
Lightsail does not have built-in Blue/Green deployment capabilities like Elastic Beanstalk.

---

**Kiroo** (Thu 11 Jan 2024 23:57) - *Upvotes: 3*
I was about to discard D because I was unsure if beanstalk supported GO (yes it does )
https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.platforms.html

So D is undoubtedly the best option to quickly move to cloud and to do blue green with an testing

---

**madperro** (Sun 10 Dec 2023 10:57) - *Upvotes: 2*
I guess D is easiest option to orchestrate blue/green deployments and A/B testing in this case.

---

**rdoty** (Thu 30 Nov 2023 15:08) - *Upvotes: 1*
D elastic beanstalk due to deployment options

---

**mgonblan** (Wed 15 Nov 2023 18:20) - *Upvotes: 1*
Maybe D, but it looks like an old approach. we need to use codebuild and codepipelines and Elastic beanstalk, but elastic beanstalk could be changed by AWS cloudformation.

---

**haazybanj** (Thu 02 Nov 2023 04:08) - *Upvotes: 4*
D. Use AWS Elastic Beanstalk to host the application. Store a zipped version of the application in Amazon S3. Use that location to deploy new versions of the application. Use Elastic Beanstalk to manage the deployment options.

AWS Elastic Beanstalk provides a platform for deploying web applications, which is well-suited for use cases that require blue/green deployments and A/B testing. Elastic Beanstalk can deploy applications written in a variety of programming languages and frameworks, including Go. Elastic Beanstalk supports blue/green deployments, which allow you to deploy a new version of your application to a separate environment before switching traffic to it. This enables you to perform A/B testing before fully rolling out a new version of your application. Elastic Beanstalk also allows you to manage the deployment options, including the deployment strategy, instance types, and autoscaling options.

---

**alce2020** (Sat 14 Oct 2023 23:13) - *Upvotes: 1*
D it is

---

**ele** (Sun 08 Oct 2023 09:51) - *Upvotes: 1*
Elastic Beanstalk

---


<br/>

## Question 28

*Date: April 14, 2023, 11:25 p.m.
Disclaimers:
- ExamTopics website is not rel*

A developer is maintaining a fleet of 50 Amazon EC2 Linux servers. The servers are part of an Amazon EC2 Auto Scaling group, and also use Elastic Load Balancing for load balancing.
Occasionally, some application servers are being terminated after failing ELB HTTP health checks. The developer would like to perform a root cause analysis on the issue, but before being able to access application logs, the server is terminated.
How can log collection be automated?

**Options:**
- A. Use Auto Scaling lifecycle hooks to put instances in a Pending:Wait state. Create an Amazon CloudWatch alarm for EC2 Instance Terminate Successful and trigger an AWS Lambda function that invokes an SSM Run Command script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.
- B. Use Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an AWS Config rule for EC2 Instance-terminate Lifecycle Action and trigger a step function that invokes a script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.
- C. Use Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an Amazon CloudWatch subscription filter for EC2 Instance Terminate Successful and trigger a CloudWatch agent that invokes a script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.
- D. Use Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an Amazon EventBridge rule for EC2 Instance-terminate Lifecycle Action and trigger an AWS Lambda function that invokes an SSM Run Command script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.

> **Suggested Answer:** D
> **Community Vote:** D (87%), 13%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**madperro** (Sat 10 Jun 2023 09:59) - *Upvotes: 11*
D is the easiest solution.

---

**wikn** (Thu 28 Nov 2024 11:31) - *Upvotes: 1*
why C is not correct？

---

**hayjaykay** (Tue 04 Feb 2025 04:06) - *Upvotes: 1*
Think fleet of EC2s, think SSM (systems manager).

---

**Ravi_Bulusu** (Sun 17 Nov 2024 17:24) - *Upvotes: 1*
Option A is the most efficient and straightforward approach to automate log collection and prevent premature termination of EC2 instances by using Auto Scaling lifecycle hooks, CloudWatch alarms, Lambda functions, and SSM to gather and store logs in Amazon S3 before the instance is terminated.

---

**Saudis** (Tue 12 Nov 2024 17:45) - *Upvotes: 1*
it is D not C because CloudWatch agent can not invokes a script

---

**jamesf** (Thu 01 Aug 2024 09:10) - *Upvotes: 2*
D as the EC2 is Terminating and Cloudwatch Agent should be not running and cannot collect the logs

---

**Rahul369** (Fri 21 Jun 2024 06:32) - *Upvotes: 1*
It must be 'C' as CloudWatch Agent will push the logs to a particular CloudWatch log group.

---

**dzn** (Mon 19 Feb 2024 08:44) - *Upvotes: 3*
Terminating:Wait refers to a state in which an instance is determined to be terminated by the Auto Scaling group as part of the termination process and is temporarily put on hold before it is actually terminated. This state pauses the termination process and provides an opportunity to perform custom actions (logging, graceful shutdown, data backup, etc).

---

**hoazgazh** (Thu 11 Apr 2024 03:33) - *Upvotes: 1*
why not C bro

---

**thanhnv142** (Mon 29 Jan 2024 14:51) - *Upvotes: 4*
D is correct: Using Eventbridge in combination with lambda is a common practice.
A: Cloudwatch alarm only alert, no action so it cannot trigger lambda (when this question came out, it could not)
B: AWS config rule cannot triger a script.
C: cloudwatch agent itself does not have any direct action on the host but collecting logs

---


<br/>

## Question 29

*Date: April 14, 2023, 2:41 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an organization in AWS Organizations. The organization includes workload accounts that contain enterprise applications. The company centrally manages users from an operations account. No users can be created in the workload accounts. The company recently added an operations team and must provide the operations team members with administrator access to each workload account.
Which combination of actions will provide this access? (Choose three.)

**Options:**
- A. Create a SysAdmin role in the operations account. Attach the AdministratorAccess policy to the role. Modify the trust relationship to allow the sts:AssumeRole action from the workload accounts.
- B. Create a SysAdmin role in each workload account. Attach the AdministratorAccess policy to the role. Modify the trust relationship to allow the sts:AssumeRole action from the operations account.
- C. Create an Amazon Cognito identity pool in the operations account. Attach the SysAdmin role as an authenticated role.
- D. In the operations account, create an IAM user for each operations team member.
- E. In the operations account, create an IAM user group that is named SysAdmins. Add an IAM policy that allows the sts:AssumeRole action for the SysAdmin role in each workload account. Add all operations team members to the group.
- F. Create an Amazon Cognito user pool in the operations account. Create an Amazon Cognito user for each operations team member.

> **Suggested Answer:** BDE
> **Community Vote:** BDE (84%), Other, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**habros** (Sat 08 Jul 2023 11:04) - *Upvotes: 7*
Any thing Cognito, safe to remove (it is only used for application identity management)
Step 1: Create each role in each workload account. Set trust relationship to only sts:AssumeRole via the operations user in operations account
Step 2: Self explanatory: whatever permission you needs once the user assumed the role
Step 3: Voila

---

**Srikantha** (Sat 29 Mar 2025 21:52) - *Upvotes: 1*
ChatGPT Explanation

---

**jamesf** (Thu 25 Jul 2024 10:29) - *Upvotes: 1*
BDE
Not A - Create SysAdmin role for workload accounts.
Not C F - No Cognito require.

---

**HarryLy** (Fri 07 Jun 2024 09:01) - *Upvotes: 1*
Operation account:
- Need to create a role to assume role in workload account --> E
- Create a group of users can perform assume role --> D
workload account
- Need to create a role with have admin perssion for operation account assume -->B

---

**c3518fc** (Fri 12 Apr 2024 09:20) - *Upvotes: 2*
Not sure why everyone is saying BDE. Why would you create an IAM user for each member and also create for the group? Make it make sense

---

**4555894** (Fri 08 Mar 2024 14:09) - *Upvotes: 1*
https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html

---

**Vitalydt** (Wed 28 Feb 2024 13:59) - *Upvotes: 1*
EBD looks like the best choice

---

**dzn** (Mon 19 Feb 2024 08:56) - *Upvotes: 1*
sts:AssumeRole is one of the AWS Security Token Service (STS) actions used to obtain temporary security credentials and assume the role of another AWS account.

---

**thanhnv142** (Mon 29 Jan 2024 14:58) - *Upvotes: 3*
BDE: No cognito here.
-step 1: create role in workload accounts
-step 2: create IAM user for each member
-step 3: move all member to the group that has permission to assume the role in step 1

---

**madperro** (Sat 10 Jun 2023 10:12) - *Upvotes: 2*
BDE seems to be right.

---


<br/>

## Question 30

*Date: April 5, 2023, 6:30 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has multiple accounts in an organization in AWS Organizations. The company's SecOps team needs to receive an Amazon Simple Notification Service (Amazon SNS) notification if any account in the organization turns off the Block Public Access feature on an Amazon S3 bucket. A DevOps engineer must implement this change without affecting the operation of any AWS accounts. The implementation must ensure that individual member accounts in the organization cannot turn off the notification.
Which solution will meet these requirements?

**Options:**
- A. Designate an account to be the delegated Amazon GuardDuty administrator account. Turn on GuardDuty for all accounts across the organization. In the GuardDuty administrator account, create an SNS topic. Subscribe the SecOps team's email address to the SNS topic. In the same account, create an Amazon EventBridge rule that uses an event pattern for GuardDuty findings and a target of the SNS topic.
- B. Create an AWS CloudFormation template that creates an SNS topic and subscribes the SecOps team’s email address to the SNS topic. In the template, include an Amazon EventBridge rule that uses an event pattern of CloudTrail activity for s3:PutBucketPublicAccessBlock and a target of the SNS topic. Deploy the stack to every account in the organization by using CloudFormation StackSets.
- C. Turn on AWS Config across the organization. In the delegated administrator account, create an SNS topic. Subscribe the SecOps team's email address to the SNS topic. Deploy a conformance pack that uses the s3-bucket-level-public-access-prohibited AWS Config managed rule in each account and uses an AWS Systems Manager document to publish an event to the SNS topic to notify the SecOps team.
- D. Turn on Amazon Inspector across the organization. In the Amazon Inspector delegated administrator account, create an SNS topic. Subscribe the SecOps team’s email address to the SNS topic. In the same account, create an Amazon EventBridge rule that uses an event pattern for public network exposure of the S3 bucket and publishes an event to the SNS topic to notify the SecOps team.

> **Suggested Answer:** C
> **Community Vote:** C (70%), A (26%), 5%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**rlf** (Sat 28 Oct 2023 16:07) - *Upvotes: 12*
Answer is C.
* AWS AWS Systems Manager Automation provides predefined runbooks(ex. AWS-PublishSNSNotification ) for Amazon Simple Notification Service - https://docs.aws.amazon.com/systems-manager-automation-runbooks/latest/userguide/automation-aws-publishsnsnotification.html
* Running automations in multiple AWS Regions and accounts (https://docs.aws.amazon.com/systems-manager/latest/userguide/running-automations-multiple-accounts-regions.html )

B seems to be old approach. With cloudformation stackset, each account can still change resource config (ex. SNS) that causes drift.... so I choose C because it utilize AWS organization fully with aws systems manager automation in multiple regions and multiple accounts with delegated administrator account( or management account )

---

**flaacko** (Fri 16 Aug 2024 20:54) - *Upvotes: 2*
With option B, you will get notifications when user accounts turn off the block public access feature but it doesn't stop them from doing it. The question requires that the implementation stops users from being able to carry out that operation successfully altogether.

---

**Impromptu** (Wed 20 Nov 2024 14:10) - *Upvotes: 1*
Just to go more into detail, as the answer C seems correct indeed. But I'd like to point out some extra details on why B is wrong.
The questions asks that a user cannot turn off the notification. They should be able to turn off the block public access feature. So B is not wrong because it doesn't implement the latter.
B is wrong because it's PutPublicAccessBlock (does not contain "Bucket" in it). And additionally, you should add a condition to the eventbridge rule that checks the content of the action: that BlockPublicPolicy is set to False. Without the condition you will get notification on all PutPublicAccessBlock events, so also those that are considered to be valid.

---

**Impromptu** (Wed 20 Nov 2024 14:15) - *Upvotes: 1*
To bad I can't edit, so to correct myself: PutBucketPublicAccessBlock is indeed the IAM permission and what you should filter on.
And the cloudformation solution in option B also lacks the safeguard to prevent users from disabling the eventbridge rule (and therefore disabling the notification)

---

**Srikantha** (Sun 13 Apr 2025 03:06) - *Upvotes: 2*
Goal: Detect and notify when Block Public Access is turned off on any S3 bucket across all AWS accounts in an organization.
AWS Config tracks resource configuration changes, like changes to S3 bucket access settings.
The s3-bucket-level-public-access-prohibited managed rule evaluates whether S3 buckets have Block Public Access settings enabled.
Conformance packs allow deployment of these rules organization-wide in a managed way.
A Systems Manager automation document can be triggered on noncompliance to publish to an SNS topic for notifications.
Central management via a delegated administrator ensures member accounts can't disable the rule or notification.

---

**Gomer** (Thu 05 Sep 2024 23:13) - *Upvotes: 1*
GuardDuty Policy
Policy:S3/BucketBlockPublicAccessDisabled
"An IAM entity invoked an API used to disable S3 Block Public Access on a bucket."
"Data source: CloudTrail management events"
"This finding informs you that Block Public Access was disabled for the listed S3 bucket. When enabled, S3 Block Public Access settings are used to filter the policies or access control lists (ACLs) applied to buckets as a security measure to prevent inadvertent public exposure of data."
https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types-s3.html#policy-s3-bucketblockpublicaccessdisabled

---

**jamesf** (Thu 25 Jul 2024 10:39) - *Upvotes: 1*
C

"A conformance pack is a collection of AWS Config rules and remediation actions that can be easily deployed as a single entity in an account and a Region or across an organization in AWS Organizations."
https://docs.aws.amazon.com/config/latest/developerguide/conformance-packs.html
https://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html

---

**aefuen1** (Mon 01 Jul 2024 06:07) - *Upvotes: 1*
It's A. GuardDuty echieves this with no effort.

---

**xdkonorek2** (Sat 29 Jun 2024 07:16) - *Upvotes: 2*
A DevOps engineer must implement this change without affecting the operation of any AWS accounts.

---

**Gomer** (Thu 30 May 2024 20:31) - *Upvotes: 1*
I was sure the answer was "C" until I started reading through some of the requirements and comments. The words "implementation must ensure that individual member accounts in the organization cannot turn off the notification" incline me to lean towards "A", because with "C", someone with admin privileges on a single account could turn off the notification in that account. As pointed out by others, there are a number of GuardDuty findings associates with S3 public access. Having GuardDuty and EventBridge pattern trigger SNS for some key words such as "s3" and "Public" seems to make sense in enforcing this across an organization. I don't have enough experience with GuardDuty in an Organization to be 100% confident, but the emphasis on SNS requirement makes me think this could be a trick question.

---


<br/>

## Question 31

*Date: April 5, 2023, 10:25 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has migrated its container-based applications to Amazon EKS and want to establish automated email notifications. The notifications sent to each email address are for specific activities related to EKS components. The solution will include Amazon SNS topics and an AWS Lambda function to evaluate incoming log events and publish messages to the correct SNS topic.
Which logging solution will support these requirements?

**Options:**
- A. Enable Amazon CloudWatch Logs to log the EKS components. Create a CloudWatch subscription filter for each component with Lambda as the subscription feed destination.
- B. Enable Amazon CloudWatch Logs to log the EKS components. Create CloudWatch Logs Insights queries linked to Amazon EventBridge events that invoke Lambda.
- C. Enable Amazon S3 logging for the EKS components. Configure an Amazon CloudWatch subscription filter for each component with Lambda as the subscription feed destination.
- D. Enable Amazon S3 logging for the EKS components. Configure S3 PUT Object event notifications with AWS Lambda as the destination.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**tartarus23** (Tue 19 Dec 2023 23:43) - *Upvotes: 16*
Correct Answer is A.
Explanation:
Amazon EKS integrates with CloudWatch Logs to provide detailed logs of the state and execution of the services in the cluster. CloudWatch subscription filters can be used to route specific log events from a CloudWatch Logs group to a Lambda function. The Lambda function can then process the events and publish notifications to the appropriate Amazon SNS topic.

---

**4555894** (Sun 08 Sep 2024 13:13) - *Upvotes: 3*
https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#LambdaFunctionExample
https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html

---

**zijo** (Thu 05 Sep 2024 18:49) - *Upvotes: 1*
AWS EKS itself does not offer native S3 logging for container logs. CloudWatch Logs Insights queries cannot directly link to Amazon EventBridge events. So the answer here is A.

---

**dzn** (Tue 20 Aug 2024 01:13) - *Upvotes: 1*
CloudWatch Logs subscription filtering is a feature that allows capture log data in real time and forward it to other AWS services such as Kinesis Data Firehose, Kinesis Streams, and Lambda.

---

**thanhnv142** (Mon 29 Jul 2024 16:45) - *Upvotes: 3*
A is correct: Use cloudwatch logs to collect logs from EKS. Use subcription filter to filter out logs and only send relevant logs to lambda to trigger it.
B: CloudWatch Logs Insights is for data analysis. Additionally, using EventBridge events to trigger lambda incur costs
C and D: Amazon S3 logging is used for monitoring actions on S3 itself, not EKS

---

**z_inderjot** (Fri 21 Jun 2024 04:59) - *Upvotes: 4*
A is right
C, D are wrong , because there is not integration in EKS to send logs to s3.
B is for log analysis , and aggreation

---

**madperro** (Fri 15 Dec 2023 15:07) - *Upvotes: 1*
A, metric filter can call Lambda.

---

**rdoty** (Thu 30 Nov 2023 15:18) - *Upvotes: 1*
certainly cloudwatch logs metric filter A

---

**haazybanj** (Thu 02 Nov 2023 04:24) - *Upvotes: 1*
A. Enable Amazon CloudWatch Logs to log the EKS components. Create a CloudWatch subscription filter for each component with Lambda as the subscription feed destination.

This solution involves enabling Amazon CloudWatch Logs to log the EKS components and creating a CloudWatch subscription filter for each component with AWS Lambda as the subscription feed destination. This approach will allow the Lambda function to evaluate incoming log events and publish messages to the correct Amazon SNS topic. Amazon SNS can then send email notifications to each email address based on the messages it receives from the corresponding SNS topic.

---

**ele** (Sun 15 Oct 2023 16:15) - *Upvotes: 1*
A, clear

---


<br/>

## Question 32

*Date: April 5, 2023, 10:32 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is implementing an Amazon Elastic Container Service (Amazon ECS) cluster to run its workload. The company architecture will run multiple ECS services on the cluster. The architecture includes an Application Load Balancer on the front end and uses multiple target groups to route traffic.
A DevOps engineer must collect application and access logs. The DevOps engineer then needs to send the logs to an Amazon S3 bucket for near-real-time analysis.
Which combination of steps must the DevOps engineer take to meet these requirements? (Choose three.)

**Options:**
- A. Download the Amazon CloudWatch Logs container instance from AWS. Configure this instance as a task. Update the application service definitions to include the logging task.
- B. Install the Amazon CloudWatch Logs agent on the ECS instances. Change the logging driver in the ECS task definition to awslogs.
- C. Use Amazon EventBridge to schedule an AWS Lambda function that will run every 60 seconds and will run the Amazon CloudWatch Logs create-export-task command. Then point the output to the logging S3 bucket.
- D. Activate access logging on the ALB. Then point the ALB directly to the logging S3 bucket.
- E. Activate access logging on the target groups that the ECS services use. Then send the logs directly to the logging S3 bucket.
- F. Create an Amazon Kinesis Data Firehose delivery stream that has a destination of the logging S3 bucket. Then create an Amazon CloudWatch Logs subscription filter for Kinesis Data Firehose.

> **Suggested Answer:** BDF
> **Community Vote:** BDF (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**tartarus23** (Mon 19 Jun 2023 22:47) - *Upvotes: 14*
Explanation:

Option B is correct because you can change the logging driver in the ECS task definition to awslogs, which will direct the logs to Amazon CloudWatch Logs. Then, the logs can be forwarded to the Amazon S3 bucket.

Option D is correct because enabling access logging on the Application Load Balancer (ALB) allows the collection of access logs that can be sent directly to an S3 bucket.

Option F is correct because you can create an Amazon Kinesis Data Firehose delivery stream that can deliver logs from CloudWatch Logs directly to an Amazon S3 bucket in near-real-time.

---

**steli0** (Sun 24 Nov 2024 22:11) - *Upvotes: 2*
https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-logging-monitoring.html

---

**4555894** (Fri 08 Mar 2024 14:14) - *Upvotes: 2*
https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-logging-monitoring.html

---

**dzn** (Tue 20 Feb 2024 02:44) - *Upvotes: 1*
Enable access logging using the ALB management console, CLI, or API. Specify the S3 bucket where the logs will be stored and, if necessary, set the log file prefix (e.g., production, staging.) to store the logs in different paths within the bucket.

---

**thanhnv142** (Mon 29 Jan 2024 18:00) - *Upvotes: 4*
BDF: There are two types of log that needs to be collected
B: push app log to Cloudwatch log
D: push access log to S3
F: using Kinesis to push app log from cloudwatch log to S3 in near real-time

A: wrong - we need cloudwatch agent, not container instance
C: No need to use event bridge and lambda to trigger cloudwatch log to push log to s3.
E: access logs lie in ALB, not ECS services.

---

**z_inderjot** (Thu 21 Dec 2023 06:13) - *Upvotes: 2*
BDF is the answer .
btw, can't we use cloudwatch ingists to collet the logs from containers in ecs there days , and then usign the subscription filter we can sends those logs to s3.
without having to install cloud watch agent.

---

**imymoco** (Mon 11 Dec 2023 04:19) - *Upvotes: 1*
Real time. so not E

---

**madperro** (Thu 15 Jun 2023 14:44) - *Upvotes: 1*
BDF makes sense. E is certainly wrong.

---

**bcx** (Tue 30 May 2023 17:54) - *Upvotes: 2*
BDF

Access logs cannot be configured by ALB target group

https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html

---

**hanbj** (Sat 27 May 2023 08:03) - *Upvotes: 1*
Option B sends data to the Cloudwatch Log. This issue requires that logs be collected in S3.

---


<br/>

## Question 33

*Date: April 14, 2023, 8:01 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company that uses electronic health records is running a fleet of Amazon EC2 instances with an Amazon Linux operating system. As part of patient privacy requirements, the company must ensure continuous compliance for patches for operating system and applications running on the EC2 instances.
How can the deployments of the operating system and application patches be automated using a default and custom repository?

**Options:**
- A. Use AWS Systems Manager to create a new patch baseline including the custom repository. Run the AWS-RunPatchBaseline document using the run command to verify and install patches.
- B. Use AWS Direct Connect to integrate the corporate repository and deploy the patches using Amazon CloudWatch scheduled events, then use the CloudWatch dashboard to create reports.
- C. Use yum-config-manager to add the custom repository under /etc/yum.repos.d and run yum-config-manager-enable to activate the repository.
- D. Use AWS Systems Manager to create a new patch baseline including the corporate repository. Run the AWS-AmazonLinuxDefaultPatchBaseline document using the run command to verify and install patches.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**dzn** (Tue 20 Aug 2024 05:45) - *Upvotes: 7*
AWS-AmazonLinuxDefaultPatchBaseline: defines which patches should be applied and which should be avoided.
AWS-RunPatchBaseline: provides commands to actually run the patching process on the instance.

---

**thanhnv142** (Mon 29 Jul 2024 17:14) - *Upvotes: 4*
A is correct: AWS system manager and AWS-RunPatchBaseline to utilize a default and custom repo
B and C are irrelevant
D: AWS-AmazonLinuxDefaultPatchBaseline: this baseline has "default" in its name, it is a predefined baseline and cannot work with a custom repo

---

**davdan99** (Sat 06 Jul 2024 11:42) - *Upvotes: 2*
Here are predefined documents that can not be modified (includes AWS-AmazonLinuxDefaultPatchBaseline)
https://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-predefined-and-custom-patch-baselines.html#patch-manager-baselines-custom
And here is about the AWS-RunPatchBaseline
https://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-aws-runpatchbaseline.html

---

**davdan99** (Sat 06 Jul 2024 11:42) - *Upvotes: 1*
the Answer is A

---

**z_inderjot** (Fri 21 Jun 2024 05:22) - *Upvotes: 3*
I was confused between A and D , i choose A instictinvily, D statement sounds like it only going to install default package for linux not from custom repo we add . But not sure any one can clarify

---

**madperro** (Fri 15 Dec 2023 15:47) - *Upvotes: 4*
A, SSM allows inclusion of custom repositories.

---

**haazybanj** (Thu 02 Nov 2023 05:00) - *Upvotes: 1*
A is it

---

**alce2020** (Sat 14 Oct 2023 23:48) - *Upvotes: 2*
A is correct

---

**jqso234** (Sat 14 Oct 2023 20:01) - *Upvotes: 1*
To automate the deployment of operating system and application patches using a default and custom repository in Amazon EC2 instances with Amazon Linux operating systems, you can use AWS Systems Manager. You can create a new patch baseline in Systems Manager that includes the custom repository, then run the AWS-RunPatchBaseline document using the run command to verify and install patches. This allows you to ensure continuous compliance for patches while also automating the patch deployment process.

---


<br/>

## Question 34

*Date: April 14, 2023, 11:53 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is using AWS CodePipeline to automate its release pipeline. AWS CodeDeploy is being used in the pipeline to deploy an application to Amazon Elastic Container Service (Amazon ECS) using the blue/green deployment model. The company wants to implement scripts to test the green version of the application before shifting traffic. These scripts will complete in 5 minutes or less. If errors are discovered during these tests, the application must be rolled back.
Which strategy will meet these requirements?

**Options:**
- A. Add a stage to the CodePipeline pipeline between the source and deploy stages. Use AWS CodeBuild to create a runtime environment and build commands in the buildspec file to invoke test scripts. If errors are found, use the aws deploy stop-deployment command to stop the deployment.
- B. Add a stage to the CodePipeline pipeline between the source and deploy stages. Use this stage to invoke an AWS Lambda function that will run the test scripts. If errors are found, use the aws deploy stop-deployment command to stop the deployment.
- C. Add a hooks section to the CodeDeploy AppSpec file. Use the AfterAllowTestTraffic lifecycle event to invoke an AWS Lambda function to run the test scripts. If errors are found, exit the Lambda function with an error to initiate rollback.
- D. Add a hooks section to the CodeDeploy AppSpec file. Use the AfterAllowTraffic lifecycle event to invoke the test scripts. If errors are found, use the aws deploy stop-deployment CLI command to stop the deployment.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**haazybanj** (Thu 02 Nov 2023 05:03) - *Upvotes: 10*
Add a hooks section to the CodeDeploy AppSpec file. The AppSpec file is a YAML file that describes how to deploy an application to Amazon ECS using CodeDeploy. We can use the AfterAllowTestTraffic lifecycle event to run the test scripts. This event is triggered after the new version of the application is deployed, and before traffic is shifted to the new version.

In the AfterAllowTestTraffic lifecycle event, invoke an AWS Lambda function to run the test scripts. The Lambda function can be written in any programming language supported by Lambda, such as Python, Node.js, or Java.

If the test scripts detect any errors, exit the Lambda function with an error code. This will cause the deployment to fail, and CodeDeploy will initiate a rollback.

---

**zijo** (Fri 06 Sep 2024 13:47) - *Upvotes: 1*
AfterAllowTestTraffic lifecycle event in the hooks section will not shift the whole traffic to the green application but only a small percentage of traffic to the newly deployed version. C is the answer

---

**dzn** (Tue 20 Aug 2024 07:44) - *Upvotes: 2*
CodeDeploy Blue/Green deployments, the AfterAllowTestTraffic hook is triggered after the test traffic redirection to the new version (Green) is set. Additional verification, testing, or other custom actions can be automated by executing Lambda functions at this time.

---

**thanhnv142** (Mon 29 Jul 2024 17:25) - *Upvotes: 4*
C is correct: we can initiate the script using lambda for advanced features
A and B are wrong: Both trigger the test script befor deploy stages
D is wrong: It only stops the deployment, not rollback it

---

**ixdb** (Tue 13 Feb 2024 11:28) - *Upvotes: 1*
C is right.

---

**madperro** (Fri 15 Dec 2023 16:03) - *Upvotes: 4*
C is the right answer.
https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-ecs

---

**haazybanj** (Thu 02 Nov 2023 05:03) - *Upvotes: 1*
The correct solution to meet these requirements is option C.

Explanation:

In this scenario, the requirement is to add scripts to test the green version of the application before shifting traffic. These scripts should be executed quickly and, in case of errors, the application must be rolled back. To achieve this, we can use the following steps:

---

**ele** (Sun 15 Oct 2023 16:28) - *Upvotes: 1*
Lifecycle event hooks for an Amazon ECS deployment:

AfterAllowTraffic – Use to run tasks after the second target group serves traffic to the replacement task set. The results of a hook function at this lifecycle event can trigger a rollback.

---

**ele** (Sun 15 Oct 2023 16:29) - *Upvotes: 1*
Correction:
AfterAllowTestTraffic – Use to run tasks after the test listener serves traffic to the replacement task set. The results of a hook function at this point can trigger a rollback.

---

**alce2020** (Sat 14 Oct 2023 23:53) - *Upvotes: 1*
C is the correct answer

---


<br/>

## Question 35

*Date: April 14, 2023, 8:03 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses AWS Storage Gateway in file gateway mode in front of an Amazon S3 bucket that is used by multiple resources. In the morning when business begins, users do not see the objects processed by a third party the previous evening. When a DevOps engineer looks directly at the S3 bucket, the data is there, but it is missing in Storage Gateway.
Which solution ensures that all the updated third-party files are available in the morning?

**Options:**
- A. Configure a nightly Amazon EventBridge event to invoke an AWS Lambda function to run the RefreshCache command for Storage Gateway.
- B. Instruct the third party to put data into the S3 bucket using AWS Transfer for SFTP.
- C. Modify Storage Gateway to run in volume gateway mode.
- D. Use S3 Same-Region Replication to replicate any changes made directly in the S3 bucket to Storage Gateway.

> **Suggested Answer:** A
> **Community Vote:** A (97%), 3%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**tartarus23** (Tue 19 Dec 2023 23:49) - *Upvotes: 16*
Explanation:

AWS Storage Gateway's file gateway mode provides a bridge between your on-premises servers and Amazon S3. File gateway caches frequently accessed files in your on-premises environment to provide low-latency access. However, if the S3 bucket's data is modified by another service, the cache does not automatically refresh. Thus, to ensure all the updated third-party files are available in the morning, you can use an AWS Lambda function triggered by Amazon EventBridge to run the RefreshCache command for Storage Gateway. This will ensure the cache is updated with the latest changes.

---

**ele** (Sun 15 Oct 2023 16:34) - *Upvotes: 12*
A: refresh cache: https://repost.aws/knowledge-center/storage-gateway-s3-changes-not-showing

---

**robertohyena** (Wed 15 May 2024 13:11) - *Upvotes: 3*
Thanks for this.
Also found https://repost.aws/knowledge-center/storage-gateway-automate-refreshcache

Storage Gateway allows you to automate the RefreshCache operation based on a Time To Live (TTL) value. TTL is the length of time since the last refresh. When a user accesses the file directory after the TTL value, the file gateway refreshes the directory's contents from the S3 bucket. Valid TTL values for automating the RefreshCache operation range from 300 seconds to 2,592,000 seconds (5 minutes to 30 days).

---

**Gomer** (Sat 30 Nov 2024 06:12) - *Upvotes: 1*
Read and concede:
"Configure an automated cache refresh schedule using AWS Lambda with an Amazon CloudWatch rule"
https://docs.aws.amazon.com/filegateway/latest/files3/refresh-cache.html#auto-refresh-lambda-procedure

---

**bhond** (Thu 15 Feb 2024 16:59) - *Upvotes: 1*
where is it saying files are written directly to s3 ?

---

**yorkicurke** (Thu 23 May 2024 19:22) - *Upvotes: 3*
You do make a point but if you read the phrase " When a DevOps engineer looks directly at the S3 bucket " it kinda implies besides you dont have any other better choice anyway. if you look at user "ele" comments and follow the link below it will get clear[hope that helps];
https://repost.aws/knowledge-center/storage-gateway-s3-changes-not-showing

---

**ixdb** (Tue 13 Feb 2024 12:02) - *Upvotes: 1*
A is right.
Storage Gateway updates the file share cache automatically when you write files to the cache
locally using the file share. However, Storage Gateway doesn't automatically update the cache
when you upload a file directly to Amazon S3. When you do this, you must perform a
RefreshCache operation to see the changes on the file share.

---

**madperro** (Fri 15 Dec 2023 16:10) - *Upvotes: 1*
A is the answer.

---

**haazybanj** (Thu 02 Nov 2023 05:04) - *Upvotes: 2*
The issue appears to be related to the Storage Gateway cache not being updated. To ensure that all the updated third-party files are available in the morning, you can use the RefreshCache API to manually refresh the cache or configure automatic cache refresh.

Option A is a possible solution to configure automatic cache refresh, but it is not necessary to run the RefreshCache command every night if you can ensure that cache refresh occurs frequently enough to meet your requirements.

---

**alce2020** (Sat 14 Oct 2023 23:56) - *Upvotes: 1*
A is correct

---


<br/>

## Question 36

*Date: April 15, 2023, 4:39 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer needs to back up sensitive Amazon S3 objects that are stored within an S3 bucket with a private bucket policy using S3 cross-Region replication functionality. The objects need to be copied to a target bucket in a different AWS Region and account.
Which combination of actions should be performed to enable this replication? (Choose three.)

**Options:**
- A. Create a replication IAM role in the source account
- B. Create a replication I AM role in the target account.
- C. Add statements to the source bucket policy allowing the replication IAM role to replicate objects.
- D. Add statements to the target bucket policy allowing the replication IAM role to replicate objects.
- E. Create a replication rule in the source bucket to enable the replication.
- F. Create a replication rule in the target bucket to enable the replication.

> **Suggested Answer:** ADE
> **Community Vote:** ADE (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**tschenhau** (Sat 11 Nov 2023 06:30) - *Upvotes: 7*
S3 cross-Region replication (CRR) automatically replicates data between buckets across different AWS Regions. To enable CRR, you need to add a replication configuration to your source bucket that specifies the destination bucket, the IAM role, and the encryption type (optional). You also need to grant permissions to the IAM role to perform replication actions on both the source and destination buckets. Additionally, you can choose the destination storage class and enable additional replication options such as S3 Replication Time Control (S3 RTC) or S3 Batch Replication.

---

**Gomer** (Sun 01 Dec 2024 00:55) - *Upvotes: 4*
Tricky question because they are trying to get one to confuse the "enable" replicaton "role"/policy ("rule") in source account with the "allow" replicaton role/"policy" in target account. These references helped me work up some summary steps:
Steps to configure S3 replication between different accounts
1. Create source and destination buckets in different accounts and regions (acctA, acctB)
2. Enable versioning on the buckets (acctA, acctB)
3. Create IAM role and attach a policy granting S3 permission to replicate objects (acctA)
4. Add the replication configuration to source bucket (acctA)
5. Add bucket "policy on the destination bucket to allow" objects replication (acctB)(req. 2nd role)
https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-walkthrough1.html#enable-replication
https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-walkthrough-2.html

---

**Gomer** (Sun 01 Dec 2024 01:09) - *Upvotes: 2*
Source Account: (Source Bucket)(Versioning)(Role/Policy to "Enable" Replicaton)
Target Account: (Target Bucket)(Versioning)(Role/Policy to "Allow" Replicaton)

---

**thanhnv142** (Tue 30 Jul 2024 02:59) - *Upvotes: 2*
ADF is correct: this task is done by S3 itself
A: Create role in the source to allow S3 access permission
D: add policy to allow repication in the target
E: enable replication in the source

---

**bugincloud** (Wed 20 Mar 2024 17:27) - *Upvotes: 2*
ADE make sense.

---

**madperro** (Fri 15 Dec 2023 16:16) - *Upvotes: 2*
ADE make sense.

---

**haazybanj** (Thu 02 Nov 2023 05:10) - *Upvotes: 2*
Confirmed

---

**alce2020** (Mon 16 Oct 2023 03:50) - *Upvotes: 1*
ADE confirmed!

---

**ele** (Sun 15 Oct 2023 16:39) - *Upvotes: 3*
ADE
https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-walkthrough-2.html

---


<br/>

## Question 37

*Date: April 16, 2023, 3:54 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has multiple member accounts that are part of an organization in AWS Organizations. The security team needs to review every Amazon EC2 security group and their inbound and outbound rules. The security team wants to programmatically retrieve this information from the member accounts using an AWS Lambda function in the management account of the organization.
Which combination of access changes will meet these requirements? (Choose three.)

**Options:**
- A. Create a trust relationship that allows users in the member accounts to assume the management account IAM role.
- B. Create a trust relationship that allows users in the management account to assume the IAM roles of the member accounts.
- C. Create an IAM role in each member account that has access to the AmazonEC2ReadOnlyAccess managed policy.
- D. Create an I AM role in each member account to allow the sts:AssumeRole action against the management account IAM role's ARN.
- E. Create an I AM role in the management account that allows the sts:AssumeRole action against the member account IAM role's ARN.
- F. Create an IAM role in the management account that has access to the AmazonEC2ReadOnlyAccess managed policy.

> **Suggested Answer:** BCE
> **Community Vote:** BCE (81%), Other, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**tartarus23** (Tue 19 Dec 2023 23:53) - *Upvotes: 9*
Explanation:

(B) The trust relationship enables an IAM entity (user, group, or role) to assume a role. In this case, the entities in the management account need to assume roles in the member accounts.

(C) The IAM role in each member account should have a policy attached that grants read-only access to EC2 instances. The AmazonEC2ReadOnlyAccess managed policy provides this access.

(E) An IAM role in the management account should be created that has the permission to perform the sts:AssumeRole action against the member account IAM role's ARN. This allows entities assuming this role to switch to the roles in the member accounts and perform actions according to the permissions of those roles.

---

**thanhnv142** (Tue 30 Jul 2024 03:34) - *Upvotes: 4*
BCE are correct:
B: create trust relationship for management to assume role in member accounts
C: create role in member account that has access to AmazoneEC2
E: Create IAM role in management account that allow access to member account IAM role

---

**svjl** (Sun 26 May 2024 16:21) - *Upvotes: 1*
The security team wants to programmatically retrieve this information from the member accounts using an AWS Lambda function in the management account of the organization.

ReadOnlyAccess and option B grant the assumeRole
Besides that the correct resource is "IAM" not "I AM" So BCF is correct

---

**RVivek** (Wed 28 Feb 2024 09:48) - *Upvotes: 1*
B- Member accounts should trust Management account
C- Memeber accounts should have a Role athat has the necessary permission
E- Managment account should have a IAM user account that has stsAssume role permission for the roles created in member accounts

---

**incorrigble_maverick** (Tue 20 Feb 2024 00:00) - *Upvotes: 2*
BCE is wrong. They want to programmatically therefore B is definitenly wrong. The Lambda function IAM Role ARN in the management account needs to be able to assume a role in the member account that has the AmazonEC2ReadOnlyAccess attached to it. Therefore, I will go with C, D, E

---

**zain1258** (Thu 02 May 2024 21:14) - *Upvotes: 1*
D is clearly wrong. You are running your lambda function to get details in management account. The IAM role should be in management account with sts:AssumeRole permission to assume IAM roles in member accounts

---

**DavidPham** (Thu 25 Jan 2024 09:36) - *Upvotes: 1*
BCE correct

---

**madperro** (Fri 15 Dec 2023 16:21) - *Upvotes: 1*
BCE is right.

---

**bcx** (Thu 30 Nov 2023 20:03) - *Upvotes: 1*
B, C and E

---

**PhuocT** (Mon 20 Nov 2023 14:51) - *Upvotes: 2*
B, C and E

---


<br/>

## Question 38

*Date: April 6, 2023, 6:30 p.m.
Disclaimers:
- ExamTopics website is not rel*

A space exploration company receives telemetry data from multiple satellites. Small packets of data are received through Amazon API Gateway and are placed directly into an Amazon Simple Queue Service (Amazon SQS) standard queue. A custom application is subscribed to the queue and transforms the data into a standard format.
Because of inconsistencies in the data that the satellites produce, the application is occasionally unable to transform the data. In these cases, the messages remain in the SQS queue. A DevOps engineer must develop a solution that retains the failed messages and makes them available to scientists for review and future processing.
Which solution will meet these requirements?

**Options:**
- A. Configure AWS Lambda to poll the SQS queue and invoke a Lambda function to check whether the queue messages are valid. If validation fails, send a copy of the data that is not valid to an Amazon S3 bucket so that the scientists can review and correct the data. When the data is corrected, amend the message in the SQS queue by using a replay Lambda function with the corrected data.
- B. Convert the SQS standard queue to an SQS FIFO queue. Configure AWS Lambda to poll the SQS queue every 10 minutes by using an Amazon EventBridge schedule. Invoke the Lambda function to identify any messages with a SentTimestamp value that is older than 5 minutes, push the data to the same location as the application's output location, and remove the messages from the queue.
- C. Create an SQS dead-letter queue. Modify the existing queue by including a redrive policy that sets the Maximum Receives setting to 1 and sets the dead-letter queue ARN to the ARN of the newly created queue. Instruct the scientists to use the dead-letter queue to review the data that is not valid. Reprocess this data at a later time.
- D. Configure API Gateway to send messages to different SQS virtual queues that are named for each of the satellites. Update the application to use a new virtual queue for any data that it cannot transform, and send the message to the new virtual queue. Instruct the scientists to use the virtual queue to review the data that is not valid. Reprocess this data at a later time.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**4555894** (Sun 08 Sep 2024 13:20) - *Upvotes: 2*
Create an SQS dead-letter queue. Modify the existing queue by including a re-drive policy that sets the Maximum Receives setting to 1 and sets the dead-letter queue ARN to the ARN of the newly created queue. Instruct the scientists to use the dead-letter queue to review the data that is not valid. Reprocess this data at a later time.

---

**zijo** (Sat 07 Sep 2024 17:30) - *Upvotes: 1*
Answer is A. Lambda function is required for automated fixing of the invalid message data and hence A is the right choice here.

---

**dzn** (Sun 08 Sep 2024 07:03) - *Upvotes: 1*
This is not a good approach because it requires unifying the validation logic of the custom application and Lambda function, requires updating both the custom application and Lambda when data specifications change, and requires that the timing of those updates be the same from the SQS perspective, making the deployment process more complex and devops cost expensive. BTW, failed messages are reviewed by scientists, and there is no requirement that they be automatically fix by the program.

---

**dzn** (Wed 21 Aug 2024 11:15) - *Upvotes: 2*
A Dead Letter Queue (DLQ) can be the destination queue for messages that cannot be successfully processed by other queues. DLQs are used to analyze why a message failed or to isolate problem messages.

---

**thanhnv142** (Tue 30 Jul 2024 03:37) - *Upvotes: 1*
C is correct: Use dead letter queue and config maximum receives is the right way

---

**Bans** (Sun 07 Jul 2024 18:56) - *Upvotes: 1*
definitely C

---

**harithzainudin** (Mon 10 Jun 2024 11:03) - *Upvotes: 3*
This is DLQ use case. So, its 100% C

---

**SafranboluLokumu** (Sat 01 Jun 2024 12:19) - *Upvotes: 4*
everyone votes C but answer seems as A. which one correct? should we trust to voters or examtopic? :D

---

**xhi158** (Mon 29 Apr 2024 16:38) - *Upvotes: 2*
The correct answer is C . This is a use case for Dead Letter Queue

---

**bugincloud** (Wed 20 Mar 2024 17:41) - *Upvotes: 1*
classic DLQ usecase

---


<br/>

## Question 39

*Date: April 15, 2023, 12:13 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company wants to use AWS CloudFormation for infrastructure deployment. The company has strict tagging and resource requirements and wants to limit the deployment to two Regions. Developers will need to deploy multiple versions of the same application.
Which solution ensures resources are deployed in accordance with company policy?

**Options:**
- A. Create AWS Trusted Advisor checks to find and remediate unapproved CloudFormation StackSets.
- B. Create a Cloud Formation drift detection operation to find and remediate unapproved CloudFormation StackSets.
- C. Create CloudFormation StackSets with approved CloudFormation templates.
- D. Create AWS Service Catalog products with approved CloudFormation templates.

> **Suggested Answer:** D
> **Community Vote:** D (78%), C (22%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**harithzainudin** (Sun 10 Dec 2023 12:07) - *Upvotes: 9*
100% D.

AWS Service Catalog lets you centrally manage your cloud resources to achieve governance at scale of your infrastructure as code (IaC) templates, written in CloudFormation or Terraform configurations. With AWS Service Catalog, you can meet your compliance requirements while making sure your customers can quickly deploy the cloud resources they need.
https://aws.amazon.com/servicecatalog/

all other service in other answer is not related.

---

**flaacko** (Sun 18 Aug 2024 01:59) - *Upvotes: 1*
Service Catalogue is a like a internal marketplace for an organization in that accounts in that organization are limited to using only the resources describe in the product catalogue. For the use case described the best choice is using Service Catalogue.

---

**jamesf** (Thu 01 Aug 2024 09:53) - *Upvotes: 1*
keywords: strict tagging, resource requirements a, limit the deployment
AWS Service Catalog

---

**shammous** (Sat 27 Jul 2024 07:56) - *Upvotes: 1*
D would be a better option, especially for developers, to abstract configuring the CloudFormation StackSets when launching applications with diverse versions. In AWS Service Catalog, they would just pick up the version and deploy. Everything would be set for them in the background including the CloudFormation StackSet with the version parameter and the tagging enforcement.

---

**Gomer** (Sat 01 Jun 2024 03:00) - *Upvotes: 1*
I'd argue that the correct answer is to use Service Catalog and StackSets. Option "D:" doesn't preclude using StackSets, it just doesn't mention it as part of the solution. ServiceCatalog is the formal method to distribute standard solutions (such as CloudFormation StackSets)

---

**Gomer** (Sat 01 Jun 2024 03:55) - *Upvotes: 1*
"AWS Service Catalog enables you to launch a product in one or more accounts and AWS Regions. To do this, administrators must apply a stack set constraint to the product with the accounts and Regions, where it can launch as a stack set."
https://docs.aws.amazon.com/servicecatalog/latest/userguide/launch-stacksets.html

---

**stoy123** (Wed 27 Mar 2024 09:55) - *Upvotes: 1*
"A provisioned Service Catalog product is an AWS CloudFormation stack"
Really confusing. I go with D...

---

**vn_thanhtung** (Thu 02 May 2024 10:17) - *Upvotes: 1*
https://docs.aws.amazon.com/servicecatalog/latest/adminguide/catalogs_constraints_template-constraints.html Please check topic this , correct answer is C

---

**zijo** (Thu 07 Mar 2024 18:46) - *Upvotes: 3*
AWS Service Catalog can be used to deploy resources to two regions (or even more) with the help of AWS CloudFormation StackSets. So Answer is C

---

**Shasha1** (Mon 26 Feb 2024 11:58) - *Upvotes: 3*
Answer C
If rules are applied across multiple accounts, the StackSets feature is more suitable. The service catalog is used for provisioning new accounts under the AWS control tower.

---


<br/>

## Question 40

*Date: April 16, 2023, 3:59 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company requires that its internally facing web application be highly available. The architecture is made up of one Amazon EC2 web server instance and one NAT instance that provides outbound internet access for updates and accessing public data.
Which combination of architecture adjustments should the company implement to achieve high availability? (Choose two.)

**Options:**
- A. Add the NAT instance to an EC2 Auto Scaling group that spans multiple Availability Zones. Update the route tables.
- B. Create additional EC2 instances spanning multiple Availability Zones. Add an Application Load Balancer to split the load between them.
- C. Configure an Application Load Balancer in front of the EC2 instance. Configure Amazon CloudWatch alarms to recover the EC2 instance upon host failure.
- D. Replace the NAT instance with a NAT gateway in each Availability Zone. Update the route tables.
- E. Replace the NAT instance with a NAT gateway that spans multiple Availability Zones. Update the route tables.

> **Suggested Answer:** BD
> **Community Vote:** BD (82%), Other, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Karamen** (Thu 26 Oct 2023 13:38) - *Upvotes: 9*
B&D

NAT Gateway does not span multiple AZ. you must create foreach AZ for HA

---

**HugoFM** (Mon 27 Nov 2023 10:44) - *Upvotes: 9*
BD
E Is incorrect see NAT gateway basics in https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html

---

**kaushald** (Sun 10 Mar 2024 12:22) - *Upvotes: 3*
Quoting the above link: "If you have resources in multiple Availability Zones and they share one NAT gateway, and if the NAT gateway’s Availability Zone is down, resources in the other Availability Zones lose internet access. To improve resiliency, create a NAT gateway in each Availability Zone, and configure your routing to ensure that resources use the NAT gateway in the same Availability Zone."

---

**ryuhei** (Fri 22 Aug 2025 11:11) - *Upvotes: 1*
I think the correct answers are A and E.

The question is a little ambiguous, but if "NAT gateways spanning multiple Availability Zones" means placing a NAT gateway in each AZ, then I think the correct answer is E, not D.

In the first place, I think D still results in a single point of failure, but what do you think?

---

**krishhhhhhhh** (Sun 05 May 2024 22:25) - *Upvotes: 2*
https://aws.amazon.com/blogs/networking-and-content-delivery/using-nat-gateways-with-multiple-amazon-vpcs-at-scale/
NAT Gateways within an AZ are automatically implemented with redundancy. However, while Amazon VPCs can span multiple AZs, each NAT Gateway operates within a single AZ. If the NAT Gateway fails, then connections with resources using that NAT Gateway also fail. Therefore, it's recommended to deploy one NAT Gateway in each AZ and routing traffic locally within the same AZ.

---

**zijo** (Thu 07 Mar 2024 19:49) - *Upvotes: 1*
Both NAT Gateway and NAT instance are regional resources. But NAT Gateway offers automatic deployment across Availability Zones, you might need to manually configure redundancy across Availability Zones for NAT Instances.

---

**thanhnv142** (Tue 30 Jan 2024 07:32) - *Upvotes: 2*
B and D are correct: We need to span EC2 to multiple avai zones and replace nat instance with nat gateway in each zone
B: span EC2 to multiple avai zones
D: replace nat instance with nat gateway

---

**Bans** (Sun 07 Jan 2024 19:49) - *Upvotes: 1*
Answer is B and D

---

**harithzainudin** (Sun 10 Dec 2023 12:12) - *Upvotes: 5*
Asnwer is B and D,
NAT gateways are regional services and do not span across Availability Zones. So, E is completely wrong.

---

**zolthar_z** (Mon 20 Nov 2023 20:49) - *Upvotes: 6*
NAT Gateway can't spans in multiple regions, only in one subnet, I just tried it using the AWS Console

---


<br/>

## Question 41

*Date: April 16, 2023, 4:02 a.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer is building a multistage pipeline with AWS CodePipeline to build, verify, stage, test, and deploy an application. A manual approval stage is required between the test stage and the deploy stage. The development team uses a custom chat tool with webhook support that requires near-real-time notifications.
How should the DevOps engineer configure status updates for pipeline activity and approval requests to post to the chat tool?

**Options:**
- A. Create an Amazon CloudWatch Logs subscription that filters on CodePipeline Pipeline Execution State Change. Publish subscription events to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the chat webhook URL to the SNS topic, and complete the subscription validation.
- B. Create an AWS Lambda function that is invoked by AWS CloudTrail events. When a CodePipeline Pipeline Execution State Change event is detected, send the event details to the chat webhook URL.
- C. Create an Amazon EventBridge rule that filters on CodePipeline Pipeline Execution State Change. Publish the events to an Amazon Simple Notification Service (Amazon SNS) topic. Create an AWS Lambda function that sends event details to the chat webhook URL. Subscribe the function to the SNS topic.
- D. Modify the pipeline code to send the event details to the chat webhook URL at the end of each stage. Parameterize the URL so that each pipeline can send to a different URL based on the pipeline environment.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**haazybanj** (Mon 01 May 2023 05:28) - *Upvotes: 14*
The DevOps engineer should configure status updates for pipeline activity and approval requests to post to the chat tool by creating an Amazon EventBridge rule that filters on CodePipeline Pipeline Execution State Change. The events should be published to an Amazon Simple Notification Service (Amazon SNS) topic, and an AWS Lambda function should be created to send event details to the chat webhook URL. The function should be subscribed to the SNS topic. Option C is the correct answer.

Option A is incorrect because it suggests using CloudWatch Logs instead of EventBridge, which is not the optimal solution for this use case. Option B is incorrect because it suggests using CloudTrail instead of CodePipeline events, which is not relevant. Option D is incorrect because modifying the pipeline code is not necessary and adds unnecessary complexity.

---

**YucelFuat** (Wed 04 Sep 2024 20:43) - *Upvotes: 4*
Exam Tip : If you see that question is related to an Event or Action --> EventBridge

---

**dzn** (Wed 21 Feb 2024 12:58) - *Upvotes: 1*
API calls related to AWS CodePipeline are logged by CloudTrail. However, changes to the execution state of CodePipeline are events, not API calls. These events can be captured via Amazon EventBridge.

---

**thanhnv142** (Tue 30 Jan 2024 08:05) - *Upvotes: 2*
C is correct: Use lambda to send event detail to the chat webhook url. Subcribe lambda to SNS topic
A: Log subscription filer is for logging, not event
B: Should not use lamda with cloudtrail events
D: no need to modify pipeline code to send event at the end of each stage

---

**thanhnv142** (Tue 30 Jan 2024 08:07) - *Upvotes: 1*
cloudtrail event cannot trigger lambda

---

**thanhnv142** (Sun 11 Feb 2024 16:08) - *Upvotes: 1*
A: no way to collect log from code pipeline

---

**madperro** (Thu 15 Jun 2023 15:36) - *Upvotes: 1*
C makes most sene.

---

**ele** (Sat 13 May 2023 11:38) - *Upvotes: 1*
C right

---

**alce2020** (Sun 16 Apr 2023 04:02) - *Upvotes: 2*
C it is

---


<br/>

## Question 42

*Date: April 16, 2023, 4:09 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company's application development team uses Linux-based Amazon EC2 instances as bastion hosts. Inbound SSH access to the bastion hosts is restricted to specific IP addresses, as defined in the associated security groups. The company's security team wants to receive a notification if the security group rules are modified to allow SSH access from any IP address.
What should a DevOps engineer do to meet this requirement?

**Options:**
- A. Create an Amazon EventBridge rule with a source of aws.cloudtrail and the event name AuthorizeSecurityGroupIngress. Define an Amazon Simple Notification Service (Amazon SNS) topic as the target.
- B. Enable Amazon GuardDuty and check the findings for security groups in AWS Security Hub. Configure an Amazon EventBridge rule with a custom pattern that matches GuardDuty events with an output of NON_COMPLIANT. Define an Amazon Simple Notification Service (Amazon SNS) topic as the target.
- C. Create an AWS Config rule by using the restricted-ssh managed rule to check whether security groups disallow unrestricted incoming SSH traffic. Configure automatic remediation to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic.
- D. Enable Amazon Inspector. Include the Common Vulnerabilities and Exposures-1.1 rules package to check the security groups that are associated with the bastion hosts. Configure Amazon Inspector to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic.

> **Suggested Answer:** C
> **Community Vote:** C (69%), A (31%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**ixdb** (Sun 13 Aug 2023 12:33) - *Upvotes: 20*
A is right.
The Config rule restricted-ssh will not check the ingress rule that use the CIDR other than 0.0.0.0/0 and not notify anyone.

---

**GripZA** (Sat 19 Apr 2025 12:38) - *Upvotes: 1*
Exactly why it should be C, not A.

---

**GripZA** (Sat 19 Apr 2025 12:40) - *Upvotes: 2*
Why not A: this could catch changes to security groups, it wouldn't analyze the actual rule content to determine if it's an unrestricted SSH rule (0.0.0.0/0 on port 22).you’d need extra custom logic to parse events and check the CIDR and port range.

---

**csG13** (Tue 12 Dec 2023 21:54) - *Upvotes: 10*
A would send a notification for ANY change in the security group. The question clearly states that wants only when 0.0.0.0/0 is allowed. Therefore, should be C.

---

**hoazgazh** (Sat 13 Apr 2024 11:10) - *Upvotes: 1*
"a notification if the security group rules are modified to allow SSH access from any IP address"
from any IP address => so A is correct, any change in SG should send noti

---

**MarDog** (Mon 19 Jun 2023 20:53) - *Upvotes: 8*
I'm going to have to go with A on this one:
https://aws.plainenglish.io/detecting-modifications-to-aws-ec2-security-groups-2ef8989a3350

https://repost.aws/knowledge-center/monitor-security-group-changes-ec2

---

**syaldram** (Wed 20 Aug 2025 20:44) - *Upvotes: 1*
It is A config managed rule only checks 0.0.0.0/0

---

**5fa1a40** (Sun 20 Jul 2025 00:42) - *Upvotes: 1*
A is right

---

**teo2157** (Tue 21 Jan 2025 16:10) - *Upvotes: 3*
The key point here is "allow SSH access from any IP address" which is exactly "the restricted-ssh managed rule", said that, it's C

---

**teo2157** (Wed 27 Nov 2024 10:58) - *Upvotes: 1*
Very, very, very hard question. I think the key point here is the ANY, based on that, it's C

---


<br/>

## Question 43

*Date: April 16, 2023, 4:16 a.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps team manages an API running on-premises that serves as a backend for an Amazon API Gateway endpoint. Customers have been complaining about high response latencies, which the development team has verified using the API Gateway latency metrics in Amazon CloudWatch. To identify the cause, the team needs to collect relevant data without introducing additional latency.
Which actions should be taken to accomplish this? (Choose two.)

**Options:**
- A. Install the CloudWatch agent server side and configure the agent to upload relevant logs to CloudWatch.
- B. Enable AWS X-Ray tracing in API Gateway, modify the application to capture request segments, and upload those segments to X-Ray during each request.
- C. Enable AWS X-Ray tracing in API Gateway, modify the application to capture request segments, and use the X-Ray daemon to upload segments to X-Ray.
- D. Modify the on-premises application to send log information back to API Gateway with each request.
- E. Modify the on-premises application to calculate and upload statistical data relevant to the API service requests to CloudWatch metrics.

> **Suggested Answer:** AC
> **Community Vote:** AC (88%), 12%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**madperro** (Thu 15 Jun 2023 15:50) - *Upvotes: 8*
AC is using standard parts of the solution.

---

**thanhnv142** (Tue 30 Jan 2024 10:01) - *Upvotes: 6*
A and C: use cloudwatch log agent to collect app log and use AWS X-ray to collect information about requests (traces).
B is incorrect because modifying app to send message directly to X-RAY introduces more latency to the app. Use X-RAY daemon to do that task is a better idea

---

**thanhnv142** (Sun 04 Feb 2024 04:33) - *Upvotes: 2*
- <API Gateway latency metrics> means dev team have collect APM. They need to collect app log as well, which indicates option A.
D and E: modifing the app introduces latencies

---

**jamesf** (Fri 26 Jul 2024 04:48) - *Upvotes: 2*
AC is less impact to Application Latencies.
Keywords: without additional latencies, cloudwatch

B will provide more latencies
DE will require modify the app and give more latencies.

---

**TEC1** (Thu 25 Apr 2024 18:51) - *Upvotes: 3*
The X-Ray daemon batches and uploads the data in the background, which helps to avoid introducing additional latency.

---

**yorkicurke** (Fri 24 Nov 2023 11:07) - *Upvotes: 3*
the reason i am not so sure about is that API Gateway have built-in integration with X-Ray. This means that they automatically send trace data to X-Ray without needing a separate X-Ray daemon. and i dont think we have the option of installing one or using one, unless someone shows me the official link.

---

**Bassel** (Fri 02 Jun 2023 14:24) - *Upvotes: 3*
Installing the CloudWatch agent server-side (option A) is not directly related to collecting latency data from API Gateway. The CloudWatch agent is typically used to collect and monitor system-level metrics from the server itself.

Enabling AWS X-Ray tracing in API Gateway and using the X-Ray daemon (option C) is not necessary in this scenario. The X-Ray daemon is primarily used when you have applications running on EC2 instances or on-premises servers that need to send trace data to X-Ray.

Modifying the on-premises application to send log information back to API Gateway with each request (option D) is not an optimal solution for collecting latency data. It may introduce additional latency and overhead to the API requests and could be challenging to implement efficiently and accurately.

---

**rhinozD** (Tue 13 Jun 2023 16:16) - *Upvotes: 2*
Do you think that doing B or E doesn't bring any latency?
I think C is necessary because you could trace the performance of the application.
And even the team can look into app logs on its server, but sending logs to Cloudwatch logs and then making a further investigation with AWS tools is not too bad.

---

**EricZhang** (Sat 20 May 2023 08:10) - *Upvotes: 2*
Why A? The team still can check logs without uploading to CloudWatch? I'd prefer E over A.

---

**NivNZ** (Sat 29 Jul 2023 01:18) - *Upvotes: 3*
I thought the same but E might cause additional latency which is NOT what we want.

---


<br/>

## Question 44

*Date: April 15, 2023, 6:31 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an application that is using a MySQL-compatible Amazon Aurora Multi-AZ DB cluster as the database. A cross-Region read replica has been created for disaster recovery purposes. A DevOps engineer wants to automate the promotion of the replica so it becomes the primary database instance in the event of a failure.
Which solution will accomplish this?

**Options:**
- A. Configure a latency-based Amazon Route 53 CNAME with health checks so it points to both the primary and replica endpoints. Subscribe an Amazon SNS topic to Amazon RDS failure notifications from AWS CloudTrail and use that topic to invoke an AWS Lambda function that will promote the replica instance as the primary.
- B. Create an Aurora custom endpoint to point to the primary database instance. Configure the application to use this endpoint. Configure AWS CloudTrail to run an AWS Lambda function to promote the replica instance and modify the custom endpoint to point to the newly promoted instance.
- C. Create an AWS Lambda function to modify the application's AWS CloudFormation template to promote the replica, apply the template to update the stack, and point the application to the newly promoted instance. Create an Amazon CloudWatch alarm to invoke this Lambda function after the failure event occurs.
- D. Store the Aurora endpoint in AWS Systems Manager Parameter Store. Create an Amazon EventBridge event that detects the database failure and runs an AWS Lambda function to promote the replica instance and update the endpoint URL stored in AWS Systems Manager Parameter Store. Code the application to reload the endpoint from Parameter Store if a database connection fails.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**haazybanj** (Wed 03 May 2023 03:42) - *Upvotes: 8*
D is the correct answer.

Explanation:

To automate the promotion of a read replica to the primary instance in the event of a failure, we need to detect the failure and then invoke an AWS Lambda function to promote the replica instance. This can be achieved using Amazon EventBridge.

Option A is incorrect because using a CNAME with health checks doesn't provide an automated way to promote the read replica. Additionally, subscribing an Amazon SNS topic to Amazon RDS failure notifications from AWS CloudTrail doesn't help to promote the replica.

Option B is incorrect because a custom endpoint is not required to promote the read replica. Additionally, using AWS CloudTrail to run an AWS Lambda function to promote the replica instance doesn't provide an automated way to update the application endpoint to point to the newly promoted instance.

---

**nlw** (Sat 28 Oct 2023 16:10) - *Upvotes: 7*
doesnt failover happen automatically in aurora?

---

**VrilianVirgil** (Thu 15 Feb 2024 10:26) - *Upvotes: 2*
Aurora supports automated failover for a single cluster. [Be it a global Aurora cluster or a multi AZ/region deployment]
In this case it's implied that the read-replica is not part of the cluster.

that's my best guess.

---

**GripZA** (Sat 19 Apr 2025 12:52) - *Upvotes: 1*
eventbridge detects failure events from RDS.
It triggers a almbda function to:
Promote the cross-region read replica.
Update the endpoint in parameter store.

the app is designed to reload the DB endpoint from parameter store if it detects a connection issues

this supports automated failover with minimal downtime and makes the endpoint configurable rather than hardcoded

---

**jamesf** (Fri 26 Jul 2024 04:57) - *Upvotes: 1*
D is correct.

Option B is wrong as AWS CloudTrail to run an AWS Lambda function to promote the replica instance doesn't provide an automated way.

---

**jamesf** (Thu 01 Aug 2024 10:19) - *Upvotes: 1*
Option B is wrong also due to:
- Custom Endpoint Management: Extra complexity in managing and updating endpoints dynamically.
- Lag in Promotion: Possible delays due to CloudTrail event delivery and Lambda invocation.
- Reliance on CloudTrail: Lag in event processing can cause potential downtime or data inconsistency.

---

**hkh2** (Thu 11 Jul 2024 06:45) - *Upvotes: 1*
Correct answer is B
Here is why.
Previously, you might have used the CNAMES mechanism to set up Domain Name Service (DNS) aliases from your own domain to achieve similar results. By using custom endpoints, you can avoid updating CNAME records when your cluster grows or shrinks. Custom endpoints also mean that you can use encrypted Transport Layer Security/Secure Sockets Layer (TLS/SSL) connections.
https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html#Aurora.Endpoints.Custom

---

**thanhnv142** (Tue 30 Jan 2024 10:38) - *Upvotes: 1*
A is correct: Using Amazon Route 53 CNAME with health checks is the way for failover recommended by AWS: https://aws.amazon.com/blogs/database/cross-region-disaster-recovery-using-amazon-aurora-global-database-for-amazon-aurora-postgresql/

---

**Ffida** (Sun 01 Oct 2023 18:26) - *Upvotes: 1*
option D is not either providing seemless solution, in option D application needed to be reload and that will cause downtime.

---

**madperro** (Thu 15 Jun 2023 16:00) - *Upvotes: 2*
D make most sense.

---


<br/>

## Question 45

*Date: April 8, 2023, 1:43 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company hosts its staging website using an Amazon EC2 instance backed with Amazon EBS storage. The company wants to recover quickly with minimal data losses in the event of network connectivity issues or power failures on the EC2 instance.
Which solution will meet these requirements?

**Options:**
- A. Add the instance to an EC2 Auto Scaling group with the minimum, maximum, and desired capacity set to 1.
- B. Add the instance to an EC2 Auto Scaling group with a lifecycle hook to detach the EBS volume when the EC2 instance shuts down or terminates.
- C. Create an Amazon CloudWatch alarm for the StatusCheckFailed System metric and select the EC2 action to recover the instance.
- D. Create an Amazon CloudWatch alarm for the StatusCheckFailed Instance metric and select the EC2 action to reboot the instance.

> **Suggested Answer:** C
> **Community Vote:** C (95%), 3%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**madperro** (Tue 20 Jun 2023 10:58) - *Upvotes: 11*
C is the right answer.
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html

---

**tartarus23** (Mon 19 Jun 2023 23:03) - *Upvotes: 10*
Explanation:

Amazon CloudWatch provides system-wide visibility into resource utilization, application performance, and operational health. If a system status check fails, this implies there's a problem with the underlying EC2 system that may require AWS involvement to repair. The "Recover this instance" action for the system status check automatically recovers the instance if it becomes impaired due to an underlying issue.

---

**jamesf** (Fri 26 Jul 2024 05:00) - *Upvotes: 2*
C
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html

In the event that AWS determines an instance is unavailable due to an underlying hardware issue, there are two mechanisms that you can configure for instance resiliency which can restore availability—simplified automatic recovery and Amazon CloudWatch action based recovery. This process is called instance recovery.

The following are examples of underlying hardware issues that might require instance recovery:
- Loss of network connectivity
- Loss of system power
- Software issues on the physical host
- Hardware issues on the physical host that impact network reachability

---

**c3518fc** (Fri 12 Apr 2024 17:33) - *Upvotes: 1*
C. This is the correct solution. By creating a CloudWatch alarm for the StatusCheckFailed System metric and configuring the alarm to trigger the "Recover this instance" action, the EC2 instance will be automatically recovered in the event of a system failure or power outage. This ensures the instance can be quickly recovered with minimal data loss, as the EBS volume remains attached during the recovery process.
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html

---

**4555894** (Fri 08 Mar 2024 14:47) - *Upvotes: 1*
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html

---

**thanhnv142** (Tue 30 Jan 2024 10:42) - *Upvotes: 1*
C is correct: recover is the right way
A and B are irrelevant
D: should not reboot in case of power failures

---

**yorkicurke** (Fri 24 Nov 2023 11:23) - *Upvotes: 3*
My Reason:
StatusCheckFailed_System: This check monitors the AWS systems on which your instance runs1. For Example loss of network connectivity, loss of system power, software issues on the physical host, and hardware issues on the physical host that impact network reachability

StatusCheckFailed_Instance: This check monitors the software and network configuration of your individual instance. These checks detect problems that require your involvement to repair. If an instance status check fails, it typically means that there’s an issue with the instance, such as a misconfigured network or a problem with the instance’s file system.

---

**bakamon** (Fri 16 Jun 2023 15:47) - *Upvotes: 2*
Correct Answer is C

---

**qan1257** (Mon 29 May 2023 03:23) - *Upvotes: 4*
A is incorrect.
Simplified automatic recovery is not initiated for instances in an Auto Scaling group. If your instance is part of an Auto Scaling group with health checks enabled, then the instance is replaced when it becomes impaired.
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html

---

**ele** (Sat 13 May 2023 12:02) - *Upvotes: 1*
C with recover action creates identical instance

---


<br/>

## Question 46

*Date: April 15, 2023, 6:22 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company wants to use AWS development tools to replace its current bash deployment scripts. The company currently deploys a LAMP application to a group of Amazon EC2 instances behind an Application Load Balancer (ALB). During the deployments, the company unit tests the committed application, stops and starts services, unregisters and re-registers instances with the load balancer, and updates file permissions. The company wants to maintain the same deployment functionality through the shift to using AWS services.
Which solution will meet these requirements?

**Options:**
- A. Use AWS CodeBuild to test the application. Use bash scripts invoked by AWS CodeDeploy's appspec.yml file to restart services, and deregister and register instances with the ALB. Use the appspec.yml file to update file permissions without a custom script.
- B. Use AWS CodePipeline to move the application from the AWS CodeCommit repository to AWS CodeDeploy. Use CodeDeploy's deployment group to test the application, unregister and re-register instances with the ALand restart services. Use the appspec.yml file to update file permissions without a custom script.
- C. Use AWS CodePipeline to move the application source code from the AWS CodeCommit repository to AWS CodeDeploy. Use CodeDeploy to test the application. Use CodeDeploy's appspec.yml file to restart services and update permissions without a custom script. Use AWS CodeBuild to unregister and re-register instances with the ALB.
- D. Use AWS CodePipeline to trigger AWS CodeBuild to test the application. Use bash scripts invoked by AWS CodeDeploy's appspec.yml file to restart services. Unregister and re-register the instances in the AWS CodeDeploy deployment group with the ALB. Update the appspec.yml file to update file permissions without a custom script.

> **Suggested Answer:** D
> **Community Vote:** D (88%), 8%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**madperro** (Tue 20 Jun 2023 11:04) - *Upvotes: 9*
D is better than A. You need to include CodePipeline to move execution from CodeBuild to CodeDeploy.

---

**haazybanj** (Wed 03 May 2023 03:55) - *Upvotes: 5*
Option D is also a viable solution. It suggests using AWS CodePipeline to trigger AWS CodeBuild to test the application, and then use bash scripts invoked by AWS CodeDeploy's appspec.yml file to restart services, unregister and re-register instances with the ALB, and update file permissions. This approach also covers all the deployment functionality required by the company

---

**92a2133** (Tue 20 May 2025 17:36) - *Upvotes: 1*
Along with everyone else's reasoning the answer is also D because the mention of deployment groups in CodeDeploy, since its multiple instances it needs to be attached to a deployment group instead of just the ALB

---

**GripZA** (Sat 19 Apr 2025 13:12) - *Upvotes: 1*
why not:
a - need some way orchestrating source - build - deploy steps.
b - says codedeploys deployment group does testing — not true. Testing should happen in codebuild
c - suggests using codebuild to unregister/reregister instances — not the right service for that.that’s something codedeploy handles natively via its ALB integration

---

**rk0509** (Thu 15 Aug 2024 05:50) - *Upvotes: 1*
Answer is B. company want to replace its bash deployment scripts so option D is not suitable

---

**SabeloM** (Sat 14 Dec 2024 23:43) - *Upvotes: 1*
Options D is suitable since it includes "Update the appspec.yml file to update file permissions without a custom script".

---

**rk0509** (Thu 15 Aug 2024 05:50) - *Upvotes: 2*
Answer is B. company want to replace its bash deployment scripts so option D is not suitable

---

**jamesf** (Fri 26 Jul 2024 05:09) - *Upvotes: 3*
Should be D

CodePipeline - execute from CodeBuild to CodeDeploy
CodeBuild - test the application
CodeDeploy - deploy app, restart services, Unregister and re-register instance

Not Option A: not using CodePipeline
Not Option BC: using CodeCommit repo, not relevant with question.

---

**zijo** (Thu 21 Mar 2024 16:48) - *Upvotes: 1*
codebuild to test not codedeploy D is correct

---

**thanhnv142** (Tue 30 Jan 2024 10:58) - *Upvotes: 4*
D: is correct: need codepipeline for a seamless deployment. Need codebuild to test and codedeploy to deploy the app on EC2
A: no mention of codepipeline
B and C both mention AWS CodeCommit repository, which is irrelevant

---


<br/>

## Question 47

*Date: April 15, 2023, 6:17 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company runs an application with an Amazon EC2 and on-premises configuration. A DevOps engineer needs to standardize patching across both environments. Company policy dictates that patching only happens during non-business hours.
Which combination of actions will meet these requirements? (Choose three.)

**Options:**
- A. Add the physical machines into AWS Systems Manager using Systems Manager Hybrid Activations.
- B. Attach an IAM role to the EC2 instances, allowing them to be managed by AWS Systems Manager.
- C. Create IAM access keys for the on-premises machines to interact with AWS Systems Manager.
- D. Run an AWS Systems Manager Automation document to patch the systems every hour
- E. Use Amazon EventBridge scheduled events to schedule a patch window.
- F. Use AWS Systems Manager Maintenance Windows to schedule a patch window.

> **Suggested Answer:** ABF
> **Community Vote:** ABF (81%), Other, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Tue 30 Jan 2024 11:04) - *Upvotes: 9*
ABF are the right answers:
A: enable hybrid on AWS system manager
B: create IAM role for System manager to manage EC2 instances
F: use maintenance windows to schedule patching on non-business hours

C: incorrect because there is no IAM access keys for on-prem
D: should not run patching every hour
E: should not use Eventbridge because AWS has its own service to schedule patching

---

**DavidPham** (Tue 25 Jul 2023 09:40) - *Upvotes: 5*
ABF is correct

---

**spring21** (Tue 24 Dec 2024 16:13) - *Upvotes: 2*
To create IAM access keys for on-premises machines to interact with AWS Systems Manager, you need to: create a dedicated IAM user with the necessary permissions for Systems Manager actions, then generate access keys for that user and securely store them on the on-premises machine; ensure you follow best practices like rotating access keys regularly and using a secure method to distribute them.

---

**jamesf** (Fri 26 Jul 2024 05:12) - *Upvotes: 2*
ABF are correct

https://docs.aws.amazon.com/systems-manager/latest/userguide/activations.html

To configure non-EC2 machines for use with AWS Systems Manager in a hybrid and multicloud environment, you create a hybrid activation. Non-EC2 machine types supported as managed nodes include the following:
- Servers on your own premises (on-premises servers)
- AWS IoT Greengrass core devices
- AWS IoT and non-AWS edge devices
- Virtual machines (VMs), including VMs in other cloud environments

---

**HarryLy** (Fri 07 Jun 2024 11:31) - *Upvotes: 1*
ABF is correct

---

**Kiroo** (Sun 16 Jul 2023 21:44) - *Upvotes: 2*
https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html

AF are right but the letter B is wrong the role is for non EC2 instances

---

**madperro** (Tue 20 Jun 2023 11:07) - *Upvotes: 4*
ABF is correct.
https://docs.aws.amazon.com/systems-manager/latest/userguide/activations.html

---

**haazybanj** (Wed 03 May 2023 03:56) - *Upvotes: 3*
ABF is right

---

**alce2020** (Sat 15 Apr 2023 18:17) - *Upvotes: 3*
ABF it is

---


<br/>

## Question 48

*Date: April 15, 2023, 6:11 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has chosen AWS to host a new application. The company needs to implement a multi-account strategy. A DevOps engineer creates a new AWS account and an organization in AWS Organizations. The DevOps engineer also creates the OU structure for the organization and sets up a landing zone by using AWS Control Tower.
The DevOps engineer must implement a solution that automatically deploys resources for new accounts that users create through AWS Control Tower Account Factory. When a user creates a new account, the solution must apply AWS CloudFormation templates and SCPs that are customized for the OU or the account to automatically deploy all the resources that are attached to the account. All the OUs are enrolled in AWS Control Tower.
Which solution will meet these requirements in the MOST automated way?

**Options:**
- A. Use AWS Service Catalog with AWS Control Tower. Create portfolios and products in AWS Service Catalog. Grant granular permissions to provision these resources. Deploy SCPs by using the AWS CLI and JSON documents.
- B. Deploy CloudFormation stack sets by using the required templates. Enable automatic deployment. Deploy stack instances to the required accounts. Deploy a CloudFormation stack set to the organization’s management account to deploy SCPs.
- C. Create an Amazon EventBridge rule to detect the CreateManagedAccount event. Configure AWS Service Catalog as the target to deploy resources to any new accounts. Deploy SCPs by using the AWS CLI and JSON documents.
- D. Deploy the Customizations for AWS Control Tower (CfCT) solution. Use an AWS CodeCommit repository as the source. In the repository, create a custom package that includes the CloudFormation templates and the SCP JSON documents.

> **Suggested Answer:** D
> **Community Vote:** D (92%), 8%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**tartarus23** (Mon 19 Jun 2023 23:14) - *Upvotes: 9*
The CfCT solution is designed for the exact purpose stated in the question. It extends the capabilities of AWS Control Tower by providing you with a way to automate resource provisioning and apply custom configurations across all AWS accounts created in the Control Tower environment. This enables the company to implement additional account customizations when new accounts are provisioned via the Control Tower Account Factory.

The CloudFormation templates and SCPs can be added to a CodeCommit repository and will be automatically deployed to new accounts when they are created. This provides a highly automated solution that does not require manual intervention to deploy resources and SCPs to new accounts.

---

**madperro** (Tue 20 Jun 2023 11:13) - *Upvotes: 6*
CfCT is designed for the purpose stated in the question. So D.
https://docs.aws.amazon.com/controltower/latest/userguide/cfct-overview.html

---

**jamesf** (Fri 26 Jul 2024 05:15) - *Upvotes: 1*
D
https://docs.aws.amazon.com/controltower/latest/userguide/cfct-overview.html
Customizations for AWS Control Tower (CfCT) helps you customize your AWS Control Tower landing zone and stay aligned with AWS best practices. Customizations are implemented with AWS CloudFormation templates and service control policies (SCPs).

---

**jamesf** (Fri 26 Jul 2024 05:17) - *Upvotes: 1*
keywords: "sets up a landing zone by using AWS Control Tower"

---

**Gomer** (Tue 04 Jun 2024 03:00) - *Upvotes: 1*
"This CfCT capability is integrated with AWS Control Tower lifecycle events, so that your resource deployments remain synchronized with your landing zone."
"For example, when a new account is created through account factory, all resources attached to the account are deployed automatically."
"You can deploy the custom templates and policies to individual accounts and organizational units (OUs) within your organization."
https://docs.aws.amazon.com/controltower/latest/userguide/cfct-overview.html

---

**thanhnv142** (Tue 30 Jan 2024 16:32) - *Upvotes: 4*
D is correct: Use CfCT is the correct solution: it utilizes both CloudFormation template and SCP
A and C: no mention of AWS CloudFormation
B: No mention of AWS control tower

---

**khchan123** (Mon 15 Jan 2024 16:35) - *Upvotes: 3*
D. B is wrong because StackSets doesn't deploy stack instances to the organization management account.

---

**Bassel** (Fri 02 Jun 2023 15:11) - *Upvotes: 2*
B. Deploying CloudFormation stack sets is the most automated way to deploy resources for new accounts created through AWS Control Tower Account Factory. With stack sets, you can define a CloudFormation template and deploy it to multiple accounts automatically. By enabling automatic deployment and deploying stack instances to the required accounts, you can ensure that the resources specified in the CloudFormation templates are automatically provisioned for each account. Additionally, by deploying a CloudFormation stack set to the organization's management account, you can deploy Service Control Policies (SCPs) across all accounts in the organization.

---

**youonebe** (Mon 29 May 2023 13:19) - *Upvotes: 2*
Customizations for AWS Control Tower combines AWS Control Tower and other highly-available, trusted AWS services to help customers more quickly set up a secure, multi-account AWS environment using AWS best practices. You can easily add customizations to your AWS Control Tower landing zone using an AWS CloudFormation template and service control policies (SCPs). You can deploy the custom template and policies to individual accounts and organizational units (OUs) within your organization. It also integrates with AWS Control Tower lifecycle events to ensure that resource deployments stay in sync with your landing zone. For example, when a new account is created using the AWS Control Tower account factory, Customizations for AWS Control Tower ensures that all resources attached to the account's OUs will be automatically deployed.

---

**haazybanj** (Mon 01 May 2023 05:59) - *Upvotes: 2*
D is it

---


<br/>

## Question 49

*Date: April 15, 2023, 6:06 p.m.
Disclaimers:
- ExamTopics website is not rel*

An online retail company based in the United States plans to expand its operations to Europe and Asia in the next six months. Its product currently runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. All data is stored in an Amazon Aurora database instance.
When the product is deployed in multiple regions, the company wants a single product catalog across all regions, but for compliance purposes, its customer information and purchases must be kept in each region.
How should the company meet these requirements with the LEAST amount of application changes?

**Options:**
- A. Use Amazon Redshift for the product catalog and Amazon DynamoDB tables for the customer information and purchases.
- B. Use Amazon DynamoDB global tables for the product catalog and regional tables for the customer information and purchases.
- C. Use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer information and purchases.
- D. Use Aurora for the product catalog and Amazon DynamoDB global tables for the customer information and purchases.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Bassel** (Fri 02 Jun 2023 15:15) - *Upvotes: 9*
C. Using Aurora with read replicas for the product catalog allows for a single product catalog across all regions. Aurora read replicas can be set up in different regions to provide low-latency access to the product catalog from each region. Additionally, by deploying additional local Aurora instances in each region for customer information and purchases, the company can comply with the requirement of keeping customer data and purchases in each region.

---

**dark4igi** (Fri 28 Feb 2025 00:14) - *Upvotes: 1*
C. Use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer information and purchases.
It's very bad to use word "local" in answer about AWS

---

**jamesf** (Fri 26 Jul 2024 05:20) - *Upvotes: 3*
C
keywords: ''the LEAST amount of application changes"

---

**Sisanda_giiven** (Thu 22 Feb 2024 04:00) - *Upvotes: 3*
How should the company meet these requirements with the LEAST amount of application changes? Anything option with DynamoDB is out since the all the data is stored Aurora(relational database).

---

**thanhnv142** (Tue 30 Jan 2024 16:56) - *Upvotes: 3*
C is correct: data is kept in each region and one product catalog for all regions
A: Redshift is for data analysis, not for the need in the question
B: DynamoDB is primarily used for session data in a web app
D: Amazon DynamoDB global tables for the customer information is against the policy

---

**amrit1227** (Mon 25 Dec 2023 01:00) - *Upvotes: 1*
C is correct

---

**madperro** (Tue 20 Jun 2023 11:16) - *Upvotes: 2*
C makes most sense and minimizes application changes.

---

**haazybanj** (Mon 01 May 2023 06:02) - *Upvotes: 3*
The best solution to meet the company's requirements with the LEAST amount of application changes is to use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer information and purchases. This will allow for a single product catalog across all regions, while still keeping customer information and purchases in each region for compliance purposes. Amazon Redshift is a data warehousing solution and is not appropriate for this use case. Amazon DynamoDB global tables may be used, but they require application changes to support them. Using local Aurora instances in each region for customer information and purchases could also work, but this would require more configuration and management than using Aurora with read replicas. Therefore, option C is the best solution.

---

**alce2020** (Sat 15 Apr 2023 18:06) - *Upvotes: 3*
C is correct

---


<br/>

## Question 50

*Date: April 15, 2023, 6:04 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is implementing a well-architected design for its globally accessible API stack. The design needs to ensure both high reliability and fast response times for users located in North America and Europe.
The API stack contains the following three tiers:

Amazon API Gateway -

AWS Lambda -

Amazon DynamoDB -
Which solution will meet the requirements?

**Options:**
- A. Configure Amazon Route 53 to point to API Gateway APIs in North America and Europe using health checks. Configure the APIs to forward requests to a Lambda function in that Region. Configure the Lambda functions to retrieve and update the data in a DynamoDB table in the same Region as the Lambda function.
- B. Configure Amazon Route 53 to point to API Gateway APIs in North America and Europe using latency-based routing and health checks. Configure the APIs to forward requests to a Lambda function in that Region. Configure the Lambda functions to retrieve and update the data in a DynamoDB global table.
- C. Configure Amazon Route 53 to point to API Gateway in North America, create a disaster recovery API in Europe, and configure both APIs to forward requests to the Lambda functions in that Region. Retrieve the data from a DynamoDB global table. Deploy a Lambda function to check the North America API health every 5 minutes. In the event of a failure, update Route 53 to point to the disaster recovery API.
- D. Configure Amazon Route 53 to point to API Gateway API in North America using latency-based routing. Configure the API to forward requests to the Lambda function in the Region nearest to the user. Configure the Lambda function to retrieve and update the data in a DynamoDB table.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**haazybanj** (Fri 03 Nov 2023 05:22) - *Upvotes: 12*
B is the correct solution.

The requirement is to ensure both high reliability and fast response times for users located in North America and Europe. To meet this requirement, we can use Amazon Route 53 with latency-based routing to direct users to the closest API Gateway endpoint. Additionally, we can use health checks to monitor the health of each endpoint and direct traffic away from unhealthy endpoints.

To maintain high reliability, we can use AWS Lambda to handle the API requests. Since Lambda scales automatically, we don't need to worry about provisioning or maintaining infrastructure. We can also use DynamoDB as the database since it provides low latency access and automatic scaling.

---

**thanhnv142** (Tue 30 Jul 2024 16:01) - *Upvotes: 3*
B is correct: using both latency-based routing and health checks ensures high reliability and fast response
A: only health check doesnt ensure fast response
C: All traffic would be routed to one location only (either NA or Europe if NA failed)
D: All traffic would be routed to one NA only. There would be no entry point which is near Europe users.

---

**amrit1227** (Tue 25 Jun 2024 00:05) - *Upvotes: 1*
B is correct

---

**z_inderjot** (Sun 23 Jun 2024 03:34) - *Upvotes: 2*
B ,
Using latency based routing for better response time . Having api gateway in each region reduce requrest fligh time. Lambda and Dynamo being a managed serivce scale automatically and having them in same region just reduce latency.

---

**Snape** (Fri 12 Jan 2024 05:44) - *Upvotes: 3*
Reliability is different that resiliency, hence A and C are out as they are focussing on health checks which is required for the resiliency. DR again for the resiliency

---

**madperro** (Wed 20 Dec 2023 12:19) - *Upvotes: 4*
B is the best solution.

---

**alce2020** (Sun 15 Oct 2023 18:04) - *Upvotes: 2*
B is correct

---


<br/>

## Question 51

*Date: April 16, 2023, 10:20 p.m.
Disclaimers:
- ExamTopics website is not rel*

A rapidly growing company wants to scale for developer demand for AWS development environments. Development environments are created manually in the AWS Management Console. The networking team uses AWS CloudFormation to manage the networking infrastructure, exporting stack output values for the Amazon VPC and all subnets. The development environments have common standards, such as Application Load Balancers, Amazon EC2 Auto Scaling groups, security groups, and Amazon DynamoDB tables.
To keep up with demand, the DevOps engineer wants to automate the creation of development environments. Because the infrastructure required to support the application is expected to grow, there must be a way to easily update the deployed infrastructure. CloudFormation will be used to create a template for the development environments.
Which approach will meet these requirements and quickly provide consistent AWS environments for developers?

**Options:**
- A. Use Fn::ImportValue intrinsic functions in the Resources section of the template to retrieve Virtual Private Cloud (VPC) and subnet values. Use CloudFormation StackSets for the development environments, using the Count input parameter to indicate the number of environments needed. Use the UpdateStackSet command to update existing development environments.
- B. Use nested stacks to define common infrastructure components. To access the exported values, use TemplateURL to reference the networking team’s template. To retrieve Virtual Private Cloud (VPC) and subnet values, use Fn::ImportValue intrinsic functions in the Parameters section of the root template. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments.
- C. Use nested stacks to define common infrastructure components. Use Fn::ImportValue intrinsic functions with the resources of the nested stack to retrieve Virtual Private Cloud (VPC) and subnet values. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments.
- D. Use Fn::ImportValue intrinsic functions in the Parameters section of the root template to retrieve Virtual Private Cloud (VPC) and subnet values. Define the development resources in the order they need to be created in the CloudFormation nested stacks. Use the CreateChangeSet. and ExecuteChangeSet commands to update existing development environments.

> **Suggested Answer:** C
> **Community Vote:** C (78%), B (22%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**ipsingh** (Wed 15 Nov 2023 18:56) - *Upvotes: 14*
C is Correct.
B is WRONG because intrinsic functions can't be used in Parameter as per AWS documentation.
https://repost.aws/knowledge-center/cloudformation-template-validation

---

**aksliveswithaws** (Fri 15 Mar 2024 05:11) - *Upvotes: 2*
You can use intrinsic functions only in specific parts of a template. Currently, you can use intrinsic functions in resource properties, outputs, metadata attributes, and update policy attributes
Refer
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html

---

**seetpt** (Fri 01 Nov 2024 19:38) - *Upvotes: 1*
C seems right

---

**thanhnv142** (Wed 31 Jul 2024 06:17) - *Upvotes: 4*
C is correct: use nested stacks and Fn::ImportValue intrinsic functions with the resources of the nested stack
A: no mention of nested stack
B and D: Fn::ImportValue intrinsic function is used on child template to import values from parent template. So it should not be used on root template, which is the universal parent tempalte of all other templates

---

**madperro** (Wed 20 Dec 2023 12:25) - *Upvotes: 2*
C is the best answer. B is wrong as you need to use Fn::ImportValue in Resource section to import CFN template outputs.

---

**ducluanxutrieu** (Wed 20 Dec 2023 10:15) - *Upvotes: 3*
I will go with C

---

**tartarus23** (Wed 20 Dec 2023 00:19) - *Upvotes: 2*
B. Use nested stacks to define common infrastructure components. To access the exported values, use TemplateURL to reference the networking team’s template. To retrieve Virtual Private Cloud (VPC) and subnet values, use Fn::ImportValue intrinsic functions in the Parameters section of the root template. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments.

Nested stacks allow you to modularize and reuse CloudFormation code. For this case, this is helpful because you have common infrastructure components that are shared across environments.

The Fn::ImportValue function is used to import values that have been exported in another stack. Since the networking team exports the VPC and subnet information, this can be used in the CloudFormation stack to reference those values.

---

**sb333** (Mon 29 Jan 2024 07:32) - *Upvotes: 2*
B is incorrect. One of the reasons is that intrinsic functions are not allowed in the Parameters section. https://repost.aws/knowledge-center/cloudformation-template-validation

---

**fanq10** (Wed 21 Feb 2024 13:09) - *Upvotes: 4*
B is WRONG, you cannot use `TemplateURL` to retrieve Network Stack export values.

---

**bakamon** (Sat 16 Dec 2023 16:43) - *Upvotes: 2*
C is the correct answer

---


<br/>

## Question 52

*Date: April 15, 2023, 5:53 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses AWS Organizations to manage multiple accounts. Information security policies require that all unencrypted Amazon EBS volumes be marked as non-compliant. A DevOps engineer needs to automatically deploy the solution and ensure that this compliance check is always present.
Which solution will accomplish this?

**Options:**
- A. Create an AWS CloudFormation template that defines an AWS Inspector rule to check whether EBS encryption is enabled. Save the template to an Amazon S3 bucket that has been shared with all accounts within the company. Update the account creation script pointing to the CloudFormation template in Amazon S3.
- B. Create an AWS Config organizational rule to check whether EBS encryption is enabled and deploy the rule using the AWS CLI. Create and apply an SCP to prohibit stopping and deleting AWS Config across the organization.
- C. Create an SCP in Organizations. Set the policy to prevent the launch of Amazon EC2 instances without encryption on the EBS volumes using a conditional expression. Apply the SCP to all AWS accounts. Use Amazon Athena to analyze the AWS CloudTrail output, looking for events that deny an ec2:RunInstances action.
- D. Deploy an IAM role to all accounts from a single trusted account. Build a pipeline with AWS CodePipeline with a stage in AWS Lambda to assume the IAM role, and list all EBS volumes in the account. Publish a report to Amazon S3.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**YucelFuat** (Wed 04 Sep 2024 21:06) - *Upvotes: 1*
Exam Tip -> Compliance = AWS Config

---

**dzn** (Thu 22 Feb 2024 08:11) - *Upvotes: 4*
Deploy CloudFormation template with encrypted-volumes in the ConfigRuleName property, AWS Config will automatically scan the environment and check for unencrypted EBS volumes.

---

**thanhnv142** (Wed 31 Jan 2024 09:04) - *Upvotes: 1*
B is correct

---

**madperro** (Tue 20 Jun 2023 11:28) - *Upvotes: 3*
B is the only solution meeting the criteria.

---

**haazybanj** (Wed 03 May 2023 04:34) - *Upvotes: 3*
B. Create an AWS Config organizational rule to check whether EBS encryption is enabled and deploy the rule using the AWS CLI. Create and apply an SCP to prohibit stopping and deleting AWS Config across the organization, will accomplish the compliance check on all accounts.

Option A is incorrect because an AWS Inspector rule is used to analyze the behavior of the application on the EC2 instance, not to check the encryption of the EBS volume.

---

**haazybanj** (Mon 01 May 2023 06:13) - *Upvotes: 2*
B is right

---

**alce2020** (Sat 15 Apr 2023 17:53) - *Upvotes: 2*
B is the answer

---


<br/>

## Question 53

*Date: April 6, 2023, 8:03 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is performing vulnerability scanning for all Amazon EC2 instances across many accounts. The accounts are in an organization in AWS Organizations. Each account's VPCs are attached to a shared transit gateway. The VPCs send traffic to the internet through a central egress VPC. The company has enabled Amazon Inspector in a delegated administrator account and has enabled scanning for all member accounts.
A DevOps engineer discovers that some EC2 instances are listed in the "not scanning" tab in Amazon Inspector.
Which combination of actions should the DevOps engineer take to resolve this issue? (Choose three.)

**Options:**
- A. Verify that AWS Systems Manager Agent is installed and is running on the EC2 instances that Amazon Inspector is not scanning.
- B. Associate the target EC2 instances with security groups that allow outbound communication on port 443 to the AWS Systems Manager service endpoint.
- C. Grant inspector:StartAssessmentRun permissions to the IAM role that the DevOps engineer is using.
- D. Configure EC2 Instance Connect for the EC2 instances that Amazon Inspector is not scanning.
- E. Associate the target EC2 instances with instance profiles that grant permissions to communicate with AWS Systems Manager.
- F. Create a managed-instance activation. Use the Activation Code and the Activation ID to register the EC2 instances.

> **Suggested Answer:** ABE
> **Community Vote:** ABE (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Dimidrol** (Fri 06 Oct 2023 20:03) - *Upvotes: 7*
A b e https://docs.aws.amazon.com/inspector/latest/user/scanning-ec2.html

---

**dzn** (Thu 22 Aug 2024 07:54) - *Upvotes: 3*
C is not a fundamental solution. Because Inspector is actually able to run, and it is not the same IAM role that DevOps uses.

---

**thanhnv142** (Wed 31 Jul 2024 09:02) - *Upvotes: 4*
ABE are correct: Check if SSM agent is installed, check connection and permission of Ec2 that allows access to SSM
C: no need to grant inspector:StartAssessmentRun permissions because the dev has already finish the scanning task
D: There is not EC2 instance Connect, only need SSM agent
F: there is no managed-instance activation

---

**yorkicurke** (Sun 09 Jun 2024 12:37) - *Upvotes: 3*
the following link explains it all;
https://repost.aws/knowledge-center/systems-manager-ec2-instance-not-appear

---

**madperro** (Wed 20 Dec 2023 12:57) - *Upvotes: 2*
ABE seem to be prerequisites to work with SSM and Inspector.

---

**bcx** (Thu 30 Nov 2023 09:02) - *Upvotes: 2*
A B E is the correct one IMHO

---

**ParagSanyashiv** (Wed 08 Nov 2023 16:53) - *Upvotes: 2*
ABE makes more sense.

---

**alce2020** (Mon 16 Oct 2023 22:44) - *Upvotes: 3*
A,B,E are correct https://docs.aws.amazon.com/inspector/latest/user/scanning-ec2.html

---

**jqso234** (Sat 14 Oct 2023 22:16) - *Upvotes: 4*
Option C suggests granting inspector:StartAssessmentRun permissions to the IAM role being used by the DevOps engineer. However, this may not be relevant to the issue of instances not being scanned by Amazon Inspector, as the IAM role may already have the necessary permissions by default.

Therefore, A, B, E is a better choice in this case as it includes the necessary steps to ensure that the instances can communicate with AWS Systems Manager, which is required for Amazon Inspector to scan the instances.

---


<br/>

## Question 54

*Date: April 7, 2023, 11:26 a.m.
Disclaimers:
- ExamTopics website is not rel*

A development team uses AWS CodeCommit for version control for applications. The development team uses AWS CodePipeline, AWS CodeBuild. and AWS CodeDeploy for CI/CD infrastructure. In CodeCommit, the development team recently merged pull requests that did not pass long-running tests in the code base. The development team needed to perform rollbacks to branches in the codebase, resulting in lost time and wasted effort.
A DevOps engineer must automate testing of pull requests in CodeCommit to ensure that reviewers more easily see the results of automated tests as part of the pull request review.
What should the DevOps engineer do to meet this requirement?

**Options:**
- A. Create an Amazon EventBridge rule that reacts to the pullRequestStatusChanged event. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild badge as a comment on the pull request so that developers will see the badge in their code review.
- B. Create an Amazon EventBridge rule that reacts to the pullRequestCreated event. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild test results as a comment on the pull request when the test results are complete.
- C. Create an Amazon EventBridge rule that reacts to pullRequestCreated and pullRequestSourceBranchUpdated events. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild badge as a comment on the pull request so that developers will see the badge in their code review.
- D. Create an Amazon EventBridge rule that reacts to the pullRequestStatusChanged event. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild test results as a comment on the pull request when the test results are complete.

> **Suggested Answer:** C
> **Community Vote:** C (65%), B (21%), 14%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**MarDog** (Thu 29 Jun 2023 23:08) - *Upvotes: 16*
C. Look at #3 in the below.
https://container-devsecops.awssecworkshops.com/04-testing/

---

**Gomer** (Thu 06 Jun 2024 01:13) - *Upvotes: 1*
Link is dead

---

**madperro** (Tue 20 Jun 2023 12:03) - *Upvotes: 12*
B, we need to run tests only when pull request is created and we need to publish test results, not only badge.

---

**GripZA** (Sat 19 Apr 2025 14:47) - *Upvotes: 1*
C triggers on both pullRequestCreated and pullRequestSourceBranchUpdated events ensures tests are re-run when a PR is first made or when new commits are pushed to the PR branch. triggers on both creations and updates is just good practice IMO. not handling updated PRs isn't ideal.

why not A: triggers only on pullRequestStatusChanged, which typically means opened/closed/merged — too late to catch new commits pushed to a PR
B Similar to A, but at least uses pullRequestCreated but doesn't handle updated PRs (new commits to the source branch)which is a major gap
D again, pullRequestStatusChanged misses the moment when new code is pushed to the PR

---

**lgallard** (Fri 17 Jan 2025 19:18) - *Upvotes: 1*
C covers both event scenarios: PR creation and update (when updating the source branch, for example, with a new commit.

---

**Gomer** (Thu 06 Jun 2024 01:12) - *Upvotes: 3*
"Automated Code Review on Pull Requests using AWS CodeCommit and AWS CodeBuild"
"The solution comprises of the following components:"
"Amazon EventBridge: AWS service to receive pullRequestCreated and pullRequestSourceBranchUpdated events and trigger Amazon EventBridge rule."
https://aws.amazon.com/blogs/devops/automated-code-review-on-pull-requests-using-aws-codecommit-and-aws-codebuild/

---

**seetpt** (Wed 01 May 2024 18:42) - *Upvotes: 1*
i go with C

---

**zijo** (Fri 22 Mar 2024 12:33) - *Upvotes: 4*
C is the answer to ensure code reviewers more easily see the results of automated tests as part of the pull request review
pullRequestStatusChanged event is triggered whenever the status of a pull request changes. This could include transitions like: Open to Closed (pull request is merged or marked as closed)
Closed to Open (pull request is reopened)
pullRequestCreated event is triggered whenever a new pull request is created in a CodeCommit repository.
pullRequestSourceBranchUpdated event is triggered whenever there are updates (new commits) pushed to the source branch of an open pull request

---

**shammous** (Sun 28 Jul 2024 06:57) - *Upvotes: 1*
I agree the C is the closed correct answer but it doesn't mention pullRequestStatusChanged (not sure why you mention it in your comment).
"The primary events in AWS CodeCommit that can trigger the pipeline are:
- pullRequestCreated: This event occurs when a new pull request is created.
- pullRequestSourceBranchUpdated: This event occurs when the source branch of an existing pull request is updated (e.g. when new commits are pushed to the branch)."
The other events that might be considered but I would exclude are:
- pullRequestStatusChanged: This event occurs when the status of a pull request changes, from closed to open (which we can consider), or from open to close (which we shouldn't consider in our case).
- pullRequestMerged: This event occurs when a pull request is merged. I would exclude it because we are looking to test before merging.

---

**thanhnv142** (Wed 31 Jan 2024 10:34) - *Upvotes: 2*
B is correct: we need to react when there is merge request (pullRequestCreated event)
A: we need to react when there is merge request, not when the status of merge request is changed (pullRequestStatusChanged event)
C: we only need to react when there is merge request, not when a sourcebranch is updated (pullRequestSourceBranchUpdated events)
D: we need to react when there is merge request, not when the status of merge request is changed (pullRequestStatusChanged event)

---


<br/>

## Question 55

*Date: April 15, 2023, 5:24 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has deployed an application in a production VPC in a single AWS account. The application is popular and is experiencing heavy usage. The company’s security team wants to add additional security, such as AWS WAF, to the application deployment. However, the application's product manager is concerned about cost and does not want to approve the change unless the security team can prove that additional security is necessary.
The security team believes that some of the application's demand might come from users that have IP addresses that are on a deny list. The security team provides the deny list to a DevOps engineer. If any of the IP addresses on the deny list access the application, the security team wants to receive automated notification in near real time so that the security team can document that the application needs additional security. The DevOps engineer creates a VPC flow log for the production VPC.
Which set of additional steps should the DevOps engineer take to meet these requirements MOST cost-effectively?

**Options:**
- A. Create a log group in Amazon CloudWatch Logs. Configure the VPC flow log to capture accepted traffic and to send the data to the log group. Create an Amazon CloudWatch metric filter for IP addresses on the deny list. Create a CloudWatch alarm with the metric filter as input. Set the period to 5 minutes and the datapoints to alarm to 1. Use an Amazon Simple Notification Service (Amazon SNS) topic to send alarm notices to the security team.
- B. Create an Amazon S3 bucket for log files. Configure the VPC flow log to capture all traffic and to send the data to the S3 bucket. Configure Amazon Athena to return all log files in the S3 bucket for IP addresses on the deny list. Configure Amazon QuickSight to accept data from Athena and to publish the data as a dashboard that the security team can access. Create a threshold alert of 1 for successful access. Configure the alert to automatically notify the security team as frequently as possible when the alert threshold is met.
- C. Create an Amazon S3 bucket for log files. Configure the VPC flow log to capture accepted traffic and to send the data to the S3 bucket. Configure an Amazon OpenSearch Service cluster and domain for the log files. Create an AWS Lambda function to retrieve the logs from the S3 bucket, format the logs, and load the logs into the OpenSearch Service cluster. Schedule the Lambda function to run every 5 minutes. Configure an alert and condition in OpenSearch Service to send alerts to the security team through an Amazon Simple Notification Service (Amazon SNS) topic when access from the IP addresses on the deny list is detected.
- D. Create a log group in Amazon CloudWatch Logs. Create an Amazon S3 bucket to hold query results. Configure the VPC flow log to capture all traffic and to send the data to the log group. Deploy an Amazon Athena CloudWatch connector in AWS Lambda. Connect the connector to the log group. Configure Athena to periodically query for all accepted traffic from the IP addresses on the deny list and to store the results in the S3 bucket. Configure an S3 event notification to automatically notify the security team through an Amazon Simple Notification Service (Amazon SNS) topic when new objects are added to the S3 bucket.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**madperro** (Wed 20 Dec 2023 13:09) - *Upvotes: 8*
A meets the requirements at the lowest cost.

---

**habros** (Mon 08 Jan 2024 13:52) - *Upvotes: 5*
opensearch cost a lot of $$$. Athena got tons of things to do afterwards. It's used purely for interactive query.

natively, vpcflow sends logs to s3 or cloudwatch logs. no brainer answer

---

**thanhnv142** (Wed 31 Jul 2024 13:37) - *Upvotes: 2*
A is correct: push all VPC flow log to cloudwatch logs. Create metric filter to find denied IP addresses. Create cloudwatch alarm with the metric filter as input. Alarm's action is send noti to Security team via SNS
B: "Configure the alert to automatically notify the security team": alert cannot notify by itself. Must use SNS
C: This option uses both S3 bucket and "Amazon OpenSearch Service cluster" to store log files, which would cost a lot of money and unnecessary
D: This option uses both S3 bucket and VPC flow log to store log files, which is costly

---

**ele** (Mon 13 Nov 2023 16:22) - *Upvotes: 2*
A most cost effective

---

**haazybanj** (Fri 03 Nov 2023 05:55) - *Upvotes: 4*
To meet the requirements most cost-effectively, the DevOps engineer should create a log group in Amazon CloudWatch Logs and configure the VPC flow log to capture accepted traffic and to send the data to the log group. Then, create an Amazon CloudWatch metric filter for IP addresses on the deny list and create a CloudWatch alarm with the metric filter as input. Set the period to 5 minutes and the datapoints to alarm to 1. Finally, use an Amazon Simple Notification Service (Amazon SNS) topic to send alarm notices to the security team.

Option A is the correct answer. It provides a cost-effective solution that meets the requirements. The CloudWatch alarm notifies the security team in near real-time when traffic from an IP address on the deny list is detected. This will help the security team document that the application needs additional security. This solution only requires the use of AWS services that the company is already using, and does not require any additional services or tools.

---

**haazybanj** (Wed 01 Nov 2023 07:22) - *Upvotes: 1*
D. Create an Amazon EventBridge rule that reacts to the pullRequestStatusChanged event. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild test results as a comment on the pull request when the test results are complete.

---

**BaburTurk** (Tue 05 Mar 2024 21:32) - *Upvotes: 1*
Option D does not mention an event bridge rule

---

**zolthar_z** (Wed 22 May 2024 20:02) - *Upvotes: 1*
This answer is for a previous question

---

**haazybanj** (Wed 01 Nov 2023 07:22) - *Upvotes: 2*
The best way to automate testing of pull requests in CodeCommit is to use Amazon EventBridge rules to detect pullRequestStatusChanged events, which are triggered when a pull request's status changes. When this event occurs, the DevOps engineer can create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Finally, program the Lambda function to post the CodeBuild test results as a comment on the pull request when the test results are complete. This approach ensures that reviewers more easily see the results of automated tests as part of the pull request review, without the need to perform manual testing or rollbacks in the codebase.

---

**alce2020** (Sun 15 Oct 2023 17:24) - *Upvotes: 2*
A seems correct

---


<br/>

## Question 56

*Date: April 15, 2023, 5:17 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer has automated a web service deployment by using AWS CodePipeline with the following steps:
1. An AWS CodeBuild project compiles the deployment artifact and runs unit tests.
2. An AWS CodeDeploy deployment group deploys the web service to Amazon EC2 instances in the staging environment.
3. A CodeDeploy deployment group deploys the web service to EC2 instances in the production environment.
The quality assurance (QA) team requests permission to inspect the build artifact before the deployment to the production environment occurs. The QA team wants to run an internal penetration testing tool to conduct manual tests. The tool will be invoked by a REST API call.
Which combination of actions should the DevOps engineer take to fulfill this request? (Choose two.)

**Options:**
- A. Insert a manual approval action between the test actions and deployment actions of the pipeline.
- B. Modify the buildspec.yml file for the compilation stage to require manual approval before completion.
- C. Update the CodeDeploy deployment groups so that they require manual approval to proceed.
- D. Update the pipeline to directly call the REST API for the penetration testing tool.
- E. Update the pipeline to invoke an AWS Lambda function that calls the REST API for the penetration testing tool.

> **Suggested Answer:** AE
> **Community Vote:** AE (81%), AD (19%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**tartarus23** (Mon 19 Jun 2023 23:26) - *Upvotes: 8*
Explanation:
The manual approval action (A) will allow the QA team to inspect the build artifact and run their internal penetration testing tool before the deployment to the production environment proceeds.

Using an AWS Lambda function (E) would provide an automated way to call the REST API of the penetration testing tool. This would allow for the tests to be conducted automatically within the pipeline. This is beneficial because it ensures consistency in the testing process and could be run programmatically, reducing manual steps.

---

**Jonalb** (Tue 22 Jul 2025 11:04) - *Upvotes: 2*
A E there is no way to call REST API directly in the code pipeline, it is possible invoke via Lambda function only

---

**iulian0585** (Tue 06 Aug 2024 11:02) - *Upvotes: 2*
Option D (updating the pipeline to directly call the REST API for the penetration testing tool) is not recommended because it tightly couples the pipeline with the QA team's tool, making it less flexible and harder to maintain. Using a Lambda function as an intermediary provides better separation of concerns and easier maintainability.

---

**jamesf** (Fri 26 Jul 2024 06:57) - *Upvotes: 1*
Should be AE
Although there are limitation 15mins of Lambda function.
But Option D is wrong as CodePipeline does not have the ability to execute HTTP requests "directly".

https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-invoke-lambda-function.html

---

**jamesf** (Thu 01 Aug 2024 11:02) - *Upvotes: 1*
For option A, keywords: conduct manual tests

---

**zijo** (Fri 22 Mar 2024 16:51) - *Upvotes: 1*
This is tricky but AD should be a better choice because of the 15 min timeout of Lambda functions. To call REST API in CodePipeline these are the two options
For complex API calls, security requirements, and access to external resources, an AWS Lambda function is the recommended approach.
For simple API calls with limited requirements, consider the inline script approach within CodeBuild, but with caution due to security and maintainability limitations.

---

**Shasha1** (Wed 28 Feb 2024 12:14) - *Upvotes: 4*
AE there is no way to call REST API directly in the code pipeline, it is possible invoke via Lambda function only

---

**dzn** (Thu 22 Feb 2024 10:37) - *Upvotes: 3*
CodePipeline does not have the ability to execute HTTP requests "directly".

---

**thanhnv142** (Wed 31 Jan 2024 14:48) - *Upvotes: 1*
A and D are correct: a manual approval action between the test actions and deployment actions allows tester to verify and test built artifacts before allowing deploying to production
B and C: no mentions of test and deployment env
E: manual test take more than 15 minutes, which is the maximum execution time of lambda

---

**a54b16f** (Mon 15 Jan 2024 16:21) - *Upvotes: 2*
D is wrong, alternative option ( not using Lambda, for example, if the pen testing will take more than 15 minutes) is using codebuild, either add a new codebuild for pen testing, or update existing unit testing codebuild to include pen testing. You should never run Pen testing inside codepipeline directly , it lacks the hooks to collect test result, inform result, etc

---


<br/>

## Question 57

*Date: April 15, 2023, 5:12 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is hosting a web application in an AWS Region. For disaster recovery purposes, a second region is being used as a standby. Disaster recovery requirements state that session data must be replicated between regions in near-real time and 1% of requests should route to the secondary region to continuously verify system functionality. Additionally, if there is a disruption in service in the main region, traffic should be automatically routed to the secondary region, and the secondary region must be able to scale up to handle all traffic.
How should a DevOps engineer meet these requirements?

**Options:**
- A. In both regions, deploy the application on AWS Elastic Beanstalk and use Amazon DynamoDB global tables for session data. Use an Amazon Route 53 weighted routing policy with health checks to distribute the traffic across the regions.
- B. In both regions, launch the application in Auto Scaling groups and use DynamoDB for session data. Use a Route 53 failover routing policy with health checks to distribute the traffic across the regions.
- C. In both regions, deploy the application in AWS Lambda, exposed by Amazon API Gateway, and use Amazon RDS for PostgreSQL with cross-region replication for session data. Deploy the web application with client-side logic to call the API Gateway directly.
- D. In both regions, launch the application in Auto Scaling groups and use DynamoDB global tables for session data. Enable an Amazon CloudFront weighted distribution across regions. Point the Amazon Route 53 DNS record at the CloudFront distribution.

> **Suggested Answer:** A
> **Community Vote:** A (82%), D (18%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**davdan99** (Sun 07 Jan 2024 17:57) - *Upvotes: 7*
I think it is A, We can have failover with CloudFront, but it can't have weighted routing, Here is the link of how auromatic failover works in the CloudFront

https://disaster-recovery.workshop.aws/en/services/networking/cloudfront/cloudfront-origin-group.html

---

**hayjaykay** (Wed 27 Aug 2025 03:00) - *Upvotes: 1*
Elastic Beanstalk is the most suitable to deploy web applications. This is already a cue for the answer among the rest of the options.

---

**jamesf** (Fri 26 Jul 2024 07:12) - *Upvotes: 3*
A
Keywords: web application - ElasticBeanstalk, weighted routing required.
DynamoDB Global Table required.

As understand, Cloudfront not support weighted routing.

---

**auxwww** (Wed 24 Jul 2024 03:57) - *Upvotes: 1*
A - correct - Elasticbeanstalk - option of ALB to register route53 with active-active alias with health checks and weighted routing
"In active-active failover, all the records that have the same name, the same type (such as A or AAAA), and the same routing policy (such as weighted or latency) are active unless Route 53 considers them unhealthy. Route 53 can respond to a DNS query using any healthy record."


D - incorrect because no ALB in front of ASG.

---

**Gomer** (Thu 06 Jun 2024 22:54) - *Upvotes: 1*
If the requirement is for "1% of requests should route to the secondary region to continuously", then that means the secondary region is in continuously in an Active state (Active/Active). A "request" is not a health check. You also have to have auto-scaling to dynamically pick up any extra traffic. The question is a little weird, in I don't know you you dynamically adjust the weighted routing policy to steer all traffice to the secondary region. I just know that "D" is the closest choice to meeting the specified requirements. This is absolutely and "Active-Active" design using weighted routing at some level, and auto-scaling just meets the demaind wherever it comes from. I think "latency-based" routing would make more sence, but the requirements are clearly describing "weighted routing" and Active-Active design.
https://aws.amazon.com/blogs/networking-and-content-delivery/latency-based-routing-leveraging-amazon-cloudfront-for-a-multi-region-active-active-architecture/

---

**Gomer** (Thu 06 Jun 2024 23:08) - *Upvotes: 1*
Just to clarify, the scenarios is absolutely desrivinb "weighted routing" with 99% to 1% traffic split between regions for normal operation (unbalanced Active/Active).

---

**seetpt** (Wed 01 May 2024 18:55) - *Upvotes: 1*
A is ok

---

**DanShone** (Sun 17 Mar 2024 11:58) - *Upvotes: 2*
A is correct
B + C no DynamoDB Global Tables
D - does not use Route53

---

**thanhnv142** (Wed 31 Jan 2024 14:50) - *Upvotes: 2*
A is correct: beanstalk is literally designed for this specific purpose

---

**Jaguaroooo** (Thu 04 Jan 2024 12:53) - *Upvotes: 2*
It is A, 1% of the traffic should be going to the 2ndary site. so that's weighted routing.

---


<br/>

## Question 58

*Date: April 16, 2023, 10:57 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company runs an application on Amazon EC2 instances. The company uses a series of AWS CloudFormation stacks to define the application resources. A developer performs updates by building and testing the application on a laptop and then uploading the build output and CloudFormation stack templates to Amazon S3. The developer's peers review the changes before the developer performs the CloudFormation stack update and installs a new version of the application onto the EC2 instances.
The deployment process is prone to errors and is time-consuming when the developer updates each EC2 instance with the new application. The company wants to automate as much of the application deployment process as possible while retaining a final manual approval step before the modification of the application or resources.
The company already has moved the source code for the application and the CloudFormation templates to AWS CodeCommit. The company also has created an AWS CodeBuild project to build and test the application.
Which combination of steps will meet the company’s requirements? (Choose two.)

**Options:**
- A. Create an application group and a deployment group in AWS CodeDeploy. Install the CodeDeploy agent on the EC2 instances.
- B. Create an application revision and a deployment group in AWS CodeDeploy. Create an environment in CodeDeploy. Register the EC2 instances to the CodeDeploy environment.
- C. Use AWS CodePipeline to invoke the CodeBuild job, run the CloudFormation update, and pause for a manual approval step. After approval, start the AWS CodeDeploy deployment.
- D. Use AWS CodePipeline to invoke the CodeBuild job, create CloudFormation change sets for each of the application stacks, and pause for a manual approval step. After approval, run the CloudFormation change sets and start the AWS CodeDeploy deployment.
- E. Use AWS CodePipeline to invoke the CodeBuild job, create CloudFormation change sets for each of the application stacks, and pause for a manual approval step. After approval, start the AWS CodeDeploy deployment.

> **Suggested Answer:** AD
> **Community Vote:** AD (67%), BD (30%), 2%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**tartarus23** (Mon 19 Jun 2023 23:29) - *Upvotes: 16*
(A) This step sets up the environment to use AWS CodeDeploy for application deployments. CodeDeploy uses an agent installed on the EC2 instances to perform the deployment tasks.

(D) This option uses CodePipeline to orchestrate the process. CodeBuild is used to build and test the application. CloudFormation is used to prepare the infrastructure updates as change sets. A manual approval step is inserted before applying the changes. After approval, the CloudFormation change sets are applied, and then CodeDeploy is invoked to deploy the new version of the application to the EC2 instances.

---

**ky11223344** (Sat 19 Aug 2023 12:00) - *Upvotes: 6*
There is no application group in CodeDeploy

---

**fanq10** (Tue 22 Aug 2023 11:51) - *Upvotes: 8*
- EC2 needs to install the CodeDeploy agent.
- CodeDeploy does not need to register EC2 instances, instead of it uses tag filter.
Therefore, B is incorrect, A is correct. Final answer: AD

---

**Karamen** (Fri 27 Oct 2023 03:01) - *Upvotes: 3*
@fanq10
you are right.
CodeDeploy doesn't require registering EC2 instances, it fillters by tag

---

**Srikantha** (Sat 29 Mar 2025 23:01) - *Upvotes: 1*
Both options automate the application deployment process, trigger manual approval, and ensure controlled updates and deployments to EC2 instances using CodePipeline, CodeBuild, CloudFormation, and CodeDeploy

---

**jamesf** (Fri 26 Jul 2024 07:20) - *Upvotes: 2*
AD
A - To run CodeDeploy on EC2, need agent.
D - The approval step will trigger both CloudFormation and CodeDeploy

B incorrect as no mention of installing agent on EC2 and CodeDeploy doesn't require registering EC2 instances, it filters by tag.
C incorrect as The approval step does not affect CloudFormation updates, which is not accepted
E incorrect as The approval step only allows CodeDeploy but no have CloudFormation updates

---

**alex_heavy** (Sat 13 Jul 2024 08:30) - *Upvotes: 2*
B:
https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html
You can configure automatic installation and updates of the CodeDeploy agent when you create your deployment group in the console.
https://docs.aws.amazon.com/codedeploy/latest/userguide/applications.html
After you configure instances, but before you can deploy a revision, you must create an application in CodeDeploy. An application is simply a name or container used by CodeDeploy to ensure the correct revision, deployment configuration, and deployment group are referenced during a deployment.
https://docs.aws.amazon.com/codedeploy/latest/userguide/application-revisions.html
In CodeDeploy, a revision contains a version of the source files CodeDeploy will deploy to your instances or scripts CodeDeploy will run on your instances.
https://aws.amazon.com/ru/blogs/devops/using-codedeploy-environment-variables/

---

**xdkonorek2** (Sun 30 Jun 2024 18:26) - *Upvotes: 1*
for those who vote B: what is creating environment in code deploy?

I think application group means application and registering instances with code deploy is basically creating deployment group, and instance is not registered manually it has to be tagged

---

**seetpt** (Wed 01 May 2024 18:57) - *Upvotes: 1*
AD is ok

---

**4555894** (Fri 08 Mar 2024 15:31) - *Upvotes: 3*
A- https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html
D - This option correctly utilizes AWS CodePipeline to invoke the CodeBuild job and create CloudFormationchange sets. It adds a manual approval step before executing the change sets and starting the AWSCodeDeploy deployment. This ensures that the deployment process is automated while retaining the
final manual approval step.

---


<br/>

## Question 59

*Date: April 15, 2023, 4:45 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer manages a web application that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in an EC2 Auto Scaling group across multiple Availability Zones. The engineer needs to implement a deployment strategy that:
Launches a second fleet of instances with the same capacity as the original fleet.
Maintains the original fleet unchanged while the second fleet is launched.
Transitions traffic to the second fleet when the second fleet is fully deployed.
Terminates the original fleet automatically 1 hour after transition.
Which solution will satisfy these requirements?

**Options:**
- A. Use an AWS CloudFormation template with a retention policy for the ALB set to 1 hour. Update the Amazon Route 53 record to reflect the new ALB.
- B. Use two AWS Elastic Beanstalk environments to perform a blue/green deployment from the original environment to the new one. Create an application version lifecycle policy to terminate the original environment in 1 hour.
- C. Use AWS CodeDeploy with a deployment group configured with a blue/green deployment configuration Select the option Terminate the original instances in the deployment group with a waiting period of 1 hour.
- D. Use AWS Elastic Beanstalk with the configuration set to Immutable. Create an .ebextension using the Resources key that sets the deletion policy of the ALB to 1 hour, and deploy the application.

> **Suggested Answer:** C
> **Community Vote:** C (94%), 6%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**haazybanj** (Mon 01 May 2023 06:41) - *Upvotes: 12*
Option B, using two AWS Elastic Beanstalk environments to perform a blue/green deployment from the original environment to the new one, would not launch a second fleet of instances. Instead, it would create a new environment and deploy the application version to it. It also requires the use of an application version lifecycle policy to terminate the original environment in 1 hour.

Option D, using AWS Elastic Beanstalk with the configuration set to Immutable and creating an .ebextension to set the deletion policy of the ALB to 1 hour, would not launch a second fleet of instances, and it would not maintain the original fleet unchanged while the second fleet is launched. Additionally, the .ebextension approach is not the recommended way to delete resources in AWS.

Therefore, the correct option is C, using AWS CodeDeploy with a deployment group configured with a blue/green deployment configuration and selecting the option to Terminate the original instances in the deployment group with a waiting period of 1 hour.

---

**GripZA** (Sat 19 Apr 2025 16:23) - *Upvotes: 1*
Codedploy:
launches a second fleet (green environment) with the same capacity as the original
it keeps the original fleet unchanged while launching the new one
switches traffic to the new fleet only after it's fully ready.
uatomatically terminates the original (blue) fleet after a 1hour waiting period — exactly as needed

---

**hayjaykay** (Tue 04 Feb 2025 08:57) - *Upvotes: 2*
Key words: EC2 for CodeDeploy, Elastic Beanstalk for CodePipeline.

---

**newpotato** (Thu 03 Oct 2024 08:31) - *Upvotes: 1*
Option B would require more manual intervention and configuration, making it a less optimal solution compared to the seamless blue/green deployment and automatic fleet termination provided by Option C.

Option D involves unnecessary complexity around setting ALB deletion policies, and while immutable deployments offer zero-downtime updates, they don't fully meet the core requirements of automatic traffic shifting and fleet termination

---

**73d8cc9** (Sat 01 Jun 2024 22:30) - *Upvotes: 1*
Immutable strategy with Elastic Beanstalk involves deploying additional instance while Blue/Green strategy involves deploying another environment. The key difference is environment vs instances

---

**seetpt** (Wed 01 May 2024 18:58) - *Upvotes: 1*
C is ok

---

**vmahilevskyi** (Sat 02 Mar 2024 19:28) - *Upvotes: 3*
https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-create-blue-green.html

---

**thanhnv142** (Wed 31 Jan 2024 15:09) - *Upvotes: 1*
A is correct: The question ask for a solution to automatic deployment of EC2 instances, which is the job of cloudFormation
- B and D is irrelevant because it is use to deploy webapps only, not EC2 instances
- C is also irrelevant because codedeploy (literlly by the name: CODEdeploy) is only used for deploying code, not EC2 instances, which is not code. Dont know why ChatGPT recommend this, but it is wrong definitely

---

**jojom19980** (Sun 18 Feb 2024 13:24) - *Upvotes: 1*
No C is the more accurate and logical

---

**madperro** (Wed 21 Jun 2023 07:03) - *Upvotes: 3*
C looks like the best solution.

---


<br/>

## Question 60

*Date: April 15, 2023, 4:38 p.m.
Disclaimers:
- ExamTopics website is not rel*

A video-sharing company stores its videos in Amazon S3. The company has observed a sudden increase in video access requests, but the company does not know which videos are most popular. The company needs to identify the general access pattern for the video files. This pattern includes the number of users who access a certain file on a given day, as well as the number of pull requests for certain files.
How can the company meet these requirements with the LEAST amount of effort?

**Options:**
- A. Activate S3 server access logging. Import the access logs into an Amazon Aurora database. Use an Aurora SQL query to analyze the access patterns.
- B. Activate S3 server access logging. Use Amazon Athena to create an external table with the log files. Use Athena to create a SQL query to analyze the access patterns.
- C. Invoke an AWS Lambda function for every S3 object access event. Configure the Lambda function to write the file access information, such as user. S3 bucket, and file key, to an Amazon Aurora database. Use an Aurora SQL query to analyze the access patterns.
- D. Record an Amazon CloudWatch Logs log message for every S3 object access event. Configure a CloudWatch Logs log stream to write the file access information, such as user, S3 bucket, and file key, to an Amazon Kinesis Data Analytics for SQL application. Perform a sliding window analysis.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**zijo** (Sun 22 Sep 2024 17:54) - *Upvotes: 4*
Bis the Answer because Athena is designed for these type of use cases
AWS Athena is a serverless interactive query service that lets you analyze data stored in Amazon Simple Storage Service (S3) using standard SQL.

---

**thanhnv142** (Wed 31 Jul 2024 14:14) - *Upvotes: 2*
B is correct:Use S3 in combination with Athena is the recommended way to analyze data
A: setups of Aurora is complex and unnecessary. It also more costly than B
C and D are both too complicated.

---

**habros** (Tue 09 Jan 2024 03:30) - *Upvotes: 4*
B is so much simpler. Athena can do interactive queries on S3 data.

---

**madperro** (Thu 21 Dec 2023 08:06) - *Upvotes: 2*
B is correct and simplest.

---

**tom_uk** (Fri 01 Dec 2023 10:38) - *Upvotes: 1*
B is the answer

---

**bcx** (Thu 30 Nov 2023 09:37) - *Upvotes: 1*
B is the natural way to do it

---

**gdtypk** (Sun 12 Nov 2023 00:24) - *Upvotes: 1*
https://repost.aws/ja/knowledge-center/analyze-logs-athena

---

**haazybanj** (Fri 03 Nov 2023 06:13) - *Upvotes: 1*
B is right

---

**Sazeka** (Fri 20 Oct 2023 22:07) - *Upvotes: 1*
I would go with B

---

**alce2020** (Sun 15 Oct 2023 16:38) - *Upvotes: 1*
B is correct

---


<br/>

## Question 61

*Date: April 15, 2023, 4:31 p.m.
Disclaimers:
- ExamTopics website is not rel*

A development team wants to use AWS CloudFormation stacks to deploy an application. However, the developer IAM role does not have the required permissions to provision the resources that are specified in the AWS CloudFormation template. A DevOps engineer needs to implement a solution that allows the developers to deploy the stacks. The solution must follow the principle of least privilege.
Which solution will meet these requirements?

**Options:**
- A. Create an IAM policy that allows the developers to provision the required resources. Attach the policy to the developer IAM role.
- B. Create an IAM policy that allows full access to AWS CloudFormation. Attach the policy to the developer IAM role.
- C. Create an AWS CloudFormation service role that has the required permissions. Grant the developer IAM role a cloudformation:* action. Use the new service role during stack deployments.
- D. Create an AWS CloudFormation service role that has the required permissions. Grant the developer IAM role the iam:PassRole permission. Use the new service role during stack deployments.

> **Suggested Answer:** D
> **Community Vote:** D (82%), Other, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**fuzzycom** (Thu 27 Jun 2024 18:35) - *Upvotes: 1*
D is totally correct

---

**4555894** (Fri 08 Mar 2024 15:33) - *Upvotes: 2*
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html

---

**thanhnv142** (Thu 01 Feb 2024 11:34) - *Upvotes: 4*
D is correct: Need to create a role for Cloud formation that has the required permissions. Then adding iam:PassRole permission to the dev IAM role to allow them to pass this role to CF
A: no mention of creating the required permissions for ACF. Additionally, should not grant permissions for dev.
B: grant full access is against the least privilege policy
C: no mention of granting iam:PassRole permission to the dev

---

**imymoco** (Tue 12 Dec 2023 02:19) - *Upvotes: 1*
A is incorrect; A would also allow resources to be used from outside of cfn.
Therefore, D is correct.

---

**jason7** (Sat 26 Aug 2023 10:51) - *Upvotes: 2*
Option D allows you to create a dedicated AWS CloudFormation service role with the precise permissions required for stack deployments. Then, you grant the developer IAM role the iam:PassRole permission, which enables it to pass the service role to AWS CloudFormation without granting it broad IAM permissions. This approach aligns best with the principle of least privilege and ensures developers can deploy stacks while maintaining control over their permissions.

---

**ogwu2000** (Sat 15 Jul 2023 12:00) - *Upvotes: 1*
B is the answer. DC wrong - Nothing like CloudFormation service-role.

---

**fanq10** (Tue 22 Aug 2023 11:58) - *Upvotes: 1*
B is not best practice of using CloudFormation.
D is correct, 100% sure. `iam:PassRole` to a CloudFormation Service Role (take a look at this: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html#:~:text=you%20can%20use.-,Important,-When%20you%20specify)

---

**DZ_Ben** (Thu 02 Nov 2023 10:48) - *Upvotes: 1*
Should be D! See here https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html

---

**madperro** (Wed 21 Jun 2023 07:19) - *Upvotes: 1*
D is the right answer.

---

**tartarus23** (Tue 20 Jun 2023 22:29) - *Upvotes: 1*
This solution follows the principle of least privilege by creating a specific AWS CloudFormation service role that only has the permissions required for the resources in the AWS CloudFormation stack. The developers are then granted permission to pass this role (iam:PassRole) to the AWS CloudFormation service when they initiate stack deployments, which allows the service to act on behalf of the developer to provision the specified resources.

---


<br/>

## Question 62

*Date: April 16, 2023, 11:05 p.m.
Disclaimers:
- ExamTopics website is not rel*

A production account has a requirement that any Amazon EC2 instance that has been logged in to manually must be terminated within 24 hours. All applications in the production account are using Auto Scaling groups with the Amazon CloudWatch Logs agent configured.
How can this process be automated?

**Options:**
- A. Create a CloudWatch Logs subscription to an AWS Step Functions application. Configure an AWS Lambda function to add a tag to the EC2 instance that produced the login event and mark the instance to be decommissioned. Create an Amazon EventBridge rule to invoke a second Lambda function once a day that will terminate all instances with this tag.
- B. Create an Amazon CloudWatch alarm that will be invoked by the login event. Send the notification to an Amazon Simple Notification Service (Amazon SNS) topic that the operations team is subscribed to, and have them terminate the EC2 instance within 24 hours.
- C. Create an Amazon CloudWatch alarm that will be invoked by the login event. Configure the alarm to send to an Amazon Simple Queue Service (Amazon SQS) queue. Use a group of worker instances to process messages from the queue, which then schedules an Amazon EventBridge rule to be invoked.
- D. Create a CloudWatch Logs subscription to an AWS Lambda function. Configure the function to add a tag to the EC2 instance that produced the login event and mark the instance to be decommissioned. Create an Amazon EventBridge rule to invoke a daily Lambda function that terminates all instances with this tag.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Aesthet** (Sun 11 Aug 2024 15:31) - *Upvotes: 1*
Opion D: "with this tag"
So, there will be one tag, like ShoudTerminate: true. But by doing so we will terminate ALL instances with a tag - even those created 10 minutes ago. It doesn't seem correct, or am I missing something?

---

**fuzzycom** (Thu 27 Jun 2024 18:41) - *Upvotes: 1*
D is best answer.
hint: question includes "~~Amazon CloudWatch Logs agent configured"
Lambda function is keyword.

---

**thanhnv142** (Thu 01 Feb 2024 15:26) - *Upvotes: 4*
D is correct:
A: If using step function, no need to include "Amazon EventBridge rule to invoke a second Lambda function"
B: With this method, policy-breaching Ec2 would be terminated manually, which cannot ensure that they are terminated within 24 hours
C: no mention of terminating the instances

---

**imymoco** (Tue 12 Dec 2023 02:27) - *Upvotes: 2*
D is correct; with B, SNS can cause delays.

---

**madperro** (Wed 21 Jun 2023 07:25) - *Upvotes: 1*
D is the best answer.

---

**haazybanj** (Mon 01 May 2023 06:52) - *Upvotes: 3*
D. Create a CloudWatch Logs subscription to an AWS Lambda function. Configure the function to add a tag to the EC2 instance that produced the login event and mark the instance to be decommissioned. Create an Amazon EventBridge rule to invoke a daily Lambda function that terminates all instances with this tag.

---

**alce2020** (Sun 16 Apr 2023 23:05) - *Upvotes: 3*
D is the correct answer

---


<br/>

## Question 63

*Date: April 15, 2023, 4:21 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has enabled all features for its organization in AWS Organizations. The organization contains 10 AWS accounts. The company has turned on AWS CloudTrail in all the accounts. The company expects the number of AWS accounts in the organization to increase to 500 during the next year. The company plans to use multiple OUs for these accounts.
The company has enabled AWS Config in each existing AWS account in the organization. A DevOps engineer must implement a solution that enables AWS Config automatically for all future AWS accounts that are created in the organization.
Which solution will meet this requirement?

**Options:**
- A. In the organization's management account, create an Amazon EventBridge rule that reacts to a CreateAccount API call. Configure the rule to invoke an AWS Lambda function that enables trusted access to AWS Config for the organization.
- B. In the organization's management account, create an AWS CloudFormation stack set to enable AWS Config. Configure the stack set to deploy automatically when an account is created through Organizations.
- C. In the organization's management account, create an SCP that allows the appropriate AWS Config API calls to enable AWS Config. Apply the SCP to the root-level OU.
- D. In the organization's management account, create an Amazon EventBridge rule that reacts to a CreateAccount API call. Configure the rule to invoke an AWS Systems Manager Automation runbook to enable AWS Config for the account.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Thu 01 Aug 2024 15:10) - *Upvotes: 2*
B is correct: The question ask a solution to "enables AWS Config automatically" for all future accounts. In AWS org, to provision or configure resources on other accounts, we use ACF
A, C and D: no mention of ACF

---

**thanhnv142** (Sun 04 Aug 2024 14:56) - *Upvotes: 1*
A: trusted access to AWS Config: this is used by other services to access to AWS config, not for account
D: enable AWS Config for the account: this means we only activate AWS config for the management account, not the newly created ones

---

**2pk** (Mon 06 May 2024 10:15) - *Upvotes: 1*
B:
Details the new feature with enable trusted access to new accounts in any region
https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-cloudformation.html

---

**hjey0329** (Thu 22 Feb 2024 06:58) - *Upvotes: 1*
B
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-sampletemplates.html

---

**madperro** (Thu 21 Dec 2023 10:27) - *Upvotes: 1*
B is the best solution.

---

**samgyeopsal** (Wed 22 Nov 2023 03:58) - *Upvotes: 2*
B
https://aws.amazon.com/about-aws/whats-new/2020/02/aws-cloudformation-stacksets-introduces-automatic-deployments-across-accounts-and-regions-through-aws-organizations/

---

**haazybanj** (Wed 01 Nov 2023 07:54) - *Upvotes: 4*
The correct solution to enable AWS Config automatically for all future AWS accounts created in the organization is Option B: In the organization's management account, create an AWS CloudFormation stack set to enable AWS Config. Configure the stack set to deploy automatically when an account is created through Organizations.

Option C is incorrect because although it suggests creating an SCP that allows the appropriate AWS Config API calls to enable AWS Config and applying the SCP to the root-level OU, it does not specifically enable AWS Config automatically for all future AWS accounts that are created in the organization.

---

**Olelukoe** (Thu 20 Jun 2024 00:53) - *Upvotes: 3*
In terms of Option C: SCP can only Deny access, not Allow

---

**alce2020** (Sun 15 Oct 2023 16:21) - *Upvotes: 1*
B is correct

---


<br/>

## Question 64

*Date: May 1, 2023, 6:57 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has many applications. Different teams in the company developed the applications by using multiple languages and frameworks. The applications run on premises and on different servers with different operating systems. Each team has its own release protocol and process. The company wants to reduce the complexity of the release and maintenance of these applications.
The company is migrating its technology stacks, including these applications, to AWS. The company wants centralized control of source code, a consistent and automatic delivery pipeline, and as few maintenance tasks as possible on the underlying infrastructure.
What should a DevOps engineer do to meet these requirements?

**Options:**
- A. Create one AWS CodeCommit repository for all applications. Put each application's code in a different branch. Merge the branches, and use AWS CodeBuild to build the applications. Use AWS CodeDeploy to deploy the applications to one centralized application server.
- B. Create one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build the applications one at a time. Use AWS CodeDeploy to deploy the applications to one centralized application server.
- C. Create one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build the applications one at a time and to create one AMI for each server. Use AWS CloudFormation StackSets to automatically provision and decommission Amazon EC2 fleets by using these AMIs.
- D. Create one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build one Docker image for each application in Amazon Elastic Container Registry (Amazon ECR). Use AWS CodeDeploy to deploy the applications to Amazon Elastic Container Service (Amazon ECS) on infrastructure that AWS Fargate manages.

> **Suggested Answer:** D
> **Community Vote:** D (93%), 7%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**haazybanj** (Wed 01 Nov 2023 07:57) - *Upvotes: 12*
Option D is the best choice to meet the requirements of centralized control of source code, a consistent and automatic delivery pipeline, and minimal maintenance tasks.
Option D is the best choice because it allows each application to have its own repository and build process, but uses containerization to create a consistent and automatic delivery pipeline that can be easily deployed to Amazon ECS on infrastructure that AWS Fargate manages. This approach also provides scalability and ease of maintenance.

---

**thanhnv142** (Thu 01 Aug 2024 15:16) - *Upvotes: 2*
D is correct: "centralized control of source code" = CodeCommit. "Consistent and automatic delivery pipeline" = codepipeline/codebuide/codedeploy. "as few maintenance tasks as possible on the underlying infrastructur" = containerization
A: "CodeCommit repository for all applications": should not, need separate repos for each app
B and C: no mention of containerization (fargate, ECS)

---

**DaddyDee** (Tue 02 Apr 2024 12:54) - *Upvotes: 1*
D is the Answer: https://aws.amazon.com/blogs/compute/building-deploying-and-operating-containerized-applications-with-aws-fargate/

---

**ddedqdqw** (Mon 22 Jan 2024 13:54) - *Upvotes: 1*
A ISNT CORRECT?

---

**davdan99** (Sun 07 Jul 2024 21:17) - *Upvotes: 2*
Of course no, it is the most wrong one, It saying to have all the applications code in one repo (bad thing to do), separated in branches, and after that merge them (second very bad thing to do).

---

**RickSk** (Thu 21 Dec 2023 18:13) - *Upvotes: 3*
Option D.
I was torn between C and D, but the requirement for ease of maintenance on the underlying infrastructure clearly points to ECS.

---

**madperro** (Thu 21 Dec 2023 10:31) - *Upvotes: 1*
D is the best option, there is virtually no infrastructure to manage.

---

**ele** (Mon 13 Nov 2023 17:21) - *Upvotes: 1*
D is best option.

---


<br/>

## Question 65

*Date: April 16, 2023, 11:12 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company's application is currently deployed to a single AWS Region. Recently, the company opened a new office on a different continent. The users in the new office are experiencing high latency. The company's application runs on Amazon EC2 instances behind an Application Load Balancer (ALB) and uses Amazon DynamoDB as the database layer. The instances run in an EC2 Auto Scaling group across multiple Availability Zones. A DevOps engineer is tasked with minimizing application response times and improving availability for users in both Regions.
Which combination of actions should be taken to address the latency issues? (Choose three.)

**Options:**
- A. Create a new DynamoDB table in the new Region with cross-Region replication enabled.
- B. Create new ALB and Auto Scaling group global resources and configure the new ALB to direct traffic to the new Auto Scaling group.
- C. Create new ALB and Auto Scaling group resources in the new Region and configure the new ALB to direct traffic to the new Auto Scaling group.
- D. Create Amazon Route 53 records, health checks, and latency-based routing policies to route to the ALB.
- E. Create Amazon Route 53 aliases, health checks, and failover routing policies to route to the ALB.
- F. Convert the DynamoDB table to a global table.

> **Suggested Answer:** CDF
> **Community Vote:** CDF (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**haazybanj** (Wed 01 Nov 2023 08:02) - *Upvotes: 10*
C. Create new ALB and Auto Scaling group resources in the new Region and configure the new ALB to direct traffic to the new Auto Scaling group. This will allow users in the new Region to access the application with lower latency by reducing the network hops between the user and the application servers.

D. Create Amazon Route 53 records, health checks, and latency-based routing policies to route to the ALB. This will enable Route 53 to route user traffic to the nearest healthy ALB, based on the latency between the user and the ALBs.

F. Convert the DynamoDB table to a global table. This will enable reads and writes to the table in both Regions with low latency, improving the overall response time of the application

---

**Jonalb** (Thu 15 May 2025 18:50) - *Upvotes: 1*
C,D,F }

C. Create new ALB and Auto Scaling group resources in the new Region and configure the new ALB to direct traffic to the new Auto Scaling group. This will allow users in the new Region to access the application with lower latency by reducing the network hops between the user and the application servers.

D. Create Amazon Route 53 records, health checks, and latency-based routing policies to route to the ALB. This will enable Route 53 to route user traffic to the nearest healthy ALB, based on the latency between the user and the ALBs.

F. Convert the DynamoDB table to a global table. This will enable reads and writes to the table in both Regions with low latency, improving the overall response time of the application

---

**xdkonorek2** (Mon 21 Oct 2024 09:07) - *Upvotes: 2*
Technically converting dynamodb table to global table requires creating replica in another region with cross-region replication and you don't "convert" you add a replica in "global tables" in specified region so this answers are a little bit misleading.

Probably F is better than A since they name this operation as "converting" e.g. here https://aws.amazon.com/blogs/aws/new-convert-your-single-region-amazon-dynamodb-tables-to-global-tables/

---

**thanhnv142** (Thu 01 Aug 2024 15:35) - *Upvotes: 1*
D is, of course, correct: <apply a core set of security controls to an existing set of AWS accounts> and <The accounts are in an organization in AWS Organizations> means we need ACF template to deploy these set of security controls. <Individual account administrators must not be able to edit or delete any of the baseline resources> means we need scp to deny permission
A and B: no mention of SCP
C: this option deploy the rules by AWS Config management account, which is not correct because we need ACF. Additionally, no mention of denying modification to CloudTrail trails

---

**thanhnv142** (Thu 01 Aug 2024 15:27) - *Upvotes: 2*
CDF: <opened a new office on a different continent> and <The users in the new office are experiencing high latency> means they need to replicate their existing site to the new region. <Amazon EC2 instances behind an Application Load Balancer (ALB)> means they need to replicate both these. <address the latency issues> means they need route53 with latency-based and health check
A: <cross-Region replication> is used for backup only, not a live site. It would introduce a lot of latency
B: <Auto Scaling group global resources>: there is no such thing
E: No mention of latency-based.

---

**z_inderjot** (Mon 24 Jun 2024 03:24) - *Upvotes: 1*
CDF very easy

---

**IIIIIIIIIlllll** (Tue 23 Apr 2024 14:15) - *Upvotes: 1*
CDF is collect

---

**alce2020** (Mon 16 Oct 2023 23:12) - *Upvotes: 2*
C,D,F are correct

---


<br/>

## Question 66

*Date: April 15, 2023, 4:14 a.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer needs to apply a core set of security controls to an existing set of AWS accounts. The accounts are in an organization in AWS Organizations. Individual teams will administer individual accounts by using the AdministratorAccess AWS managed policy. For all accounts. AWS CloudTrail and AWS Config must be turned on in all available AWS Regions. Individual account administrators must not be able to edit or delete any of the baseline resources. However, individual account administrators must be able to edit or delete their own CloudTrail trails and AWS Config rules.
Which solution will meet these requirements in the MOST operationally efficient way?

**Options:**
- A. Create an AWS CloudFormation template that defines the standard account resources. Deploy the template to all accounts from the organization's management account by using CloudFormation StackSets. Set the stack policy to deny Update:Delete actions.
- B. Enable AWS Control Tower. Enroll the existing accounts in AWS Control Tower. Grant the individual account administrators access to CloudTrail and AWS Config.
- C. Designate an AWS Config management account. Create AWS Config recorders in all accounts by using AWS CloudFormation StackSets. Deploy AWS Config rules to the organization by using the AWS Config management account. Create a CloudTrail organization trail in the organization’s management account. Deny modification or deletion of the AWS Config recorders by using an SCP.
- D. Create an AWS CloudFormation template that defines the standard account resources. Deploy the template to all accounts from the organization's management account by using Cloud Formation StackSets Create an SCP that prevents updates or deletions to CloudTrail resources or AWS Config resources unless the principal is an administrator of the organization's management account.

> **Suggested Answer:** D
> **Community Vote:** D (51%), C (41%), 9%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**haazybanj** (Mon 01 May 2023 07:07) - *Upvotes: 20*
C
This solution meets the requirements in the most operationally efficient way. It uses AWS CloudFormation StackSets to deploy AWS Config recorders in all accounts and AWS Config rules to the organization, which can be centrally managed from an AWS Config management account. A CloudTrail organization trail can also be created in the organization’s management account to collect logs from all accounts. An SCP can be used to deny modification or deletion of the AWS Config recorders, ensuring that the baseline resources cannot be modified or deleted by individual account administrators. However, individual account administrators can still edit or delete their own CloudTrail trails and AWS Config rules.

---

**koenigParas2324** (Wed 22 Nov 2023 06:21) - *Upvotes: 4*
this solution lacks clarity on allowing individual account administrators control over their CloudTrail trails.

---

**bnagaraja9099** (Thu 14 Dec 2023 23:52) - *Upvotes: 1*
C is good.
https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html
https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html

---

**bnagaraja9099** (Thu 14 Dec 2023 23:52) - *Upvotes: 1*
An SCP restricts permissions for IAM users and roles in member accounts, including the member account's root user. Any account has only those permissions permitted by every parent above it. If a permission is blocked at any level above the account, either implicitly (by not being included in an Allow policy statement) or explicitly (by being included in a Deny policy statement), a user or role in the affected account can't use that permission, even if the account administrator attaches the AdministratorAccess IAM policy with */* permissions to the user.
https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html

---

**a1234321606** (Sat 18 Nov 2023 15:29) - *Upvotes: 5*
Why C? If you deny modification or deletion of the AWS Config recorders by using an SCP, how do individual account administrators edit or delete their own CloudTrail trails and AWS Config rules?

---

**dzn** (Sat 24 Feb 2024 08:22) - *Upvotes: 6*
When Control Tower is enabled, AWS-GR_CLOUDTRAIL_ENABLED and AWS-GR_CONFIG_ENABLED will enable CloudTrail and Config in all available regions. The guardrails are automatically set to disallow changes to baseline resources.

A, C, D - No mention about baseline resource.

---

**ryuhei** (Sat 23 Aug 2025 05:05) - *Upvotes: 2*
Grok says D is the correct answer.

---

**toma** (Wed 23 Jul 2025 21:42) - *Upvotes: 1*
SCPs do not support conditions based on the principal's IAM role or policy (like "administrator of management account").

---

**toma** (Tue 24 Jun 2025 21:34) - *Upvotes: 2*
Defiantly D

---

**GripZA** (Sat 19 Apr 2025 20:41) - *Upvotes: 1*
my 2 cents: "AWS CloudTrail and AWS Config must be turned on in all available AWS Regions" these are the baseline resources which cannot be deleted. Definitely operatioanlly efficient to have a designated AWS config management account and organizational trail. Then the deny with SCP will prevent baseline resource deletion, but admins can still edit their own trails and config rules.

Why not D: SCP is used, which is good — but the condition in the SCP to allow updates only by "administrators of the management account" is hard to implement cleanly

---


<br/>

## Question 67

*Date: April 6, 2023, 8:30 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has its AWS accounts in an organization in AWS Organizations. AWS Config is manually configured in each AWS account. The company needs to implement a solution to centrally configure AWS Config for all accounts in the organization The solution also must record resource changes to a central account.
Which combination of actions should a DevOps engineer perform to meet these requirements? (Choose two.)

**Options:**
- A. Configure a delegated administrator account for AWS Config. Enable trusted access for AWS Config in the organization.
- B. Configure a delegated administrator account for AWS Config. Create a service-linked role for AWS Config in the organization’s management account.
- C. Create an AWS CloudFormation template to create an AWS Config aggregator. Configure a CloudFormation stack set to deploy the template to all accounts in the organization.
- D. Create an AWS Config organization aggregator in the organization's management account. Configure data collection from all AWS accounts in the organization and from all AWS Regions.
- E. Create an AWS Config organization aggregator in the delegated administrator account. Configure data collection from all AWS accounts in the organization and from all AWS Regions.

> **Suggested Answer:** AE
> **Community Vote:** AE (86%), 14%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**asfsdfsdf** (Tue 18 Apr 2023 21:17) - *Upvotes: 18*
AE
https://aws.amazon.com/blogs/mt/org-aggregator-delegated-admin/
A - When enabling trust - the service-linked role will be created but not the other way around.
E - the delegated account will be the account that manages AWS config so it should collect all data centrally.

---

**jamesf** (Fri 26 Jul 2024 09:12) - *Upvotes: 2*
A - You can enable trusted access using either the AWS Config console or the AWS Organizations console.
https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-config.html

---

**zijo** (Mon 25 Mar 2024 16:48) - *Upvotes: 1*
AE is the answer
AWS Config offers an organization-wide data aggregation capability called the Config organization aggregator. It allows you to collect and view configuration data from all member accounts within your AWS Organization in a single location. This centralizes your view of resource configurations and compliance posture across your entire AWS environment.

---

**thanhnv142** (Thu 01 Feb 2024 17:03) - *Upvotes: 1*
A and E are correct: <AWS Config is manually configured in each AWS account> means we dont need ACF (only used for the deployment of AWS config). <centrally configure AWS Config for all accounts> means we need to allow a central account to control AWS config in all member accounts.
- <record resource changes to a central account> means we need to collect data from all member accounts and push to the central account
B: service-linked role only used for interacting with other AWS services
C: no need ACF
D: we need AWS Config organization aggregator in the delegated administrator account, not the organization's management account

---

**hoaile257** (Tue 26 Sep 2023 09:03) - *Upvotes: 2*
AE is most correct

---

**Just_Ninja** (Wed 12 Jul 2023 09:11) - *Upvotes: 3*
Here you have the Tutorial :)
https://aws.amazon.com/blogs/mt/org-aggregator-delegated-admin/

---

**rhinozD** (Wed 14 Jun 2023 17:44) - *Upvotes: 3*
https://aws.amazon.com/blogs/mt/org-aggregator-delegated-admin/
https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-config.html

---

**Kodoma** (Tue 23 May 2023 22:28) - *Upvotes: 3*
BE is the most efficient

---

**ParagSanyashiv** (Mon 08 May 2023 18:13) - *Upvotes: 3*
BD is most suitable in this case

---

**2pk** (Mon 06 Nov 2023 11:01) - *Upvotes: 1*
Why ? it says setup service linked role in management account not in Delegated account?

---


<br/>

## Question 68

*Date: May 3, 2023, 6:20 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company wants to migrate its content sharing web application hosted on Amazon EC2 to a serverless architecture. The company currently deploys changes to its application by creating a new Auto Scaling group of EC2 instances and a new Elastic Load Balancer, and then shifting the traffic away using an Amazon Route 53 weighted routing policy.
For its new serverless application, the company is planning to use Amazon API Gateway and AWS Lambda. The company will need to update its deployment processes to work with the new application. It will also need to retain the ability to test new features on a small number of users before rolling the features out to the entire user base.
Which deployment strategy will meet these requirements?

**Options:**
- A. Use AWS CDK to deploy API Gateway and Lambda functions. When code needs to be changed, update the AWS CloudFormation stack and deploy the new version of the APIs and Lambda functions. Use a Route 53 failover routing policy for the canary release strategy.
- B. Use AWS CloudFormation to deploy API Gateway and Lambda functions using Lambda function versions. When code needs to be changed, update the CloudFormation stack with the new Lambda code and update the API versions using a canary release strategy. Promote the new version when testing is complete.
- C. Use AWS Elastic Beanstalk to deploy API Gateway and Lambda functions. When code needs to be changed, deploy a new version of the API and Lambda functions. Shift traffic gradually using an Elastic Beanstalk blue/green deployment.
- D. Use AWS OpsWorks to deploy API Gateway in the service layer and Lambda functions in a custom layer. When code needs to be changed, use OpsWorks to perform a blue/green deployment and shift traffic gradually.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**haazybanj** (Wed 03 May 2023 06:20) - *Upvotes: 13*
The deployment strategy that will meet the company's requirements is B. Use AWS CloudFormation to deploy API Gateway and Lambda functions using Lambda function versions. When code needs to be changed, update the CloudFormation stack with the new Lambda code and update the API versions using a canary release strategy. Promote the new version when testing is complete.

Explanation:

Option B provides a deployment strategy for the company's new serverless architecture, allowing the company to retain the ability to test new features on a small number of users before rolling the features out to the entire user base. Using AWS CloudFormation, the company can deploy API Gateway and Lambda functions using Lambda function versions. When code needs to be changed, the company can update the CloudFormation stack with the new Lambda code and update the API versions using a canary release strategy. Once testing is complete, the new version can be promoted.

---

**Snape** (Thu 13 Jul 2023 04:31) - *Upvotes: 7*
A Wrong: not using Canary, or Blue/green
C Wrong: Beanstalk is not serverless deployment platform
D Wrong: Irrelevant, OpsWork is configuration management platform and situation is requesting application deployment /AWS resource provisioning platform

---

**beanxyz** (Thu 07 Sep 2023 12:11) - *Upvotes: 3*
Your answer is correct but the explanation is not.
A is wrong because we can't use Route 53 failover routing for canary release. If it says Route53 weighted routing, then it is a possible option.
D is wrong because when you use blue/green mode, the switch from blue to green is all done at once, not like a granular canary change

---

**teo2157** (Thu 28 Nov 2024 16:02) - *Upvotes: 1*
It could be either A or B but the key here is that "Use a Route 53 failover routing policy for the canary release strategy." is a wrong statement, it should be a Route 53 weighted routing policy so B is the correct answer

---

**thanhnv142** (Thu 01 Feb 2024 17:15) - *Upvotes: 1*
B is correct: <serverless architecture> means ECS, lambda, Beanstalk. < It will also need to retain the ability to test new features on a small number of users before rolling the features out to the entire user base> means canary deployment
A: <Route 53 failover routing policy for the canary release strategy>: there is no such thing
C and D: no mention of canary deployment

---

**thanhnv142** (Thu 01 Feb 2024 17:14) - *Upvotes: 1*
B is correct: <serverless architecture> means ECS, lambda, Beanstalk. < It will also need to retain the ability to test new features on a small number of users before rolling the features out to the entire user base> means canary deployment
A: <Route 53 failover routing policy for the canary release strategy>: there is no such thing
C: no mention of canary deployment

---

**Jeanphi72** (Wed 03 May 2023 16:37) - *Upvotes: 1*
ONly B is possible

---


<br/>

## Question 69

*Date: May 1, 2023, 7:23 a.m.
Disclaimers:
- ExamTopics website is not rel*

A development team uses AWS CodeCommit, AWS CodePipeline, and AWS CodeBuild to develop and deploy an application. Changes to the code are submitted by pull requests. The development team reviews and merges the pull requests, and then the pipeline builds and tests the application.
Over time, the number of pull requests has increased. The pipeline is frequently blocked because of failing tests. To prevent this blockage, the development team wants to run the unit and integration tests on each pull request before it is merged.
Which solution will meet these requirements?

**Options:**
- A. Create a CodeBuild project to run the unit and integration tests. Create a CodeCommit approval rule template. Configure the template to require the successful invocation of the CodeBuild project. Attach the approval rule to the project's CodeCommit repository.
- B. Create an Amazon EventBridge rule to match pullRequestCreated events from CodeCommit Create a CodeBuild project to run the unit and integration tests. Configure the CodeBuild project as a target of the EventBridge rule that includes a custom event payload with the CodeCommit repository and branch information from the event.
- C. Create an Amazon EventBridge rule to match pullRequestCreated events from CodeCommit. Modify the existing CodePipeline pipeline to not run the deploy steps if the build is started from a pull request. Configure the EventBridge rule to run the pipeline with a custom payload that contains the CodeCommit repository and branch information from the event.
- D. Create a CodeBuild project to run the unit and integration tests. Create a CodeCommit notification rule that matches when a pull request is created or updated. Configure the notification rule to invoke the CodeBuild project.

> **Suggested Answer:** B
> **Community Vote:** B (71%), 14%, Other, Other, A (35%), C (25%), B (20%), Other

### Discussions

**haazybanj** (Wed 01 Nov 2023 08:23) - *Upvotes: 14*
To run the unit and integration tests on each pull request before it is merged, a solution that listens to pullRequestCreated events and runs a CodeBuild project to execute tests would be the most appropriate option.

Option B describes a solution that creates an Amazon EventBridge rule to match pullRequestCreated events from CodeCommit and configures a CodeBuild project to run the unit and integration tests, passing the CodeCommit repository and branch information from the event as a custom payload.

Therefore, option B is the correct answer.

---

**nickp84** (Wed 14 May 2025 19:49) - *Upvotes: 1*
The main requirement is: "To prevent this blockage, the development team wants to run the unit and integration tests on each pull request before it is merged."

Option B doesn’t stop someone from merging a PR even if the tests fail — it's informational only unless you add more enforcement logic manually (which Option A includes by design).

No Approval Integration:

There is no mechanism in B to integrate CodeBuild results with CodeCommit's approval rules or to block the merge.

That means a developer can still manually merge a PR before the tests complete or even if they fail.

Manual Enforcement Needed:

You'd need to custom build additional logic (e.g., using AWS Lambda or webhooks) to actually enforce that tests must pass before allowing a merge — which Option A handles automatically with approval rules.

---

**that1guy** (Wed 13 Nov 2024 19:15) - *Upvotes: 1*
These days it would be C instead of B, it's very common to reuse the same pipeline but with conditions to skip certain steps depending on the branch.

https://aws.amazon.com/blogs/devops/aws-codepipeline-adds-support-for-branch-based-development-and-monorepos/

---

**jojom19980** (Sun 18 Aug 2024 20:49) - *Upvotes: 2*
The Answer should Be A because this option is allows test the code and the approval is depending on test's result

---

**vn_thanhtung** (Wed 20 Nov 2024 14:17) - *Upvotes: 1*
The development team reviews and merges the pull requests, and then the pipeline builds and tests the application.

---

**Jon_aws** (Sun 02 Feb 2025 13:05) - *Upvotes: 1*
shouldn't review be after build and test?

---

**thanhnv142** (Thu 01 Aug 2024 16:27) - *Upvotes: 2*
A is definitely correct: <The development team reviews and merges the pull requests> and <the development team wants to run the unit and integration tests on each pull request before it is merged> means the dev team always review all pull requests and they need a solution to test committed code before merging to main. option A allow them to do tests and manually approve it before allow merging
B C and D: no mention of the step that allow the dev team to manually approve the merge.

---

**2pk** (Fri 10 May 2024 20:31) - *Upvotes: 2*
B is the answer . D is wrong. Code commit only can setup notification rule to SNS topics or Chatbot.

---

**BaburTurk** (Fri 08 Mar 2024 17:06) - *Upvotes: 3*
https://aws.amazon.com/blogs/devops/validating-aws-codecommit-pull-requests-with-aws-codebuild-and-aws-lambda/

---

**Seoyong** (Wed 21 Feb 2024 11:02) - *Upvotes: 1*
Only D covers create pull request and update pull request

---


<br/>

## Question 70

*Date: May 3, 2023, 4:25 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an application that runs on a fleet of Amazon EC2 instances. The application requires frequent restarts. The application logs contain error messages when a restart is required. The application logs are published to a log group in Amazon CloudWatch Logs.
An Amazon CloudWatch alarm notifies an application engineer through an Amazon Simple Notification Service (Amazon SNS) topic when the logs contain a large number of restart-related error messages. The application engineer manually restarts the application on the instances after the application engineer receives a notification from the SNS topic.
A DevOps engineer needs to implement a solution to automate the application restart on the instances without restarting the instances.
Which solution will meet these requirements in the MOST operationally efficient manner?

**Options:**
- A. Configure an AWS Systems Manager Automation runbook that runs a script to restart the application on the instances. Configure the SNS topic to invoke the runbook.
- B. Create an AWS Lambda function that restarts the application on the instances. Configure the Lambda function as an event destination of the SNS topic.
- C. Configure an AWS Systems Manager Automation runbook that runs a script to restart the application on the instances. Create an AWS Lambda function to invoke the runbook. Configure the Lambda function as an event destination of the SNS topic.
- D. Configure an AWS Systems Manager Automation runbook that runs a script to restart the application on the instances. Configure an Amazon EventBridge rule that reacts when the CloudWatch alarm enters ALARM state. Specify the runbook as a target of the rule.

> **Suggested Answer:** D
> **Community Vote:** D (71%), C (16%), 13%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**daburahjail** (Mon 18 Sep 2023 05:35) - *Upvotes: 23*
It is debatable, as both C and D are correct and simple in their own ways, however, take a look at the number of components in each approach:

C: CW -> SNS -> LAMBDA -> SSM (4)
D: CW -> EVENTBRIDGE -> SSM (3)

There is an extra component (SNS) to maintain on C, also, there is some coding involved on this option, which also needs to be maintained.
Even if we already have the SNS created on option C, we still have to go there to remove the notification and configure the lambda invocation.

Option D has fewer components, and require less customization.

---

**ParagSanyashiv** (Tue 09 May 2023 06:41) - *Upvotes: 10*
C makes more sense here

---

**jamesf** (Fri 26 Jul 2024 09:38) - *Upvotes: 1*
D is more simpler solution than C.

---

**xdkonorek2** (Sun 30 Jun 2024 20:09) - *Upvotes: 1*
D)

B is wrong since it's way easier to use SSM automation runbook to execute logic inside instance using "run command" action within automation runbook than doing this with lambda

---

**zijo** (Mon 25 Mar 2024 18:26) - *Upvotes: 1*
A is not possible - AWS Systems Manager (SSM) Run Command or Automation runbooks cannot be directly triggered by an Amazon SNS topic.
Then C and D are the next best options. C is flexible but D is the most simple solution

---

**Diego1414** (Sun 18 Feb 2024 22:21) - *Upvotes: 3*
Both C and D are valid answers. However, D is less complicated.

---

**jojom19980** (Sun 18 Feb 2024 21:59) - *Upvotes: 2*
C is correct , But D is more easy to implement , cost saving, managed services by AWS ^_^

---

**thanhnv142** (Thu 01 Feb 2024 17:39) - *Upvotes: 1*
B is correct: <implement a solution to automate the application restart on the instances> means we need to automate the restart step. We can use lambda, AWS system manager. <CloudWatch alarm notifies an application engineer through an Amazon Simple Notification Service> means we already have the alarm. We just need to simply trigger the restart process with lambda
A, C and D are all too complicated compared to B. They ask for "the MOST operationally efficient manner", not the most complicated one

---

**vn_thanhtung** (Mon 13 May 2024 10:02) - *Upvotes: 1*
Option B not correct.
https://docs.aws.amazon.com/systems-manager/latest/userguide/running-automations-event-bridge.html

---

**thanhnv142** (Thu 01 Feb 2024 17:39) - *Upvotes: 1*
B is correct: <implement a solution to automate the application restart on the instances> means we need to automate the restart step. We can use lambda, AWS system manager. <CloudWatch alarm notifies an application engineer through an Amazon Simple Notification Service> means we already have the alarm. We just need to simply trigger the restart process with lambda
A, C and D are all too complicated compared to A. They ask for "the MOST operationally efficient manner", not the most complicated one

---


<br/>

## Question 71

*Date: May 3, 2023, 6:32 a.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer at a company is supporting an AWS environment in which all users use AWS IAM Identity Center (AWS Single Sign-On). The company wants to immediately disable credentials of any new IAM user and wants the security team to receive a notification.
Which combination of steps should the DevOps engineer take to meet these requirements? (Choose three.)

**Options:**
- A. Create an Amazon EventBridge rule that reacts to an IAM CreateUser API call in AWS CloudTrail.
- B. Create an Amazon EventBridge rule that reacts to an IAM GetLoginProfile API call in AWS CloudTrail.
- C. Create an AWS Lambda function that is a target of the EventBridge rule. Configure the Lambda function to disable any access keys and delete the login profiles that are associated with the IAM user.
- D. Create an AWS Lambda function that is a target of the EventBridge rule. Configure the Lambda function to delete the login profiles that are associated with the IAM user.
- E. Create an Amazon Simple Notification Service (Amazon SNS) topic that is a target of the EventBridge rule. Subscribe the security team's group email address to the topic.
- F. Create an Amazon Simple Queue Service (Amazon SQS) queue that is a target of the Lambda function. Subscribe the security team's group email address to the queue.

> **Suggested Answer:** ACE
> **Community Vote:** ACE (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**mrjaehong** (Wed 13 Nov 2024 07:42) - *Upvotes: 1*
The IAM user that was created cannot have an access key from the beginning. You need to log in and get an access key.

---

**0b005fc** (Wed 17 Apr 2024 21:53) - *Upvotes: 1*
Took the test 4/15 and passed. Almost all of the questions appeared.
ACE is correct.

---

**thanhnv142** (Fri 02 Feb 2024 10:18) - *Upvotes: 4*
ACE are correct: <disable credentials of any new IAM user> means disable all access key and profile related to the user. <the security team to receive a notification> means SNS
B: GetLoginProfile API is not equal to creating new user
D: we should delete all access key and profile related to the user, not just profile
F: we need SNS, not SQS

---

**khchan123** (Wed 17 Jan 2024 16:15) - *Upvotes: 2*
Answer ACE

---

**yuliaqwerty** (Tue 09 Jan 2024 11:48) - *Upvotes: 1*
Answer ACE

---

**Snape** (Thu 13 Jul 2023 06:08) - *Upvotes: 3*
No Brainer

---

**Jeanphi72** (Wed 03 May 2023 16:22) - *Upvotes: 4*
My answer ACE

---

**haazybanj** (Wed 03 May 2023 06:32) - *Upvotes: 3*
ACE is the right answer

---


<br/>

## Question 72

*Date: May 3, 2023, 6:33 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company wants to set up a continuous delivery pipeline. The company stores application code in a private GitHub repository. The company needs to deploy the application components to Amazon Elastic Container Service (Amazon ECS). Amazon EC2, and AWS Lambda. The pipeline must support manual approval actions.
Which solution will meet these requirements?

**Options:**
- A. Use AWS CodePipeline with Amazon ECS. Amazon EC2, and Lambda as deploy providers.
- B. Use AWS CodePipeline with AWS CodeDeploy as the deploy provider.
- C. Use AWS CodePipeline with AWS Elastic Beanstalk as the deploy provider.
- D. Use AWS CodeDeploy with GitHub integration to deploy the application.

> **Suggested Answer:** B
> **Community Vote:** B (80%), A (17%), 3%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**haazybanj** (Wed 03 May 2023 06:33) - *Upvotes: 9*
B is correct

---

**Just_Ninja** (Wed 12 Jul 2023 10:16) - *Upvotes: 7*
Because the Term "The pipeline must support manual approval actions."
That is not possible without a pipeline :)

---

**1rob** (Tue 31 Dec 2024 13:57) - *Upvotes: 1*
Lambda is not defined as a deployment provider. Only as an "invoke" option. Amazon ECS is possible as deploy provider , check https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html where it gives: CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services. . So I go for B.

---

**Zdujgfr567783ff** (Wed 25 Dec 2024 01:49) - *Upvotes: 1*
Option B is partially correct but lacks native support for ECS deployments. AWS CodeDeploy is excellent for deploying to EC2 and Lambda, but it doesn't natively handle ECS deployments without additional configuration.

---

**Zdujgfr567783ff** (Wed 25 Dec 2024 01:47) - *Upvotes: 2*
asked chat GPT says a

---

**hzaki** (Sun 01 Sep 2024 14:00) - *Upvotes: 1*
A is correct, CodeDeploy can't deploy the ECS

---

**thanhnv142** (Fri 02 Feb 2024 10:40) - *Upvotes: 1*
A is correct

---

**thanhnv142** (Mon 12 Feb 2024 11:50) - *Upvotes: 1*
Correction: B is correct

---

**due** (Tue 14 Nov 2023 08:58) - *Upvotes: 4*
The solution for deploy ECS by codePipeline and codeDeploy

Create your CodeDeploy application and deployment group (ECS compute platform
https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-ecs-ecr-codedeploy.html#tutorials-ecs-ecr-codedeploy-deployment

---

**RVivek** (Tue 19 Sep 2023 10:00) - *Upvotes: 3*
Why not A ?
B (Code depoly providr does not support ECS)
D does not have "codepipeleine" and the question says ""The pipeline must support manual approval actions."

So A is the only feasible option

---


<br/>

## Question 73

*Date: April 8, 2023, 2:33 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an application that runs on Amazon EC2 instances that are in an Auto Scaling group. When the application starts up. the application needs to process data from an Amazon S3 bucket before the application can start to serve requests.
The size of the data that is stored in the S3 bucket is growing. When the Auto Scaling group adds new instances, the application now takes several minutes to download and process the data before the application can serve requests. The company must reduce the time that elapses before new EC2 instances are ready to serve requests.
Which solution is the MOST cost-effective way to reduce the application startup time?

**Options:**
- A. Configure a warm pool for the Auto Scaling group with warmed EC2 instances in the Stopped state. Configure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook when the application is ready to serve requests.
- B. Increase the maximum instance count of the Auto Scaling group. Configure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook when the application is ready to serve requests.
- C. Configure a warm pool for the Auto Scaling group with warmed EC2 instances in the Running state. Configure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook when the application is ready to serve requests.
- D. Increase the maximum instance count of the Auto Scaling group. Configure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook and to place the new instance in the Standby state when the application is ready to serve requests.

> **Suggested Answer:** A
> **Community Vote:** A (88%), 13%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**haazybanj** (Mon 01 May 2023 07:37) - *Upvotes: 18*
Option A is the most cost-effective solution. By configuring a warm pool of EC2 instances in the Stopped state, the company can reduce the time it takes for new instances to be ready to serve requests. When the Auto Scaling group launches a new instance, it can attach the stopped EC2 instance from the warm pool. The instance can then be started up immediately, rather than having to wait for the data to be downloaded and processed. This reduces the overall startup time for the application.

Option C is also a solution that involves a warm pool of EC2 instances, but the instances are in the Running state. This means that they are already running and incurring costs, even though they are not currently serving requests. This is not a cost-effective solution.

---

**jamesf** (Fri 26 Jul 2024 09:45) - *Upvotes: 2*
keywords: MOST cost-effective way to reduce the application startup time

---

**stoy123** (Mon 25 Mar 2024 14:24) - *Upvotes: 1*
C " The company must reduce the time that elapses before new EC2 instances are ready to serve requests."!!!!!!!!!! this cannot happen with a stopped instance as it will still need to read the data from S3 upon startup,

---

**Jay_2pt0_1** (Wed 01 May 2024 10:50) - *Upvotes: 1*
I thought this, as well, but A appears to be correct. See https://aws.amazon.com/blogs/compute/scaling-your-applications-faster-with-ec2-auto-scaling-warm-pools/

---

**Shasha1** (Thu 29 Feb 2024 14:54) - *Upvotes: 1*
A
for warm pool in the hibernated or stop status we will pay only for the attached EBS volume, therefore its much cost effective rather than running instance

---

**dzn** (Sun 25 Feb 2024 04:19) - *Upvotes: 1*
Warm Pool allows instances to be set to a stopped state after performing any process (e.g., running initialization scripts, warm-up tasks, etc.).

---

**thanhnv142** (Fri 02 Feb 2024 10:45) - *Upvotes: 1*
A is correct: the question says <the application needs to process data from an Amazon S3 bucket before the application can start to serve requests> but <The size of the data that is stored in the S3 bucket is growing>. This means we should maintain a warm pool for EC2 so that they are always ready to process data (reduce the time that elapses before new EC2 instances are ready)
B and D: no mention of warmpool
C: If the instance is up and running, no need to configure warm pool

---

**zolthar_z** (Thu 23 Nov 2023 16:31) - *Upvotes: 2*
Answer is A, the question is cost-effective, and even with A you will have less wait time to download the S3 data, it will download the delta from the warm up process to ready to join to ASG

---

**Jaguaroooo** (Sat 06 Jan 2024 04:53) - *Upvotes: 1*
A&C are both good in terms of solutions, however, the caveat here is the "cost-effective" solution and that's why I agree with A.
https://aws.amazon.com/blogs/compute/scaling-your-applications-faster-with-ec2-auto-scaling-warm-pools/

---

**RVivek** (Tue 12 Sep 2023 10:29) - *Upvotes: 1*
excerpt from the url: https://aws.amazon.com/blogs/compute/scaling-your-applications-faster-with-ec2-auto-scaling-warm-pools/
EC2 Auto Scaling Warm Pools works by launching a configured number of EC2 instances in the background, allowing any lengthy application initialization processes to run as necessary, and then stopping those instances until they are needed

---


<br/>

## Question 74

*Date: May 1, 2023, 7:39 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company is using an AWS CodeBuild project to build and package an application. The packages are copied to a shared Amazon S3 bucket before being deployed across multiple AWS accounts.
The buildspec.yml file contains the following:

The DevOps engineer has noticed that anybody with an AWS account is able to download the artifacts.
What steps should the DevOps engineer take to stop this?

**Options:**
- A. Modify the post_build command to use --acl public-read and configure a bucket policy that grants read access to the relevant AWS accounts only.
- B. Configure a default ACL for the S3 bucket that defines the set of authenticated users as the relevant AWS accounts only and grants read-only access.
- C. Create an S3 bucket policy that grants read access to the relevant AWS accounts and denies read access to the principal “*”.
- D. Modify the post_build command to remove --acl authenticated-read and configure a bucket policy that allows read access to the relevant AWS accounts only.

> **Suggested Answer:** D
> **Community Vote:** D (82%), A (18%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**haazybanj** (Mon 01 May 2023 07:39) - *Upvotes: 14*
D is correct

---

**beanxyz** (Tue 22 Aug 2023 12:31) - *Upvotes: 5*
--acl authenticated-read means any authenticated users can read the S3 bucket. We should remove it and configure the bucket policy to explicitly grant access

---

**beanxyz** (Tue 22 Aug 2023 12:31) - *Upvotes: 6*
I mean D...

---

**jamesf** (Fri 26 Jul 2024 09:48) - *Upvotes: 3*
"--acl authenticated-read" means any authenticated users can read the S3 bucket. We should remove it and configure the bucket policy to explicitly grant access

---

**zijo** (Tue 16 Apr 2024 21:00) - *Upvotes: 2*
D is the answer

ACL-authenticated users: This refers to any user who has successfully authenticated with AWS credentials, including IAM users and federated users. It does not include anonymous users (public access).
It's generally recommended to use bucket policies for access control in S3 rather than ACLs. Bucket policies offer more granular control and better security practices. You can achieve "acl-authenticated reads" access using a bucket policy as well.

---

**dzn** (Sun 25 Feb 2024 04:58) - *Upvotes: 4*
`remove --acl authenticated-read` is required to fulfill the requirement.

---

**thanhnv142** (Fri 02 Feb 2024 11:26) - *Upvotes: 1*
B is correct: In the "buildspec.yml file", we see that there is "--acl authenticated-read". This allow all aws users who successfully authen to AWS can download the file. To restrict access, we need to modify ACL that only grant access to some specific users.
Note that we should not use bucket policy because it will affect all ojbects in the bucket (that is why it is called BUCKET policy). We only need to restrict acess to an object, then ACL is the right choice.
A is incorrect: Use use --acl public-read means we allow all user to access the object
C and D: Use bucket policy, which is incorrect

---

**zolthar_z** (Thu 23 Nov 2023 16:33) - *Upvotes: 2*
D is correct

---


<br/>

## Question 75

*Date: May 1, 2023, 7:42 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has developed a serverless web application that is hosted on AWS. The application consists of Amazon S3. Amazon API Gateway, several AWS Lambda functions, and an Amazon RDS for MySQL database. The company is using AWS CodeCommit to store the source code. The source code is a combination of AWS Serverless Application Model (AWS SAM) templates and Python code.
A security audit and penetration test reveal that user names and passwords for authentication to the database are hardcoded within CodeCommit repositories. A DevOps engineer must implement a solution to automatically detect and prevent hardcoded secrets.
What is the MOST secure solution that meets these requirements?

**Options:**
- A. Enable Amazon CodeGuru Profiler. Decorate the handler function with @with_lambda_profiler(). Manually review the recommendation report. Write the secret to AWS Systems Manager Parameter Store as a secure string. Update the SAM templates and the Python code to pull the secret from Parameter Store.
- B. Associate the CodeCommit repository with Amazon CodeGuru Reviewer. Manually check the code review for any recommendations. Choose the option to protect the secret. Update the SAM templates and the Python code to pull the secret from AWS Secrets Manager.
- C. Enable Amazon CodeGuru Profiler. Decorate the handler function with @with_lambda_profiler(). Manually review the recommendation report. Choose the option to protect the secret. Update the SAM templates and the Python code to pull the secret from AWS Secrets Manager.
- D. Associate the CodeCommit repository with Amazon CodeGuru Reviewer. Manually check the code review for any recommendations. Write the secret to AWS Systems Manager Parameter Store as a string. Update the SAM templates and the Python code to pull the secret from Parameter Store.

> **Suggested Answer:** B
> **Community Vote:** B (96%), 4%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**haazybanj** (Wed 01 Nov 2023 08:42) - *Upvotes: 12*
B
The MOST secure solution that meets the requirement of automatically detecting and preventing hardcoded secrets is to use AWS CodeGuru Reviewer to check the code for any hardcoded secrets, and then update the SAM templates and Python code to retrieve the secrets from AWS Secrets Manager.

Option B is the correct answer. By associating the CodeCommit repository with Amazon CodeGuru Reviewer, the code can be checked for any hardcoded secrets during code reviews. When a hardcoded secret is detected, CodeGuru Reviewer will recommend updating the code to retrieve the secret from a secure storage service like AWS Secrets Manager. The DevOps engineer can choose the option to protect the secret and then update the SAM templates and Python code to retrieve the secret from AWS Secrets Manager instead of hardcoding it in the code.

---

**rhinozD** (Fri 15 Dec 2023 15:57) - *Upvotes: 8*
B is correct.
CodeGuru Reviewer for security problems.
Amazon CodeGuru Profiler is for performance.

---

**thanhnv142** (Fri 02 Aug 2024 11:03) - *Upvotes: 1*
B is correct: <implement a solution to automatically detect and prevent hardcoded secrets> means we need CodeGuru reviewer to analyze the code and uncover hardcoded credentials.
A and C: no mention of CodeGuru reviewer
D: using System Manager Parameter store is a good method to avoid hardcoded credentials. However, the question requires <the MOST secure solution>, so we should use AWS secret manager (option B). It costs more than Para store, but more secure.

---

**a16a848** (Tue 25 Jun 2024 06:48) - *Upvotes: 1*
I'd say it's C, because the system to examine includes Python code and CodeGuru profiles for Python needs the decorator: https://docs.aws.amazon.com/codeguru/latest/profiler-ug/python-lambda.html

---

**Aja1** (Sat 27 Jan 2024 14:32) - *Upvotes: 1*
option C
https://docs.aws.amazon.com/codeguru/latest/profiler-ug/python-lambda-command-line.html

---

**Aja1** (Thu 08 Feb 2024 18:12) - *Upvotes: 6*
Sorry B

Amazon CodeGuru Reviewer and Amazon CodeGuru Profiler are both tools that can be used to improve the quality and security of your code. However, they have different strengths and weaknesses.

CodeGuru Reviewer is a static code analysis tool that can be used to find potential defects in your code. It can scan your code for hardcoded secrets, security vulnerabilities, and other potential problems. CodeGuru Reviewer can also provide recommendations on how to fix the problems that it finds.

CodeGuru Profiler is a dynamic code analysis tool that can be used to understand how your code performs. It can track the performance of your code, identify bottlenecks, and suggest ways to improve performance. CodeGuru Profiler can also be used to find potential memory leaks and other performance problems.

---

**MarDog** (Wed 20 Dec 2023 00:07) - *Upvotes: 2*
Definitely B.

---


<br/>

## Question 76

*Date: May 6, 2023, 7 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company is using Amazon S3 buckets to store important documents. The company discovers that some S3 buckets are not encrypted. Currently, the company’s IAM users can create new S3 buckets without encryption. The company is implementing a new requirement that all S3 buckets must be encrypted.

A DevOps engineer must implement a solution to ensure that server-side encryption is enabled on all existing S3 buckets and all new S3 buckets. The encryption must be enabled on new S3 buckets as soon as the S3 buckets are created. The default encryption type must be 256-bit Advanced Encryption Standard (AES-256).

Which solution will meet these requirements?

**Options:**
- A. Create an AWS Lambda function that is invoked periodically by an Amazon EventBridge scheduled rule. Program the Lambda function to scan all current S3 buckets for encryption status and to set AES-256 as the default encryption for any S3 bucket that does not have an encryption configuration.
- B. Set up and activate the s3-bucket-server-side-encryption-enabled AWS Config managed rule. Configure the rule to use the AWS-EnableS3BucketEncryption AWS Systems Manager Automation runbook as the remediation action. Manually run the re-evaluation process to ensure that existing S3 buckets are compliant.
- C. Create an AWS Lambda function that is invoked by an Amazon EventBridge event rule. Define the rule with an event pattern that matches the creation of new S3 buckets. Program the Lambda function to parse the EventBridge event, check the configuration of the S3 buckets from the event, and set AES-256 as the default encryption.
- D. Configure an IAM policy that denies the s3:CreateBucket action if the s3:x-amz-server-side-encryption condition key has a value that is not AES-256. Create an IAM group for all the company’s IAM users. Associate the IAM policy with the IAM group.

> **Suggested Answer:** B
> **Community Vote:** B (88%), 13%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**paali** (Sat 27 May 2023 15:32) - *Upvotes: 13*
B caters to both existing and new buckets.
C is triggered on when new bucket is created, existing buckets are not handled by the event.

---

**Zoe_zoe** (Sat 06 May 2023 07:00) - *Upvotes: 10*
B to me

---

**Gomer** (Wed 12 Jun 2024 04:22) - *Upvotes: 2*
I think neither "B" or "C" is complete solution. They both need to be done to deal with both existing and new buckets.
A carefull reading of the question doesn't preclude the need to do both.
However, the specific and emphasized criteria of enabling encryption "as soon as the S3 buckets are created" can only be done by "C" (event driven action)
I think this may be a trick question. I'm very confident they are defining an event driven action as part of the solution, and only "C" provides that.

B: (NO) "Manually run the re-evaluation process to ensure that existing S3 buckets are compliant."
Comment: Doesn't achieve "encryption must be enabled on new S3 buckets as soon as the S3 buckets are created."

---

**dzn** (Sun 25 Feb 2024 05:16) - *Upvotes: 1*
`s3-bucket-server-side-encryption-enabled` checks if your Amazon S3 bucket either has the Amazon S3 default encryption enabled or that the Amazon S3 bucket policy explicitly denies put-object requests without server side encryption that uses AES-256 or AWS Key Management Service.

---

**thanhnv142** (Fri 02 Feb 2024 12:25) - *Upvotes: 2*
A is correct: <implement a solution to ensure that server-side encryption is enabled on all existing S3 buckets and all new S3 buckets>: We can use lambda to configure all S3. Use Eventbridge to schedule-run lambda.
B: This option uses AWS config rule to activate AWS-EnableS3BucketEncryption AWS Systems Manager Automation runbook, which is incorrect. Remember that AWS config have no action and cannot trigger anything. It only collect data and report. Additionally, this option does not mention actions to new S3 bucket
C: <define the rule with an event pattern that matches the creation of new S3 buckets> means that this only affect newly-created bucket, not existing ones.
D: No mention of enforcing encryption on S3

Note: Should not use chatgpt for this exam, its answers are mostly wrong

---

**thanhnv142** (Tue 13 Feb 2024 10:33) - *Upvotes: 1*
Correct: D

---

**davdan99** (Mon 08 Jan 2024 13:04) - *Upvotes: 5*
Answer is B

https://docs.aws.amazon.com/config/latest/developerguide/s3-bucket-server-side-encryption-enabled.html

---

**Jaguaroooo** (Sat 06 Jan 2024 13:51) - *Upvotes: 1*
I would have chose B over D because aws config can do this with lambda.

---

**Jaguaroooo** (Sat 06 Jan 2024 13:50) - *Upvotes: 1*
A has automation. I didn't like B: because of this statement: Manually run the re-evaluation process to ensure that existing S3 buckets are compliant.

---

**Jamshif01** (Mon 25 Dec 2023 17:18) - *Upvotes: 1*
Amazon S3 Encrypts New Objects By Default
https://aws.amazon.com/blogs/aws/amazon-s3-encrypts-new-objects-by-default/#:~:text=At%20AWS%2C%20security%20is%20the,specify%20a%20different%20encryption%20option.

---


<br/>

## Question 77

*Date: May 7, 2023, 11:30 a.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer is architecting a continuous development strategy for a company’s software as a service (SaaS) web application running on AWS. For application and security reasons, users subscribing to this application are distributed across multiple Application Load Balancers (ALBs), each of which has a dedicated Auto Scaling group and fleet of Amazon EC2 instances. The application does not require a build stage, and when it is committed to AWS CodeCommit, the application must trigger a simultaneous deployment to all ALBs, Auto Scaling groups, and EC2 fleets.

Which architecture will meet these requirements with the LEAST amount of configuration?

**Options:**
- A. Create a single AWS CodePipeline pipeline that deploys the application in parallel using unique AWS CodeDeploy applications and deployment groups created for each ALB-Auto Scaling group pair.
- B. Create a single AWS CodePipeline pipeline that deploys the application using a single AWS CodeDeploy application and single deployment group.
- C. Create a single AWS CodePipeline pipeline that deploys the application in parallel using a single AWS CodeDeploy application and unique deployment group for each ALB-Auto Scaling group pair.
- D. Create an AWS CodePipeline pipeline for each ALB-Auto Scaling group pair that deploys the application using an AWS CodeDeploy application and deployment group created for the same ALB-Auto Scaling group pair.

> **Suggested Answer:** C
> **Community Vote:** C (77%), B (19%), 4%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**rhinozD** (Thu 15 Jun 2023 15:06) - *Upvotes: 14*
You can just use one CodeDeploy application and multiple deployment groups in this case.
so C.

---

**nickp84** (Wed 14 May 2025 20:30) - *Upvotes: 1*
B. Single deployment group:
Won’t support multiple Auto Scaling groups and ALBs unless they’re all in one group, which is not the case here.
Cannot handle independent ALB/ASG pairs cleanly.
Doesn’t scale well

---

**steli0** (Mon 25 Nov 2024 11:14) - *Upvotes: 1*
I was about to vote B since the link from xdkonorek2 shows that one deployment can include up to 10 ELBs. Nevertheless the question says multiple instead of 10.

---

**trungtd** (Tue 09 Jul 2024 02:25) - *Upvotes: 1*
Option B not feasible as it assumes a single deployment group can manage deployments across multiple ALBs and Auto Scaling groups simultaneously, which is not supported.

---

**xdkonorek2** (Tue 18 Jun 2024 19:37) - *Upvotes: 2*
https://aws.amazon.com/about-aws/whats-new/2023/10/aws-codedeploy-multiple-load-balancers-amazon-ec2-applications/

---

**xdkonorek2** (Sun 21 Apr 2024 13:53) - *Upvotes: 3*
B is the simplest :)

During creation of deployment group:
1. select "Amazon EC2 Auto Scaling groups"
2. tip appears: "You can select up to 10 Amazon EC2 Auto Scaling groups to deploy your application revision to."

---

**that1guy** (Wed 24 Apr 2024 19:18) - *Upvotes: 2*
Also B for me, you can target multiple ASGs as part of one deployment: https://docs.aws.amazon.com/codedeploy/latest/APIReference/API_TargetInstances.html#CodeDeploy-Type-TargetInstances-autoScalingGroups

---

**thanhnv142** (Fri 02 Feb 2024 16:56) - *Upvotes: 4*
C is correct: <the application must trigger a simultaneous deployment> means deployment in parallel
B and D: no mention of deployment in parallel
A: <unique AWS CodeDeploy applications and deployment groups created for each ALB-Auto Scaling group pair> means there are multiple AWS CodeDeploy applications and deployment groups for each site, which is unnecessary

---

**Aja1** (Thu 27 Jul 2023 14:05) - *Upvotes: 1*
Option C
deployed in parallel to all ALB-Auto Scaling group pairs simultaneously. This means that the deployment process is efficient and fast, and all ALBs and Auto Scaling groups receive updates at the same time.

---

**devnv** (Sun 14 May 2023 07:03) - *Upvotes: 3*
C is the correct answer.

---


<br/>

## Question 78

*Date: May 7, 2023, 11:47 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company is hosting a static website from an Amazon S3 bucket. The website is available to customers at example.com. The company uses an Amazon Route 53 weighted routing policy with a TTL of 1 day. The company has decided to replace the existing static website with a dynamic web application. The dynamic web application uses an Application Load Balancer (ALB) in front of a fleet of Amazon EC2 instances.

On the day of production launch to customers, the company creates an additional Route 53 weighted DNS record entry that points to the ALB with a weight of 255 and a TTL of 1 hour. Two days later, a DevOps engineer notices that the previous static website is displayed sometimes when customers navigate to example.com.

How can the DevOps engineer ensure that the company serves only dynamic content for example.com?

**Options:**
- A. Delete all objects, including previous versions, from the S3 bucket that contains the static website content.
- B. Update the weighted DNS record entry that points to the S3 bucket. Apply a weight of 0. Specify the domain reset option to propagate changes immediately.
- C. Configure webpage redirect requests on the S3 bucket with a hostname that redirects to the ALB.
- D. Remove the weighted DNS record entry that points to the S3 bucket from the example.com hosted zone. Wait for DNS propagation to become complete.

> **Suggested Answer:** D
> **Community Vote:** D (80%), B (20%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**2pk** (Mon 06 Nov 2023 09:56) - *Upvotes: 9*
D is the answer
B wrong because
Route 53 initially considers only the nonzero weighted records, if any.

If all the records that have a weight greater than 0 are unhealthy, then Route 53 considers the zero-weighted records.

---

**92a2133** (Thu 22 May 2025 14:02) - *Upvotes: 1*
Went with D because correct me if I'm wrong almost all changes to anything DNS related does not happen instantly and usually takes some time to propagate

---

**Rs123x** (Thu 24 Apr 2025 16:21) - *Upvotes: 1*
ChatGPT suggests B...

---

**steli0** (Mon 25 Nov 2024 11:25) - *Upvotes: 1*
B would be correct if instead of domain reset option (which doesn't exist) there was a TTL decrease or nothing.

---

**jamesf** (Thu 01 Aug 2024 15:53) - *Upvotes: 2*
D is correct

Not B as
- Route 53 initially considers only the nonzero weighted records, if any.
- If all the records that have a weight greater than 0 are unhealthy, then Route 53 considers the zero-weighted records.
https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-weighted.html

Besides, in B, <domain reset option to propagate changes immediately>, i don't think DNS record will update immediately

---

**jojom19980** (Sun 18 Feb 2024 22:32) - *Upvotes: 2*
D is correct but I will go with B because is more save to the customers, some clients have the old record (TTL 1 day) so after 1 day I can confirm that all the clients have the the new DNs record so I can delete the record

---

**thanhnv142** (Fri 02 Feb 2024 17:15) - *Upvotes: 4*
D is correct:
A: should not delete all objects from S3, they change nothing
C: should not do this, we have a more efficient method
B: < domain reset option to propagate changes immediately>: there is no such thing. DNS record will expire after TTL. Cannot force DNS resolvers to query for DNS record before TTL expire

---

**RVivek** (Tue 12 Sep 2023 12:38) - *Upvotes: 3*
B- is incorrect , as gigi_devops has metioned setting 0 weight is not enough. Also reset domain option is not available.

---

**ixdb** (Tue 15 Aug 2023 12:11) - *Upvotes: 2*
D is right.
Just setting the weight to 0 does not ensure that traffic will not go to example.com.

---

**gigi_devops** (Wed 02 Aug 2023 03:50) - *Upvotes: 3*
Setting the weight to 0 is not enough. So C is the best answer. https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-weighted.html#:~:text=Si%20tous%20les%20enregistrements%20dont%20le%20poids%20est%20sup%C3%A9rieur%20%C3%A0%200%20ne%20sont%20pas%20sains%2C%20Route%2053%20prend%20en%20compte%20les%20enregistrements%20pond%C3%A9r%C3%A9s%20%C3%A0%20z%C3%A9ro

---


<br/>

## Question 79

*Date: May 7, 2023, 11:54 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company is implementing AWS CodePipeline to automate its testing process. The company wants to be notified when the execution state fails and used the following custom event pattern in Amazon EventBridge:



Which type of events will match this event pattern?

**Options:**
- A. Failed deploy and build actions across all the pipelines
- B. All rejected or failed approval actions across all the pipelines
- C. All the events across all pipelines
- D. Approval actions across all the pipelines

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**willhsien** (Wed 27 Dec 2023 04:14) - *Upvotes: 14*
Use this sample event pattern to capture all rejected or failed approval actions across all the pipelines.
https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html

---

**thanhnv142** (Fri 02 Aug 2024 16:25) - *Upvotes: 3*
B is correct: <state:failed and category:approval> means failed approval
A, C and D: no mention of approval

---

**gdtypk** (Fri 24 Nov 2023 00:44) - *Upvotes: 3*
https://docs.aws.amazon.com/ja_jp/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html

---

**marcoforexam** (Tue 07 Nov 2023 12:54) - *Upvotes: 1*
category: approval

---


<br/>

## Question 80

*Date: May 5, 2023, 10:14 p.m.
Disclaimers:
- ExamTopics website is not rel*

An application running on a set of Amazon EC2 instances in an Auto Scaling group requires a configuration file to operate. The instances are created and maintained with AWS CloudFormation. A DevOps engineer wants the instances to have the latest configuration file when launched, and wants changes to the configuration file to be reflected on all the instances with a minimal delay when the CloudFormation template is updated. Company policy requires that application configuration files be maintained along with AWS infrastructure configuration files in source control.

Which solution will accomplish this?

**Options:**
- A. In the CloudFormation template, add an AWS Config rule. Place the configuration file content in the rule’s InputParameters property, and set the Scope property to the EC2 Auto Scaling group. Add an AWS Systems Manager Resource Data Sync resource to the template to poll for updates to the configuration.
- B. In the CloudFormation template, add an EC2 launch template resource. Place the configuration file content in the launch template. Configure the cfn-init script to run when the instance is launched, and configure the cfn-hup script to poll for updates to the configuration.
- C. In the CloudFormation template, add an EC2 launch template resource. Place the configuration file content in the launch template. Add an AWS Systems Manager Resource Data Sync resource to the template to poll for updates to the configuration.
- D. In the CloudFormation template, add CloudFormation init metadata. Place the configuration file content in the metadata. Configure the cfn-init script to run when the instance is launched, and configure the cfn-hup script to poll for updates to the configuration.

> **Suggested Answer:** D
> **Community Vote:** D (79%), B (21%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**4555894** (Fri 08 Mar 2024 17:18) - *Upvotes: 7*
Use the AWS::CloudFormation::Init type to include metadata on an Amazon EC2 instance for the cfn-init helper script. If your template calls the cfn-init script, the script looks for resource metadata rooted in the AWS::CloudFormation::Init metadata key. Reference:
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-init.html

---

**steli0** (Mon 25 Nov 2024 11:30) - *Upvotes: 1*
It's D

---

**jamesf** (Fri 26 Jul 2024 10:45) - *Upvotes: 2*
Require CloudFormation init for cfn-init and cfn-hup
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-hup.html
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-init.html

---

**thanhnv142** (Fri 02 Feb 2024 17:37) - *Upvotes: 3*
D is correct: <The instances are created and maintained with AWS CloudFormation> means we will only use ACF to satisfy the requirements of this question. <changes to the configuration file to be reflected on all the instances with a minimal delay when the CloudFormation template is updated> means we nead cfn-init, which is a daemon that check for updates and update the changes
A and C: no mention of cfn-init
B: no mention of CloudFormation init. we need CloudFormation init because cfn-init is specified in CloudFormation init key.

---

**khchan123** (Wed 17 Jan 2024 16:40) - *Upvotes: 4*
D. cfn-hup poll for cloudformation metadata. B is wrong because putting the config content in launch template instead of metadata, where cfn-hub is not able to poll.

---

**a54b16f** (Fri 12 Jan 2024 13:47) - *Upvotes: 2*
cfn-init is defined inside AWS::CloudFormation::Init

---

**yuliaqwerty** (Tue 09 Jan 2024 12:11) - *Upvotes: 1*
I vote for B

---

**Jaguaroooo** (Sat 06 Jan 2024 14:07) - *Upvotes: 4*
But what happened to the aspect of using source control?

---

**a16a848** (Mon 25 Dec 2023 08:12) - *Upvotes: 2*
Google Bart says it is B.
By using an EC2 launch template resource, the configuration file will be installed and configured on all instances when they are launched. The cfn-init script will also poll for updates to the configuration, so that all instances will have the latest configuration file as soon as it is updated.

In addition, the solution will comply with company policy by storing the configuration file in source control along with the AWS infrastructure configuration files. This will ensure that changes to the configuration file are tracked and managed in a consistent way.
Option C: Using an AWS Systems Manager Resource Data Sync resource alone is not enough to ensure that all instances have the latest configuration file. The cfn-init script is needed to install and configure the configuration file on the instance, and the cfn-hup script is needed to poll for updates to the configuration.

---

**svjl** (Sun 03 Dec 2023 18:25) - *Upvotes: 3*
Selected B:
You can have cfn on Launch Config and Launch Template
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-hup.html
https://stackoverflow.com/questions/54691327/cfn-init-for-cloudformation-launchtemplate

---


<br/>

## Question 81

*Date: May 15, 2023, 9:07 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company manages an application that stores logs in Amazon CloudWatch Logs. The company wants to archive the logs to an Amazon S3 bucket. Logs are rarely accessed after 90 days and must be retained for 10 years.

Which combination of steps should a DevOps engineer take to meet these requirements? (Choose two.)

**Options:**
- A. Configure a CloudWatch Logs subscription filter to use AWS Glue to transfer all logs to an S3 bucket.
- B. Configure a CloudWatch Logs subscription filter to use Amazon Kinesis Data Firehose to stream all logs to an S3 bucket.
- C. Configure a CloudWatch Logs subscription filter to stream all logs to an S3 bucket.
- D. Configure the S3 bucket lifecycle policy to transition logs to S3 Glacier after 90 days and to expire logs after 3.650 days.
- E. Configure the S3 bucket lifecycle policy to transition logs to Reduced Redundancy after 90 days and to expire logs after 3.650 days.

> **Suggested Answer:** BD
> **Community Vote:** BD (85%), CD (15%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**2pk** (Mon 15 May 2023 09:07) - *Upvotes: 6*
Amazon Kinesis Data Firehose simplifies the process of loading streaming data into S3 and provides automatic scaling, buffering, and retries.

---

**xxxdolorxxx** (Thu 10 Jul 2025 15:45) - *Upvotes: 1*
I vote B/D

---

**jamesf** (Fri 02 Aug 2024 02:42) - *Upvotes: 1*
B - keywords: continue stream but not one time task
D - keywords: S3 Glacier

---

**ericphl** (Sun 28 Jul 2024 12:19) - *Upvotes: 3*
vote B and D.
I initially thought the C is better than B, because Amazon Kinesis Data Firehose is primarily used for time-sensitive tasks, which is not suitable for this case, But when I read the C. I found the Directly streaming logs from cloudwatch log to s3 is not a feature provided by Cloudwatch.
So, I will go with B and D.

---

**Gomer** (Thu 13 Jun 2024 06:42) - *Upvotes: 3*
You can absolutly directly "export log data from your log groups to an Amazon S3 bucket"
However, this is a one time export, and NOT an ongoing stream.
If you want to steam continuously you have to use "subscription filter with Kinesis Data Streams, Lambda, or Firehose."
https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html
https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html
https://dev.to/aws-builders/automate-export-of-cloudwatch-logs-to-s3-bucket-using-lambda-with-eventbridge-trigger-2ieg

---

**zijo** (Fri 17 May 2024 15:56) - *Upvotes: 1*
Looks like creating subscription filters in AWS cloudwatch logs, there are only limited destination options. There is no S3 as a direct destination. You have to either create Elasticsearch or Kinesis or Kinesis Firehose or Lambda subscription filters. Given the choices we have, we need to pick B & D

---

**Jay_2pt0_1** (Thu 02 May 2024 10:31) - *Upvotes: 1*
C & D for the reasons that thanhnv142 mentioned.

---

**vn_thanhtung** (Mon 13 May 2024 12:11) - *Upvotes: 2*
https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html
Pls check link. You can use a subscription filter with Kinesis Data Streams, Lambda, or Firehose. Logs that are sent to a receiving service through a subscription filter are base64 encoded and compressed with the gzip format. correct is B and D

---

**Heyang** (Tue 27 Feb 2024 11:29) - *Upvotes: 2*
CD，The question does not mention trying to switch to S3 in real time. C is more cost-effective.
https://docs.aws.amazon.com/zh_cn/AmazonCloudWatch/latest/logs/S3ExportTasksConsole.html

---

**dzn** (Sun 25 Feb 2024 06:16) - *Upvotes: 3*
Amazon S3 Glacier is a secure, durable, and very low-cost cloud storage service that can be used for data archiving and long-term backup.

---


<br/>

## Question 82

*Date: May 15, 2023, 9:27 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company is developing a new application. The application uses AWS Lambda functions for its compute tier. The company must use a canary deployment for any changes to the Lambda functions. Automated rollback must occur if any failures are reported.

The company’s DevOps team needs to create the infrastructure as code (IaC) and the CI/CD pipeline for this solution.

Which combination of steps will meet these requirements? (Choose three.)

**Options:**
- A. Create an AWS CloudFormation template for the application. Define each Lambda function in the template by using the AWS::Lambda::Function resource type. In the template, include a version for the Lambda function by using the AWS::Lambda::Version resource type. Declare the CodeSha256 property. Configure an AWS::Lambda::Alias resource that references the latest version of the Lambda function.
- B. Create an AWS Serverless Application Model (AWS SAM) template for the application. Define each Lambda function in the template by using the AWS::Serverless::Function resource type. For each function, include configurations for the AutoPublishAlias property and the DeploymentPreference property. Configure the deployment configuration type to LambdaCanary10Percent10Minutes.
- C. Create an AWS CodeCommit repository. Create an AWS CodePipeline pipeline. Use the CodeCommit repository in a new source stage that starts the pipeline. Create an AWS CodeBuild project to deploy the AWS Serverless Application Model (AWS SAM) template. Upload the template and source code to the CodeCommit repository. In the CodeCommit repository, create a buildspec.yml file that includes the commands to build and deploy the SAM application.
- D. Create an AWS CodeCommit repository. Create an AWS CodePipeline pipeline. Use the CodeCommit repository in a new source stage that starts the pipeline. Create an AWS CodeDeploy deployment group that is configured for canary deployments with a DeploymentPreference type of Canary10Percent10Minutes. Upload the AWS CloudFormation template and source code to the CodeCommit repository. In the CodeCommit repository, create an appspec.yml file that includes the commands to deploy the CloudFormation template.
- E. Create an Amazon CloudWatch composite alarm for all the Lambda functions. Configure an evaluation period and dimensions for Lambda. Configure the alarm to enter the ALARM state if any errors are detected or if there is insufficient data.
- F. Create an Amazon CloudWatch alarm for each Lambda function. Configure the alarms to enter the ALARM state if any errors are detected. Configure an evaluation period, dimensions for each Lambda function and version, and the namespace as AWS/Lambda on the Errors metric.

> **Suggested Answer:** BCF
> **Community Vote:** BCF (91%), 9%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**2pk** (Wed 15 Nov 2023 10:27) - *Upvotes: 5*
BCF correct

---

**thanhnv142** (Sat 03 Aug 2024 03:45) - *Upvotes: 3*
BCF is my choice

---

**thanhnv142** (Sat 03 Aug 2024 03:44) - *Upvotes: 2*
BCF are correct:
A is not correct: <needs to create the infrastructure as code (IaC)> means we prefer AWS SAM over ACF. ACF is used to deploy AWS instances, not for IaC
D is wrong: no mention of AWS SAM
E is wrong: <Amazon CloudWatch composite alarm for all the Lambda functions>, but we need alarm for each lambda func, not one alarm for all of them

---

**sarlos** (Sun 30 Jun 2024 04:33) - *Upvotes: 2*
it should be BDF because code deploy can be configured for canary
https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html

---

**HugoFM** (Wed 29 May 2024 05:05) - *Upvotes: 3*
BCF, E is not correct you need to monitor each lambda to do a rollback of a particular deploy

---

**AzureDP900** (Sun 26 May 2024 19:34) - *Upvotes: 1*
BCF is right

---

**rlf** (Tue 30 Apr 2024 14:11) - *Upvotes: 1*
Answer is BCF.
https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html

---

**sivre** (Sun 21 Apr 2024 18:51) - *Upvotes: 4*
Can someone please explain why not A and D? seem the same of B C without using SAM

---

**RVivek** (Tue 12 Mar 2024 14:45) - *Upvotes: 2*
A is wrong beacuse of AWS::Lambda::Function
B - Can work
C: is correct
D: SAM or Lambda deployment in Codedeploy cannot be canary deployment. canaray deployment should be included in the Lambda code as mentioned in option B.
E: Composite Alram is not required. if any Lambda fails , it should generate alarm
F: works
Basically select B from AB which is for lambda coding, Select C from CD for deploying and F from EF for monitoring and alerting

---

**hotblooded** (Tue 30 Jul 2024 07:07) - *Upvotes: 1*
Why we cannot use code deploy for canary there are already few deployment percentage for codedeploy BDF is correct

---


<br/>

## Question 83

*Date: May 9, 2023, 3:56 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer is deploying a new version of a company’s application in an AWS CodeDeploy deployment group associated with its Amazon EC2 instances. After some time, the deployment fails. The engineer realizes that all the events associated with the specific deployment ID are in a Skipped status, and code was not deployed in the instances associated with the deployment group.

What are valid reasons for this failure? (Choose two.)

**Options:**
- A. The networking configuration does not allow the EC2 instances to reach the internet via a NAT gateway or internet gateway, and the CodeDeploy endpoint cannot be reached.
- B. The IAM user who triggered the application deployment does not have permission to interact with the CodeDeploy endpoint.
- C. The target EC2 instances were not properly registered with the CodeDeploy endpoint.
- D. An instance profile with proper permissions was not attached to the target EC2 instances.
- E. The appspec.yml file was not included in the application revision.

> **Suggested Answer:** AD
> **Community Vote:** AD (93%), 7%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**haazybanj** (Tue 25 Jul 2023 18:50) - *Upvotes: 12*
A.

Explanation: For CodeDeploy to work, the EC2 instances need to reach the CodeDeploy endpoint to download the deployment artifacts. If the networking configuration of the EC2 instances does not allow them to access the internet via a NAT gateway or internet gateway, they won't be able to reach the CodeDeploy endpoint, leading to deployment failure.
D

Explanation: When EC2 instances are part of a CodeDeploy deployment group, they need to have an associated IAM instance profile with the necessary permissions to interact with CodeDeploy and download the deployment artifacts. If the instance profile with proper permissions is not attached to the target EC2 instances, the deployment will fail as the instances won't have the required permissions to complete the deployment process.

---

**rhinozD** (Thu 15 Jun 2023 15:47) - *Upvotes: 7*
AD
https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html
Search with: Troubleshooting all lifecycle events skipped errors

---

**bnagaraja9099** (Fri 15 Dec 2023 04:39) - *Upvotes: 1*
C is correct.
the first reason for skipped errors on the link.
The CodeDeploy agent might not be installed or running on the instance. To determine if the CodeDeploy agent is running:

---

**sejar** (Mon 11 Mar 2024 13:54) - *Upvotes: 2*
No registration required, once agent is installed it should be sufficient. However permissions and network connectivity to S3 or code deploy would be must. Since that takes priority, A&D should be right.

---

**YucelFuat** (Fri 06 Sep 2024 12:12) - *Upvotes: 1*
My question is "skipped" situation doesn't sound like a network error. There is no 4xx error or fail status

---

**zijo** (Fri 17 May 2024 17:46) - *Upvotes: 1*
The user needs to create a service role and attach the AWSCodeDeployRole policy to it to grant the correct permissions for CodeDeploy to access EC2 instances. The role chosen should allow access to start and stop EC2 instances.
If the IAM role used by CodeDeploy doesn't have the necessary permissions to access the deployment artifacts or interact with the EC2 instances, the deployment may be skipped.
So it is not the IAM permissions of the user invoking the CodeDeploy.

---

**thanhnv142** (Sat 03 Feb 2024 05:05) - *Upvotes: 2*
A and D are correct: the deployment process might be skipped because of codedeploy agent
A: no connection means skipped deployment
D: insufficient permission means skipped deployment

---

**khchan123** (Wed 17 Jan 2024 16:52) - *Upvotes: 1*
A and D. See https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events

---

**3a29cc4** (Tue 02 Jan 2024 03:57) - *Upvotes: 1*
Do you really have to have internet connectivity to use CodeDeploy? Why not use VPC endpoint in such cases? I go for CD.

---

**yorkicurke** (Sun 10 Dec 2023 11:29) - *Upvotes: 2*
Some of the other options could cause a deployment to fail, but not specifically result in a "Skipped" status:

A Networking issues may prevent the deployment from reaching instances, but this would likely cause the deployment to fail, not be skipped.
B Lack of permissions for the IAM user would cause the deployment job itself to fail authorization.
E Missing appspec.yml would cause validation errors prior to the deployment attempt.

Anyone has diffrent views?

---


<br/>

## Question 84

*Date: May 9, 2023, 3:58 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has a guideline that every Amazon EC2 instance must be launched from an AMI that the company’s security team produces. Every month, the security team sends an email message with the latest approved AMIs to all the development teams.

The development teams use AWS CloudFormation to deploy their applications. When developers launch a new service, they have to search their email for the latest AMIs that the security department sent. A DevOps engineer wants to automate the process that the security team uses to provide the AMI IDs to the development teams.

What is the MOST scalable solution that meets these requirements?

**Options:**
- A. Direct the security team to use CloudFormation to create new versions of the AMIs and to list the AMI ARNs in an encrypted Amazon S3 object as part of the stack’s Outputs section. Instruct the developers to use a cross-stack reference to load the encrypted S3 object and obtain the most recent AMI ARNs.
- B. Direct the security team to use a CloudFormation stack to create an AWS CodePipeline pipeline that builds new AMIs and places the latest AMI ARNs in an encrypted Amazon S3 object as part of the pipeline output. Instruct the developers to use a cross-stack reference within their own CloudFormation template to obtain the S3 object location and the most recent AMI ARNs.
- C. Direct the security team to use Amazon EC2 Image Builder to create new AMIs and to place the AMI ARNs as parameters in AWS Systems Manager Parameter Store. Instruct the developers to specify a parameter of type SSM in their CloudFormation stack to obtain the most recent AMI ARNs from Parameter Store.
- D. Direct the security team to use Amazon EC2 Image Builder to create new AMIs and to create an Amazon Simple Notification Service (Amazon SNS) topic so that every development team can receive notifications. When the development teams receive a notification, instruct them to write an AWS Lambda function that will update their CloudFormation stack with the most recent AMI ARNs.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Sat 03 Feb 2024 05:19) - *Upvotes: 6*
C is correct: <automate the process that the security team uses to provide the AMI IDs to the development teams> and <MOST scalable solution> means we need a pipeline (imange builder) to build AMI and to automate sharing
A and B: no mention of EC2 Imange builder, which is better than codepipeline in building Ec2 image
D: They have to do this manually

---

**ad3fdb1** (Sat 16 Nov 2024 00:46) - *Upvotes: 1*
A question to answer of option C - is it able to update the System Manager Parameter Store automatically? Option A seems able to do it automatically, right?

---

**yuliaqwerty** (Tue 09 Jan 2024 15:53) - *Upvotes: 2*
C is the best option

---

**rlf** (Wed 01 Nov 2023 03:27) - *Upvotes: 2*
Answer is C.
https://aws.amazon.com/ko/blogs/compute/tracking-the-latest-server-images-in-amazon-ec2-image-builder-pipelines/

---

**habros** (Sun 09 Jul 2023 04:53) - *Upvotes: 2*
Use SSM Parameter Store or Secret Manager as the lookup K/V store for all the related AMIs. ANother way is also for security team to constantly update and share the images cross-account and grant them KMS keys to the encrypted AMIs. (not in question)

---

**devnv** (Sun 14 May 2023 07:32) - *Upvotes: 2*
C is correct

---

**ParagSanyashiv** (Tue 09 May 2023 15:58) - *Upvotes: 4*
C make more sense

---


<br/>

## Question 85

*Date: May 14, 2023, 7:36 a.m.
Disclaimers:
- ExamTopics website is not rel*

An application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). A DevOps engineer is using AWS CodeDeploy to release a new version. The deployment fails during the AllowTraffic lifecycle event, but a cause for the failure is not indicated in the deployment logs.

What would cause this?

**Options:**
- A. The appspec.yml file contains an invalid script that runs in the AllowTraffic lifecycle hook.
- B. The user who initiated the deployment does not have the necessary permissions to interact with the ALB.
- C. The health checks specified for the ALB target group are misconfigured.
- D. The CodeDeploy agent was not installed in the EC2 instances that are part of the ALB target group.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**rhinozD** (Fri 15 Dec 2023 16:54) - *Upvotes: 17*
C is the answer
refer this: https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-deployments-allowtraffic-no-logs

---

**thanhnv142** (Sat 03 Aug 2024 04:23) - *Upvotes: 5*
C is correct: <deployment fails during the AllowTraffic lifecycle event> means there are problems with ALB.
A: no mention of the ALB
B: The user who init the deployment does not need necessary permission
D: If agent was not installed, it would fail from the start

---

**OrganizedChaos25** (Thu 16 Nov 2023 15:39) - *Upvotes: 2*
C is the answer

---

**devnv** (Tue 14 Nov 2023 08:36) - *Upvotes: 1*
C is the correct answer

---


<br/>

## Question 86

*Date: May 5, 2023, 10:20 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has 20 service teams. Each service team is responsible for its own microservice. Each service team uses a separate AWS account for its microservice and a VPC with the 192.168.0.0/22 CIDR block. The company manages the AWS accounts with AWS Organizations.

Each service team hosts its microservice on multiple Amazon EC2 instances behind an Application Load Balancer. The microservices communicate with each other across the public internet. The company’s security team has issued a new guideline that all communication between microservices must use HTTPS over private network connections and cannot traverse the public internet.

A DevOps engineer must implement a solution that fulfills these obligations and minimizes the number of changes for each service team.

Which solution will meet these requirements?

**Options:**
- A. Create a new AWS account in AWS Organizations. Create a VPC in this account, and use AWS Resource Access Manager to share the private subnets of this VPC with the organization. Instruct the service teams to launch a new Network Load Balancer (NLB) and EC2 instances that use the shared private subnets. Use the NLB DNS names for communication between microservices.
- B. Create a Network Load Balancer (NLB) in each of the microservice VPCs. Use AWS PrivateLink to create VPC endpoints in each AWS account for the NLBs. Create subscriptions to each VPC endpoint in each of the other AWS accounts. Use the VPC endpoint DNS names for communication between microservices.
- C. Create a Network Load Balancer (NLB) in each of the microservice VPCs. Create VPC peering connections between each of the microservice VPCs. Update the route tables for each VPC to use the peering links. Use the NLB DNS names for communication between microservices.
- D. Create a new AWS account in AWS Organizations. Create a transit gateway in this account, and use AWS Resource Access Manager to share the transit gateway with the organization. In each of the microservice VPCs, create a transit gateway attachment to the shared transit gateway. Update the route tables of each VPC to use the transit gateway. Create a Network Load Balancer (NLB) in each of the microservice VPCs. Use the NLB DNS names for communication between microservices.

> **Suggested Answer:** B
> **Community Vote:** B (81%), D (19%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Blueee** (Wed 05 Jul 2023 11:18) - *Upvotes: 9*
B is correct because all 20 services team in different separate AWS accounts are using the same CIDR block, which means they are overlapping CIDR.

D state that to update the route tables of each VPC to use the transit gateway but they are all having the same CIDR block so this cannot proceed, as shared by Arnaud92 link the pre-requisite of using the transit gateway is "No-overlapping CIDR block between VPCs."

---

**Saudis** (Wed 13 Nov 2024 14:00) - *Upvotes: 1*
PrivateLink = HTTPS connection

---

**zijo** (Tue 21 May 2024 19:43) - *Upvotes: 3*
B is the answer
When VPCs have overlapping CIDR blocks, AWS PrivateLink still ensures secure and private connectivity by using Interface Endpoints (ENIs) and Network Load Balancers (NLBs) to route traffic, bypassing the need for direct IP routing between the VPCs.

---

**thanhnv142** (Sat 03 Feb 2024 05:43) - *Upvotes: 2*
B is correct: <all communication between microservices must use HTTPS over private network connections and cannot traverse the public internet> means privatelink
A and C: no mention of privatelink
D: Using transite gateway. But this solution need IP to route traffic and cannot be used for overlapped VPC CIDR block (every team uses 192.168.0.0/22)

---

**zolthar_z** (Thu 23 Nov 2023 19:16) - *Upvotes: 4*
Answer is B, Transit gateway can't route overlapping networks, the solution for this is privatelink: https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-privatelink.html

---

**RVivek** (Tue 12 Sep 2023 14:40) - *Upvotes: 2*
Thanks to rhinozD. Please check the side by sode comparision at the bottom of this page https://tomgregory.com/cross-account-vpc-access-in-aws

---

**ixdb** (Tue 15 Aug 2023 13:15) - *Upvotes: 2*
B is right.,

---

**Just_Ninja** (Wed 12 Jul 2023 14:42) - *Upvotes: 3*
B. is the right Solution!
Due to AWS's Transit Gateway not supporting same CIDRs (https://aws.amazon.com › transit-gateway › faqs), the most viable solution is the deployment of a Network Load Balancer (NLB) in each VPC. However, it's crucial to note that NLB operates similar to a NAT Gateway, allowing only incoming requests. After an incoming request is accepted, the NLB can then provide a response.

---

**SVGoogle89** (Tue 11 Jul 2023 19:42) - *Upvotes: 1*
AWS Transit Gateway doesn’t support routing between Amazon VPCs with identical CIDRs. If you attach a new Amazon VPC that has a CIDR which is identical to an already attached Amazon VPC, AWS Transit Gateway will not propagate the new Amazon VPC route into the AWS Transit Gateway route table.

---

**habros** (Sun 09 Jul 2023 05:05) - *Upvotes: 1*
I’ll lean towards B. For D, transit gateway is really expensive and does get the job done. There is also a need for NAT gateway as by default all AWS API traffic passes through the public internet. Hence, PrivateLink endpoints are for.

---


<br/>

## Question 87

*Date: May 14, 2023, 7:44 a.m.
Disclaimers:
- ExamTopics website is not rel*

An Amazon EC2 instance is running in a VPC and needs to download an object from a restricted Amazon S3 bucket. When the DevOps engineer tries to download the object, an AccessDenied error is received.

What are the possible causes for this error? (Choose two.)

**Options:**
- A. The S3 bucket default encryption is enabled.
- B. There is an error in the S3 bucket policy.
- C. The object has been moved to S3 Glacier.
- D. There is an error in the IAM role configuration.
- E. S3 Versioning is enabled.

> **Suggested Answer:** BD
> **Community Vote:** BD (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**xxxdolorxxx** (Thu 19 Jun 2025 05:08) - *Upvotes: 1*
B/D Get my vote.

---

**yuliaqwerty** (Tue 09 Jul 2024 15:01) - *Upvotes: 3*
I think B and D

---

**Jamshif01** (Tue 25 Jun 2024 17:32) - *Upvotes: 2*
ACCESS DENIED - you got it

---

**RVivek** (Tue 12 Mar 2024 15:47) - *Upvotes: 3*
IMHO it is BD

---

**vherman** (Fri 02 Feb 2024 15:52) - *Upvotes: 4*
BD
Not an error though. Misconfiguration.

---

**FunkyFresco** (Mon 25 Dec 2023 14:24) - *Upvotes: 2*
B and D for sure.

---

**OrganizedChaos25** (Thu 16 Nov 2023 15:43) - *Upvotes: 1*
BD are the answers I got

---

**devnv** (Tue 14 Nov 2023 08:44) - *Upvotes: 1*
BD are correct

---


<br/>

## Question 88

*Date: May 14, 2023, 7:46 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company wants to use a grid system for a proprietary enterprise in-memory data store on top of AWS. This system can run in multiple server nodes in any Linux-based distribution. The system must be able to reconfigure the entire cluster every time a node is added or removed. When adding or removing nodes, an /etc/cluster/nodes.config file must be updated, listing the IP addresses of the current node members of that cluster.

The company wants to automate the task of adding new nodes to a cluster.

What can a DevOps engineer do to meet these requirements?

**Options:**
- A. Use AWS OpsWorks Stacks to layer the server nodes of that cluster. Create a Chef recipe that populates the content of the /etc/cluster/nodes.config file and restarts the service by using the current members of the layer. Assign that recipe to the Configure lifecycle event.
- B. Put the file nodes.config in version control. Create an AWS CodeDeploy deployment configuration and deployment group based on an Amazon EC2 tag value for the cluster nodes. When adding a new node to the cluster, update the file with all tagged instances, and make a commit in version control. Deploy the new file and restart the services.
- C. Create an Amazon S3 bucket and upload a version of the /etc/cluster/nodes.config file. Create a crontab script that will poll for that S3 file and download it frequently. Use a process manager, such as Monit or systemd, to restart the cluster services when it detects that the new file was modified. When adding a node to the cluster, edit the file’s most recent members. Upload the new file to the S3 bucket.
- D. Create a user data script that lists all members of the current security group of the cluster and automatically updates the /etc/cluster/nodes.config file whenever a new instance is added to the cluster.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**habros** (Sun 09 Jul 2023 05:10) - *Upvotes: 6*
I’ll use config management tool as well. In this case Opsworks (Chef/Puppet).

---

**Exto1124** (Wed 24 Jul 2024 17:59) - *Upvotes: 1*
But how the files content (get actual nodes list) is updated in that case?

---

**thanhnv142** (Sat 03 Feb 2024 09:23) - *Upvotes: 5*
A is correct: <wants to use a grid system> means opswork stacks
B, C and D: no mention of opswork stack

---

**youonebe** (Fri 20 Dec 2024 20:45) - *Upvotes: 4*
AWS OpsWorks services have reached end of life and have been disabled for both new and existing customers. Will this question surface in the exam?


https://aws.amazon.com/blogs/mt/migrate-your-aws-opsworks-stacks-to-aws-systems-manager/

---

**hayjaykay** (Tue 22 Oct 2024 02:35) - *Upvotes: 2*
D.
This approach ensures that your nodes.config file is kept up-to-date with minimal manual intervention. The script dynamically adjusts the cluster configuration by reflecting changes in the security group, making the process seamless. Efficient and automated—just the way it should be!

---

**Cloudxie** (Mon 02 Sep 2024 10:26) - *Upvotes: 1*
D is the best

---

**Dushank** (Fri 22 Sep 2023 20:13) - *Upvotes: 2*
1
The best solution to meet the company's requirements is to use AWS OpsWorks Stacks to layer the server nodes of the cluster. Create a Chef recipe that populates the content of the /etc/cluster/nodes.config file and restarts the service by using the current members of the layer. Assign that recipe to the Configure lifecycle event.

---

**rhinozD** (Thu 15 Jun 2023 16:17) - *Upvotes: 2*
A is correct.
This event occurs on all of the stack's instances when one of the following occurs:
An instance enters or leaves the online state.
You associate an Elastic IP address with an instance or disassociate one from an instance.
You attach an Elastic Load Balancing load balancer to a layer, or detach one from a layer.
https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html

---

**devnv** (Sun 14 May 2023 07:46) - *Upvotes: 2*
A is correct

---


<br/>

## Question 89

*Date: May 14, 2023, 7:49 a.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer is working on a data archival project that requires the migration of on-premises data to an Amazon S3 bucket. The DevOps engineer develops a script that incrementally archives on-premises data that is older than 1 month to Amazon S3. Data that is transferred to Amazon S3 is deleted from the on-premises location. The script uses the S3 PutObject operation.

During a code review, the DevOps engineer notices that the script does not verify whether the data was successfully copied to Amazon S3. The DevOps engineer must update the script to ensure that data is not corrupted during transmission. The script must use MD5 checksums to verify data integrity before the on-premises data is deleted.

Which solutions for the script will meet these requirements? (Choose two.)

**Options:**
- A. Check the returned response for the VersionId. Compare the returned VersionId against the MD5 checksum.
- B. Include the MD5 checksum within the Content-MD5 parameter. Check the operation call’s return status to find out if an error was returned.
- C. Include the checksum digest within the tagging parameter as a URL query parameter.
- D. Check the returned response for the ETag. Compare the returned ETag against the MD5 checksum.
- E. Include the checksum digest within the Metadata parameter as a name-value pair. After upload, use the S3 HeadObject operation to retrieve metadata from the object.

> **Suggested Answer:** BD
> **Community Vote:** BD (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**haazybanj** (Thu 25 Jan 2024 19:56) - *Upvotes: 11*
B. Explanation: When using the S3 PutObject operation, you can include the MD5 checksum of the object in the Content-MD5 parameter of the request. Amazon S3 will calculate the MD5 checksum of the object and compare it to the provided checksum. If the checksums do not match, Amazon S3 will return an error response, indicating that the data integrity check failed. This way, you can ensure that the data was successfully copied to Amazon S3 without corruption.

D. Explanation: When you use the S3 PutObject operation, it returns an ETag in the response, which is the MD5 checksum of the object that was stored in Amazon S3. After performing the upload, you can check the returned ETag against the MD5 checksum you have locally calculated. If they match, it means the data was transferred successfully without corruption. If they don't match, it indicates a data integrity issue, and you can take appropriate actions.

---

**GripZA** (Sun 20 Apr 2025 10:29) - *Upvotes: 1*
B: the content-MD5 header is used to provide a base64encoded 128-bit MD5 digest of the object data
when included in a PutObject request, S3 checks the provided MD5 against what it calculates during upload
If the checksums dont match, the request fails with an error (like http 4xx)
this will ensure data was not corrupted during upload

D: for single part uploads, the ETag returned by S3 is usually the MD5 checksum of the object.
by calculating the MD5 of the local file and comparing it to the returned ETag, the script can verify integrity

---

**dzn** (Mon 26 Aug 2024 04:56) - *Upvotes: 3*
If the object was created by a PutObject, PostObject, or Copy operation, or via the AWS Management Console, and the object is either plain text or encrypted with server-side encryption using the Amazon S3 managed key ( SSE-S3), the object's ETag is the MD5 digest of the object data.

---

**thanhnv142** (Sat 03 Aug 2024 08:30) - *Upvotes: 3*
B and D are correct: <verify whether the data was successfully copied to Amazon S3> means we need to check <operation call’s return status> code. <use MD5 checksums to verify data integrity> means we need to check ETag
A: no mention of ETag
C and E: no mention of ETag or return status code

---

**rhinozD** (Fri 15 Dec 2023 17:24) - *Upvotes: 4*
BD
refer this link: https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html

---

**devnv** (Tue 14 Nov 2023 08:49) - *Upvotes: 3*
BD are correct

---


<br/>

## Question 90

*Date: May 14, 2023, 7:53 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company deploys updates to its Amazon API Gateway API several times a week by using an AWS CodePipeline pipeline. As part of the update process, the company exports the JavaScript SDK for the API from the API Gateway console and uploads the SDK to an Amazon S3 bucket.

The company has configured an Amazon CloudFront distribution that uses the S3 bucket as an origin. Web clients then download the SDK by using the CloudFront distribution’s endpoint. A DevOps engineer needs to implement a solution to make the new SDK available automatically during new API deployments.

Which solution will meet these requirements?

**Options:**
- A. Create a CodePipeline action immediately after the deployment stage of the API. Configure the action to invoke an AWS Lambda function. Configure the Lambda function to download the SDK from API Gateway, upload the SDK to the S3 bucket, and create a CloudFront invalidation for the SDK path.
- B. Create a CodePipeline action immediately after the deployment stage of the API. Configure the action to use the CodePipeline integration with API Gateway to export the SDK to Amazon S3. Create another action that uses the CodePipeline integration with Amazon S3 to invalidate the cache for the SDK path.
- C. Create an Amazon EventBridge rule that reacts to UpdateStage events from aws.apigateway. Configure the rule to invoke an AWS Lambda function to download the SDK from API Gateway, upload the SDK to the S3 bucket, and call the CloudFront API to create an invalidation for the SDK path.
- D. Create an Amazon EventBridge rule that reacts to CreateDeployment events from aws.apigateway. Configure the rule to invoke an AWS Lambda function to download the SDK from API Gateway, upload the SDK to the S3 bucket, and call the S3 API to invalidate the cache for the SDK path.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Sat 03 Aug 2024 09:15) - *Upvotes: 6*
A is correct: <by using an AWS CodePipeline pipeline> means we need CodePipeline.
C and D: no mention of CodePipeline.
B: < Configure the action to use the CodePipeline integration with API Gateway to export the SDK to Amazon S3>: codepipeline need to invoke other tools to do its task. There is not integration with API gateway

---

**GripZA** (Sun 20 Apr 2025 10:53) - *Upvotes: 1*
A - AWS Lambda is a compute service that lets you run code without provisioning or managing servers. You can create Lambda functions and add them as actions in your pipelines. Don't need eventbridge

---

**zanhsieh** (Tue 30 Apr 2024 15:04) - *Upvotes: 4*
Vote A. Reasons:
C: No. "aws.apigateway needs API Gateway AWS integration to send events to EventBridge without using compute service, such as Lambda or Amazon EC2."
https://aws.amazon.com/blogs/compute/capturing-client-events-using-amazon-api-gateway-and-amazon-eventbridge/
B&D: No. S3 API doesn't contain invalidate cache call, whereas CloudFront does. Search "invalidat" in
https://docs.aws.amazon.com/cli/latest/reference/s3api/
https://docs.aws.amazon.com/cli/latest/reference/cloudfront/

---

**RVivek** (Tue 12 Mar 2024 17:00) - *Upvotes: 2*
B & D are wrong as it suggests to invalidate the cache fro S3
D uses event bridge rule to invoke lambada however we know Codepipeline is available and that can be used to perform the action

---

**FEEREWMWKA** (Thu 29 Feb 2024 19:22) - *Upvotes: 1*
I pick C due to it stating during the deployment rather than at the end.

---

**ProfXsamson** (Wed 03 Jan 2024 04:42) - *Upvotes: 4*
Lambda is king

---

**devnv** (Tue 14 Nov 2023 08:53) - *Upvotes: 3*
A is the right answer

---


<br/>

## Question 91

*Date: May 14, 2023, 10:22 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has developed an AWS Lambda function that handles orders received through an API. The company is using AWS CodeDeploy to deploy the Lambda function as the final stage of a CI/CD pipeline.

A DevOps engineer has noticed there are intermittent failures of the ordering API for a few seconds after deployment. After some investigation, the DevOps engineer believes the failures are due to database changes not having fully propagated before the Lambda function is invoked.

How should the DevOps engineer overcome this?

**Options:**
- A. Add a BeforeAllowTraffic hook to the AppSpec file that tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function.
- B. Add an AfterAllowTraffic hook to the AppSpec file that forces traffic to wait for any pending database changes before allowing the new version of the Lambda function to respond.
- C. Add a BeforeInstall hook to the AppSpec file that tests and waits for any necessary database changes before deploying the new version of the Lambda function.
- D. Add a ValidateService hook to the AppSpec file that inspects incoming traffic and rejects the payload if dependent services, such as the database, are not yet ready.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Sat 03 Feb 2024 10:25) - *Upvotes: 5*
A is correct: <using AWS CodeDeploy to deploy> and <a CI/CD pipeline> means lifecycle event hook
B: AfterAllowTraffic wont solve the problem, we need to hook before traffic is allowed, as in <not having fully propagated before the Lambda function is invoked>
C: beforeInstall is used to prepare for the installation process, so it is not relevant
D: there is no ValidateService hook

---

**Gomer** (Fri 14 Jun 2024 22:38) - *Upvotes: 5*
In my research, there are only TWO CodeDeploy AppSpec ifecycle event hooks for Lambda deployment:
BeforeAllowTraffic # Use to run tasks before traffic is shifted to the deployed Lambda function version.
AfterAllowTraffic # Use to run tasks after all traffic is shifted to the deployed Lambda function version.

A: (YES) Don't redirect traffic untill ready
B: (NO) Block traffic until ready
C: (NO) Event hook N/A for Lambda
D: (NO) Event hook N/A for Lambda

---

**seetpt** (Thu 02 May 2024 12:33) - *Upvotes: 1*
I think A

---

**jojom19980** (Mon 19 Feb 2024 09:49) - *Upvotes: 3*
D can be correct if there is a wait to database to be ready so I will go with A

---

**habros** (Sun 09 Jul 2023 05:51) - *Upvotes: 3*
"Hooks": [
{
"BeforeInstall": "BeforeInstallHookFunctionName"
},
{
"AfterInstall": "AfterInstallHookFunctionName"
},
{
"AfterAllowTestTraffic": "AfterAllowTestTrafficHookFunctionName"
},
{
"BeforeAllowTraffic": "BeforeAllowTrafficHookFunctionName"
},
{
"AfterAllowTraffic": "AfterAllowTrafficHookFunctionName"
}
]
}

---

**habros** (Sun 09 Jul 2023 05:52) - *Upvotes: 2*
Opting for A based on this

---

**HugoFM** (Tue 28 Nov 2023 10:56) - *Upvotes: 6*
Those hooks are not valid for a Lambda, see the doc.
Lambda only supports BeforeAllowTraffic and AfterAllowTraffic. Anyway the answer is A

---

**OrganizedChaos25** (Tue 16 May 2023 14:57) - *Upvotes: 2*
A is the right answer

---

**devnv** (Sun 14 May 2023 10:22) - *Upvotes: 2*
A is correct

---


<br/>

## Question 92

*Date: May 14, 2023, 10:28 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses a single AWS account to test applications on Amazon EC2 instances. The company has turned on AWS Config in the AWS account and has activated the restricted-ssh AWS Config managed rule.

The company needs an automated monitoring solution that will provide a customized notification in real time if any security group in the account is not compliant with the restricted-ssh rule. The customized notification must contain the name and ID of the noncompliant security group.

A DevOps engineer creates an Amazon Simple Notification Service (Amazon SNS) topic in the account and subscribes the appropriate personnel to the topic.

What should the DevOps engineer do next to meet these requirements?

**Options:**
- A. Create an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Configure an input transformer for the EventBridge rule. Configure the EventBridge rule to publish a notification to the SNS topic.
- B. Configure AWS Config to send all evaluation results for the restricted-ssh rule to the SNS topic. Configure a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers.
- C. Create an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Configure the EventBridge rule to invoke AWS Systems Manager Run Command on the SNS topic to customize a notification and to publish the notification to the SNS topic.
- D. Create an Amazon EventBridge rule that matches all AWS Config evaluation results of NON_COMPLIANT. Configure an input transformer for the restricted-ssh rule. Configure the EventBridge rule to publish a notification to the SNS topic.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**steli0** (Mon 25 Nov 2024 13:10) - *Upvotes: 1*
D is tricky since it's not clear if the input transformer mentioned in the answer is supposed to be applied to the config rule or the EventBridge rule.

---

**zijo** (Thu 06 Jun 2024 19:13) - *Upvotes: 3*
AWS Config can send notifications to an SNS topic directly but here you need a customized notification which is only possible with the input transformer in Amazon EventBridge. So I think A is the better choice.

---

**MalonJay** (Tue 07 May 2024 16:07) - *Upvotes: 2*
B
AWS Config can send notifications directly to SNS.

---

**Heyang** (Wed 28 Feb 2024 08:29) - *Upvotes: 1*
A，About strict-ssh https://docs.aws.amazon.com/zh_cn/config/latest/developerguide/restricted-ssh.html

---

**thanhnv142** (Sat 03 Feb 2024 11:17) - *Upvotes: 4*
A is correct: <needs an automated monitoring solution that will provide a customized notification> and <creates an Amazon Simple Notification Service (Amazon SNS) topic> means they have already have SNS. we need to trigger alarm with eventbridge and send noti to SNS
B: no mention of event bride
C: AWS Systems Manager Run Command on the SNS topic to customize a notification: this step is unnecessary
D: <matches all AWS Config evaluation results of NON_COMPLIAN>: we need to match NON_COMPLIANT for the restricted-ssh rule only

---

**beanxyz** (Thu 24 Aug 2023 12:20) - *Upvotes: 2*
Here is an example
https://repost.aws/knowledge-center/config-resource-non-compliant

---

**Aja1** (Sat 29 Jul 2023 10:42) - *Upvotes: 1*
Option C is the most appropriate solution for creating a customized SNS notification when the restricted-ssh AWS Config rule is evaluated as NON_COMPLIANT.

---

**Aja1** (Mon 14 Aug 2023 13:10) - *Upvotes: 3*
Sorry A

EventBridge input transformers are used to customize the data that is sent to a target of an EventBridge rule. They can be used to extract specific data from the event, to convert the data to a different format, or to filter the data.

---

**Jaguaroooo** (Sat 06 Jan 2024 22:01) - *Upvotes: 1*
why would you want to customize anything to SNS. I chose C, but A makes more sense. no need for sns customization

---

**haazybanj** (Thu 20 Jul 2023 17:11) - *Upvotes: 3*
A
The Amazon EventBridge rule should be set up to match AWS Config evaluation results specifically for the restricted-ssh rule.
An input transformer should be configured for the EventBridge rule to extract and format the required information (e.g., name and ID of the noncompliant security group) from the AWS Config evaluation result.
The EventBridge rule should be configured to publish a notification to the SNS topic once it detects a noncompliant result.

---


<br/>

## Question 93

*Date: May 10, 2023, 2:22 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company requires an RPO of 2 hours and an RTO of 10 minutes for its data and application at all times. An application uses a MySQL database and Amazon EC2 web servers. The development team needs a strategy for failover and disaster recovery.

Which combination of deployment strategies will meet these requirements? (Choose two.)

**Options:**
- A. Create an Amazon Aurora cluster in one Availability Zone across multiple Regions as the data store. Use Aurora’s automatic recovery capabilities in the event of a disaster.
- B. Create an Amazon Aurora global database in two Regions as the data store. In the event of a failure, promote the secondary Region as the primary for the application.
- C. Create an Amazon Aurora multi-master cluster across multiple Regions as the data store. Use a Network Load Balancer to balance the database traffic in different Regions.
- D. Set up the application in two Regions and use Amazon Route 53 failover-based routing that points to the Application Load Balancers in both Regions. Use health checks to determine the availability in a given Region. Use Auto Scaling groups in each Region to adjust capacity based on demand.
- E. Set up the application in two Regions and use a multi-Region Auto Scaling group behind Application Load Balancers to manage the capacity based on demand. In the event of a disaster, adjust the Auto Scaling group’s desired instance count to increase baseline capacity in the failover Region.

> **Suggested Answer:** BD
> **Community Vote:** BD (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**zijo** (Fri 06 Dec 2024 21:14) - *Upvotes: 1*
Amazon Aurora clusters are designed to be region-specific, meaning that an Aurora DB cluster is limited to a single AWS region. However, you can use Amazon Aurora Global Database to span multiple AWS regions. But Amazon Aurora clusters can span across multiple AZs.
An AWS Auto Scaling group cannot span multiple regions. Each Auto Scaling group is limited to a single AWS region. However, within that region, an Auto Scaling group can span multiple Availability Zones to ensure high availability and fault tolerance.

---

**thanhnv142** (Sat 03 Aug 2024 10:33) - *Upvotes: 4*
B and D are correct: <needs a strategy for failover and disaster recovery> means global table or db cluster and route53 fail-over policy
A and C: These options mention spanning an Amazon Aurora cluster across multiple region. This is not true. A cluster can span across multiple AZs, not regions. The only Aurora solution that can span multiple regions is global table, which includes multiple clusters.
E: No mention of Route 53 failover-based routing

---

**MaiHuong** (Sun 14 Apr 2024 15:36) - *Upvotes: 4*
between ABC, choose B. A is wrong because “Amazon Aurora cluster in one Availability Zone across multiple Regions” is nonsense. C is incorrect too because Aurora multi-master cluster can't be across multiple regions

between DE, choose D because using Route 53 failover-based routing makes sense. E is wrong Auto Scaling group can't be multi-region

---

**Snape** (Tue 16 Jan 2024 23:15) - *Upvotes: 4*
No brainer

---

**OrganizedChaos25** (Thu 16 Nov 2023 16:08) - *Upvotes: 4*
Got BD as my answers

---

**devnv** (Tue 14 Nov 2023 11:33) - *Upvotes: 2*
BD are correct

---

**ParagSanyashiv** (Fri 10 Nov 2023 15:22) - *Upvotes: 3*
BD is the correct answer

---


<br/>

## Question 94

*Date: May 10, 2023, 2:30 p.m.
Disclaimers:
- ExamTopics website is not rel*

A business has an application that consists of five independent AWS Lambda functions.

The DevOps engineer has built a CI/CD pipeline using AWS CodePipeline and AWS CodeBuild that builds, tests, packages, and deploys each Lambda function in sequence. The pipeline uses an Amazon EventBridge rule to ensure the pipeline starts as quickly as possible after a change is made to the application source code.

After working with the pipeline for a few months, the DevOps engineer has noticed the pipeline takes too long to complete.

What should the DevOps engineer implement to BEST improve the speed of the pipeline?

**Options:**
- A. Modify the CodeBuild projects within the pipeline to use a compute type with more available network throughput.
- B. Create a custom CodeBuild execution environment that includes a symmetric multiprocessing configuration to run the builds in parallel.
- C. Modify the CodePipeline configuration to run actions for each Lambda function in parallel by specifying the same runOrder.
- D. Modify each CodeBuild project to run within a VPC and use dedicated instances to increase throughput.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Dushank** (Fri 22 Mar 2024 21:49) - *Upvotes: 9*
Parallel Execution:
By modifying the CodePipeline configuration to run actions for each Lambda function in parallel with the same runOrder, you allow multiple Lambda functions to be built and deployed simultaneously, which significantly improves the overall speed of the pipeline.

RunOrder:
The runOrder parameter in CodePipeline allows you to specify the order in which actions run. If multiple actions have the same runOrder, they can run in parallel.

---

**davdan99** (Mon 08 Jul 2024 17:05) - *Upvotes: 1*
Thanks for runOrder

---

**ParagSanyashiv** (Fri 10 Nov 2023 15:30) - *Upvotes: 7*
Agree with C

---

**thanhnv142** (Sat 03 Aug 2024 16:00) - *Upvotes: 2*
C is correct: <the pipeline takes too long to complete> and <consists of five independent AWS Lambda functions> means we should run the lambda funcs in parallel by specifying the same runOrder
A and D: no mention of running in parallel
B: No mention of runOrder

---

**MarDog** (Wed 20 Dec 2023 23:51) - *Upvotes: 4*
Yeah, it's definitely C.

---

**OrganizedChaos25** (Thu 16 Nov 2023 16:08) - *Upvotes: 3*
Answer is C

---

**devnv** (Tue 14 Nov 2023 11:41) - *Upvotes: 3*
C is right answer

---


<br/>

## Question 95

*Date: May 14, 2023, 10:48 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses AWS CloudFormation stacks to deploy updates to its application. The stacks consist of different resources. The resources include AWS Auto Scaling groups, Amazon EC2 instances, Application Load Balancers (ALBs), and other resources that are necessary to launch and maintain independent stacks. Changes to application resources outside of CloudFormation stack updates are not allowed.

The company recently attempted to update the application stack by using the AWS CLI. The stack failed to update and produced the following error message: “ERROR: both the deployment and the CloudFormation stack rollback failed. The deployment failed because the following resource(s) failed to update: [AutoScalingGroup].”

The stack remains in a status of UPDATE_ROLLBACK_FAILED.

Which solution will resolve this issue?

**Options:**
- A. Update the subnet mappings that are configured for the ALBs. Run the aws cloudformation update-stack-set AWS CLI command.
- B. Update the IAM role by providing the necessary permissions to update the stack. Run the aws cloudformation continue-update-rollback AWS CLI command.
- C. Submit a request for a quota increase for the number of EC2 instances for the account. Run the aws cloudformation cancel-update-stack AWS CLI command.
- D. Delete the Auto Scaling group resource. Run the aws cloudformation rollback-stack AWS CLI command.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Blueee** (Fri 05 Jan 2024 13:11) - *Upvotes: 11*
https://repost.aws/knowledge-center/cloudformation-update-rollback-failed
If your stack is stuck in the UPDATE_ROLLBACK_FAILED state after a failed update, then the only actions that you can perform on the stack are the ContinueUpdateRollback or DeleteStack operations.

So only B has ContinueUpdateRollback

---

**zijo** (Fri 06 Dec 2024 22:29) - *Upvotes: 1*
To update an AWS CloudFormation stack, you need an IAM role with permissions that allow you to perform the necessary actions on the resources defined in your CloudFormation template, as well as on the CloudFormation service itself. B is the answer

---

**thanhnv142** (Sat 03 Aug 2024 16:08) - *Upvotes: 3*
B is correct: <UPDATE_ROLLBACK_FAILED> means we are left with only two options: continue-update-rollback or delete-stack. We should provide necessary permissions to update the stack as well
A, C and D: no mention of continue-update-rollback or adding necessary permissions

---

**Dushank** (Fri 22 Mar 2024 21:55) - *Upvotes: 2*
They should update the IAM role by providing the necessary permissions to update the stack and then run the aws cloudformation continue-update-rollback AWS CLI command

---

**Blueee** (Fri 05 Jan 2024 13:06) - *Upvotes: 3*
B is correct

---

**OrganizedChaos25** (Thu 16 Nov 2023 16:11) - *Upvotes: 3*
B is the answer I got

---

**devnv** (Tue 14 Nov 2023 11:48) - *Upvotes: 3*
B is correct

---


<br/>

## Question 96

*Date: May 14, 2023, 10:51 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company is deploying a new application that uses Amazon EC2 instances. The company needs a solution to query application logs and AWS account API activity.

Which solution will meet these requirements?

**Options:**
- A. Use the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon CloudWatch Logs. Configure AWS CloudTrail to deliver the API logs to Amazon S3. Use CloudWatch to query both sets of logs.
- B. Use the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon CloudWatch Logs. Configure AWS CloudTrail to deliver the API logs to CloudWatch Logs. Use CloudWatch Logs Insights to query both sets of logs.
- C. Use the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon Kinesis. Configure AWS CloudTrail to deliver the API logs to Kinesis. Use Kinesis to load the data into Amazon Redshift. Use Amazon Redshift to query both sets of logs.
- D. Use the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon S3. Use AWS CloudTrail to deliver the API logs to Amazon S3. Use Amazon Athena to query both sets of logs in Amazon S3.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**ogwu2000** (Tue 16 Jan 2024 10:27) - *Upvotes: 8*
B is correct
A - wrong because CloudWatch is not a query tool.
C - Wrong because CloudWatch agent cant send logs directly to Kinesis. Should be from CloudWatch log
D - Wrong because CloudWatch agent cant send logs directly to S3. Should be from CloudWatch log to firehorse to S3

---

**haazybanj** (Fri 26 Jan 2024 19:28) - *Upvotes: 5*
Explanation:
Option B provides a comprehensive solution for querying application logs and AWS account API activity. The Amazon CloudWatch agent is used to send logs from the EC2 instances to Amazon CloudWatch Logs, allowing easy access to application logs. AWS CloudTrail is configured to deliver the API logs to CloudWatch Logs, enabling monitoring and analysis of AWS account activity. Finally, CloudWatch Logs Insights is utilized to query and analyze both sets of logs efficiently.

---

**thanhnv142** (Sat 03 Aug 2024 16:15) - *Upvotes: 4*
B is correct: <query application logs and AWS account API activity> means we need cloudwatch log and cloud trail
C and D: cloudwatch agent cannot directly send logs to S3 or Kinesis.
A: Cloudwatch query works only on cloudwatch, not S3

---

**ProfXsamson** (Wed 03 Jan 2024 05:19) - *Upvotes: 4*
Since Cloudwatch Insights can perform query, no need to use s3/athena.

---

**OrganizedChaos25** (Thu 16 Nov 2023 16:13) - *Upvotes: 1*
B is correct

---

**devnv** (Tue 14 Nov 2023 11:51) - *Upvotes: 1*
B is the right answer

---


<br/>

## Question 97

*Date: May 14, 2023, 10:55 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company wants to ensure that their EC2 instances are secure. They want to be notified if any new vulnerabilities are discovered on their instances, and they also want an audit trail of all login activities on the instances.

Which solution will meet these requirements?

**Options:**
- A. Use AWS Systems Manager to detect vulnerabilities on the EC2 instances. Install the Amazon Kinesis Agent to capture system logs and deliver them to Amazon S3.
- B. Use AWS Systems Manager to detect vulnerabilities on the EC2 instances. Install the Systems Manager Agent to capture system logs and view login activity in the CloudTrail console.
- C. Configure Amazon CloudWatch to detect vulnerabilities on the EC2 instances. Install the AWS Config daemon to capture system logs and view them in the AWS Config console.
- D. Configure Amazon Inspector to detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and record them via Amazon CloudWatch Logs.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**ProfXsamson** (Mon 03 Jul 2023 04:20) - *Upvotes: 9*
Aamzon Inspector detect software vulnerabilities and unintended network exposure in near real time in AWS workloads such as Amazon EC2, AWS Lambda functions, and Amazon ECR.

---

**thanhnv142** (Sat 03 Feb 2024 17:23) - *Upvotes: 5*
D is correct: <new vulnerabilities are discovered> means AWS inspector
A and B: AWS SSM does not support vulnerabilities scanning
C: Amazon CloudWatch does not support vulnerabilities scanning

---

**92a2133** (Fri 23 May 2025 18:02) - *Upvotes: 1*
This is probably one of the easiest questions yet on this entire site lol, never seen a more obvious answer

---

**YucelFuat** (Fri 06 Sep 2024 12:42) - *Upvotes: 3*
Exam tip : if you see "vulnerabilities" in the question -> choose the answer covers "Amazon Inspector"

---

**EVAAWS** (Fri 09 Aug 2024 08:06) - *Upvotes: 1*
D is the right answer

---

**OrganizedChaos25** (Tue 16 May 2023 15:22) - *Upvotes: 2*
D is correct

---

**devnv** (Sun 14 May 2023 10:55) - *Upvotes: 2*
D is the right answer

---


<br/>

## Question 98

*Date: May 14, 2023, 10:58 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company is running an application on Amazon EC2 instances in an Auto Scaling group. Recently, an issue occurred that prevented EC2 instances from launching successfully, and it took several hours for the support team to discover the issue. The support team wants to be notified by email whenever an EC2 instance does not start successfully.

Which action will accomplish this?

**Options:**
- A. Add a health check to the Auto Scaling group to invoke an AWS Lambda function whenever an instance status is impaired.
- B. Configure the Auto Scaling group to send a notification to an Amazon SNS topic whenever a failed instance launch occurs.
- C. Create an Amazon CloudWatch alarm that invokes an AWS Lambda function when a failed AttachInstances Auto Scaling API call is made.
- D. Create a status check alarm on Amazon EC2 to send a notification to an Amazon SNS topic whenever a status check fail occurs.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**MarDog** (Thu 21 Dec 2023 00:07) - *Upvotes: 7*
Likely B:
https://aws.amazon.com/blogs/aws/auto-scaling-notifications-recurrence-and-more-control/
"EC2_INSTANCE_LAUNCH_ ERROR"

---

**thanhnv142** (Sat 03 Aug 2024 16:26) - *Upvotes: 4*
B is correct: <wants to be notified by email> means SNS. <EC2 instances in an Auto Scaling group> means this should be triggered by auto scaling group
A, C and D: no mention of both SNS and auto scaling group

---

**rhinozD** (Sat 16 Dec 2023 10:00) - *Upvotes: 3*
B is correct
https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-sns-notifications.html

---

**OrganizedChaos25** (Thu 16 Nov 2023 16:24) - *Upvotes: 1*
I got B as my answer

---

**2pk** (Wed 15 Nov 2023 00:31) - *Upvotes: 3*
i think B is correct , but Option A is incorrect because, this would only be triggered if an instance was already running and experiencing issues. It would not provide notification when an instance fails to launch.

---

**devnv** (Tue 14 Nov 2023 11:58) - *Upvotes: 1*
B is correct

---


<br/>

## Question 99

*Date: May 9, 2023, 2:25 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is using AWS Organizations to centrally manage its AWS accounts. The company has turned on AWS Config in each member account by using AWS CloudFormation StackSets. The company has configured trusted access in Organizations for AWS Config and has configured a member account as a delegated administrator account for AWS Config.

A DevOps engineer needs to implement a new security policy. The policy must require all current and future AWS member accounts to use a common baseline of AWS Config rules that contain remediation actions that are managed from a central account. Non-administrator users who can access member accounts must not be able to modify this common baseline of AWS Config rules that are deployed into each member account.

Which solution will meet these requirements?

**Options:**
- A. Create a CloudFormation template that contains the AWS Config rules and remediation actions. Deploy the template from the Organizations management account by using CloudFormation StackSets.
- B. Create an AWS Config conformance pack that contains the AWS Config rules and remediation actions. Deploy the pack from the Organizations management account by using CloudFormation StackSets.
- C. Create a CloudFormation template that contains the AWS Config rules and remediation actions. Deploy the template from the delegated administrator account by using AWS Config.
- D. Create an AWS Config conformance pack that contains the AWS Config rules and remediation actions. Deploy the pack from the delegated administrator account by using AWS Config.

> **Suggested Answer:** D
> **Community Vote:** D (88%), 13%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Certified101** (Thu 11 Jan 2024 10:38) - *Upvotes: 10*
Option D. Create an AWS Config conformance pack that contains the AWS Config rules and remediation actions. Deploy the pack from the delegated administrator account by using AWS Config.

Conformance packs are a collection of AWS Config rules and remediation actions that can be easily deployed as a single entity in an account and a region, and across an organization in AWS Organizations. These packs are created and managed from a central account, and help to establish a secure and compliant posture for your accounts. Non-administrator users can view the AWS Config rules within a conformance pack but they cannot modify them. AWS Config conformance packs are therefore a good fit for achieving the desired control and security policy.

The other options, while potentially viable for deploying Config rules, do not inherently protect the baseline AWS Config rules from being modified by non-administrator users in the member accounts.

---

**Jeanphi72** (Thu 09 Nov 2023 15:25) - *Upvotes: 5*
https://docs.aws.amazon.com/config/latest/developerguide/conformance-packs.html

---

**Srikantha** (Sat 29 Mar 2025 23:58) - *Upvotes: 1*
ption B is the most appropriate solution because it leverages AWS Config conformance packs, which are purpose-built for applying and managing AWS Config rules across multiple accounts. Deploying the conformance pack from the central management account using CloudFormation StackSets ensures that the security policy is applied uniformly across all accounts in the organization, and it restricts non-administrator users from modifying the baseline.

---

**MalonJay** (Thu 07 Nov 2024 17:39) - *Upvotes: 1*
The question says
'The policy must require all current and future AWS member accounts to use a common baseline of AWS Config rules'
Does D account for that?

---

**thanhnv142** (Sat 03 Aug 2024 16:38) - *Upvotes: 5*
D is correct: <a common baseline of AWS Config rule> means conformance pack. <a member account as a delegated administrator account for AWS Config> means delegated admin
A and C: no mentionf of conformance pack
B: should deploy this using AWS config and in the delegated account, not the management account

---

**lunt** (Fri 05 Jan 2024 19:19) - *Upvotes: 5*
Not sure why some people are saying B.
A= CFN cannot protect the config.
B= Yes technically, where is the actual CONFIG management plane? Its in the delegated admin account, which is not the management account = delegated admin config account will have no idea of management account config.
C= CFN cannot protect config.
D= Yes. Delegated CONFIG account can config on orgz level & protect the rules. Only logical option.

---

**allen_devops** (Tue 19 Dec 2023 23:09) - *Upvotes: 1*
D is correct. Deploying via Cloudformation StackSet cannot make sure that the aws config itself is not modified by the member accounts. Deploy aws organizational rule will achieve both permission restriction and auto deployment

https://docs.aws.amazon.com/config/latest/developerguide/config-rule-multi-account-deployment.html

---

**rhinozD** (Sat 16 Dec 2023 10:12) - *Upvotes: 3*
D is correct
https://aws.amazon.com/blogs/mt/deploying-conformance-packs-across-an-organization-with-automatic-remediation/

---

**Nickexams** (Wed 29 Nov 2023 16:02) - *Upvotes: 1*
option B is the most appropriate solution for centrally managing and enforcing the common baseline of AWS Config rules across all member accounts while ensuring that non-administrator users cannot modify the rules.

---

**stream3652** (Tue 28 Nov 2023 10:22) - *Upvotes: 2*
Can't you use D?

---


<br/>

## Question 100

*Date: May 9, 2023, 2:19 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer manages a large commercial website that runs on Amazon EC2. The website uses Amazon Kinesis Data Streams to collect and process web logs. The DevOps engineer manages the Kinesis consumer application, which also runs on Amazon EC2.

Sudden increases of data cause the Kinesis consumer application to fall behind, and the Kinesis data streams drop records before the records can be processed. The DevOps engineer must implement a solution to improve stream handling.

Which solution meets these requirements with the MOST operational efficiency?

**Options:**
- A. Modify the Kinesis consumer application to store the logs durably in Amazon S3. Use Amazon EMR to process the data directly on Amazon S3 to derive customer insights. Store the results in Amazon S3.
- B. Horizontally scale the Kinesis consumer application by adding more EC2 instances based on the Amazon CloudWatch GetRecords.IteratorAgeMilliseconds metric. Increase the retention period of the Kinesis data streams.
- C. Convert the Kinesis consumer application to run as an AWS Lambda function. Configure the Kinesis data streams as the event source for the Lambda function to process the data streams.
- D. Increase the number of shards in the Kinesis data streams to increase the overall throughput so that the consumer application processes the data faster.

> **Suggested Answer:** B
> **Community Vote:** B (67%), C (30%), 3%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**emupsx1** (Mon 17 Jul 2023 11:57) - *Upvotes: 42*
The answer is B because:
A few hours ago, I just finished the DOP-C02 exam.
My score is 1000 points.
This question has come up, I choose B.

---

**yorkicurke** (Sat 25 Nov 2023 08:56) - *Upvotes: 1*
First of all Congratulations.
Now how do you know that this question was not from those 10 questions that do not count towards your score. and your answer to this question was Wrong but not counted towards your score. Just Saying!!
Peace :)

---

**Jaguaroooo** (Tue 09 Jan 2024 11:48) - *Upvotes: 5*
B is the Answer, let him show off, it is ok.

---

**youonebe** (Fri 20 Dec 2024 23:39) - *Upvotes: 3*
Remember, only 50 of the questions are graded.
First, there is no way what you said is true.
Second, your answer might not be graded.
Correct answer is a firm C.

---

**yorkicurke** (Sat 25 Nov 2023 09:03) - *Upvotes: 7*
why not C?
because we just replace ONE ec2 with ONE lambda here. And no mention of aws lambda reserved concurrency or provisioned concurrency.
In the question were are asked for 'MOST operational efficiency'. that's my two cents.
Ciao

---

**nickp84** (Thu 15 May 2025 13:18) - *Upvotes: 1*
B. Scaling EC2 instances based on metrics is valid but requires manual infrastructure management, which is less efficient than serverless.
C is correct:
AWS Lambda integrates natively with Kinesis Data Streams and automatically scales with the number of shards.
It removes the need to manage EC2 instances, reducing operational overhead.
Lambda handles parallel processing of stream records and automatically polls the stream.
You can configure batch size and parallelization factor to optimize throughput.
Built-in checkpointing ensures records are not lost or reprocessed unnecessarily.
This approach provides high scalability, low maintenance, and cost efficiency, which aligns with the requirement for operational efficiency.

---

**DKM** (Mon 17 Mar 2025 12:58) - *Upvotes: 1*
The GetRecords.IteratorAgeMilliseconds metric in Amazon Kinesis Data Streams measures the age of the last record in all GetRecords calls made against a Kinesis stream. This age is calculated as the difference between the current time and when the last record of the GetRecords call was written to the stream.

This metric is crucial for monitoring the latency of your Kinesis consumer applications. If the IteratorAgeMilliseconds value is increasing, it could indicate issues such as slow record processing, read throttles, or connection timeouts. Monitoring this metric helps ensure that your consumer applications are processing records in a timely manner and not falling behind.

---

**ce0df07** (Wed 05 Feb 2025 23:37) - *Upvotes: 1*
Option B because:
1. It uses GetRecords.IteratorAgeMilliseconds metric
- This metric indicates how far behind the consumer is processing
- Perfect indicator for when scaling is needed
- Automatically detects processing lag
2. Horizontal scaling of EC2 consumers
- Adds processing capacity when needed
- Can scale down when demand decreases
- Maintains existing application architecture
3. Increasing retention period
- Provides buffer against temporary processing delays
- Prevents data loss during scaling events
- Default is 24 hours, can be increased up to 365 days

---

**2d943d1** (Tue 21 Jan 2025 13:20) - *Upvotes: 1*
I think most people are missing the part where the question focuses on MOST operational efficiency. Lambda fits this purpose, as ww are not managing instances and Lambda can scale to meet throughput demands.

---

**ZinggieG87** (Tue 24 Dec 2024 04:08) - *Upvotes: 1*
A, sounds a lot effort towards a different architecture solution.
B, I don't think the insufficient kinesis throughput has can be resolved.
C, adding lambda is a big change, no mention about the concurrency limit isn't clear mentioned in the question.

---


<br/>

## Question 101

*Date: May 13, 2023, 9:36 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company recently created a new AWS Control Tower landing zone in a new organization in AWS Organizations. The landing zone must be able to demonstrate compliance with the Center for Internet Security (CIS) Benchmarks for AWS Foundations.

The company’s security team wants to use AWS Security Hub to view compliance across all accounts. Only the security team can be allowed to view aggregated Security Hub findings. In addition, specific users must be able to view findings from their own accounts within the organization. All accounts must be enrolled in Security Hub after the accounts are created.

Which combination of steps will meet these requirements in the MOST automated way? (Choose three.)

**Options:**
- A. Turn on trusted access for Security Hub in the organization’s management account. Create a new security account by using AWS Control Tower. Configure the new security account as the delegated administrator account for Security Hub. In the new security account, provide Security Hub with the CIS Benchmarks for AWS Foundations standards.
- B. Turn on trusted access for Security Hub in the organization’s management account. From the management account, provide Security Hub with the CIS Benchmarks for AWS Foundations standards.
- C. Create an AWS IAM Identity Center (AWS Single Sign-On) permission set that includes the required permissions. Use the CreateAccountAssignment API operation to associate the security team users with the permission set and with the delegated security account.
- D. Create an SCP that explicitly denies any user who is not on the security team from accessing Security Hub.
- E. In Security Hub, turn on automatic enablement.
- F. In the organization’s management account, create an Amazon EventBridge rule that reacts to the CreateManagedAccount event. Create an AWS Lambda function that uses the Security Hub CreateMembers API operation to add new accounts to Security Hub. Configure the EventBridge rule to invoke the Lambda function.

> **Suggested Answer:** ACE
> **Community Vote:** ACE (71%), ADE (21%), 7%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**emupsx1** (Mon 17 Jul 2023 11:58) - *Upvotes: 17*
The answer is ACE because:
A few hours ago, I just finished the DOP-C02 exam.
My score is 1000 points.
This question has come up, I choose ACE.

---

**BaburTurk** (Sat 09 Sep 2023 08:55) - *Upvotes: 7*
bot account, Pics or it didn't happen,

---

**Gomer** (Mon 17 Jun 2024 21:51) - *Upvotes: 3*
Either a bot or a bot for brains. Same useless comments made on multiple questions.

---

**danish1234** (Mon 18 Aug 2025 12:15) - *Upvotes: 1*
you can use SCP to prevent access to other teams except security team by using condition in the policy .

---

**nickp84** (Thu 15 May 2025 13:24) - *Upvotes: 1*
D. An SCP that denies access to Security Hub is too broad and could interfere with legitimate access by account owners who need to see their own findings.

---

**eugene2owl** (Mon 02 Dec 2024 07:32) - *Upvotes: 1*
I prefer "D" over "C", because no-one asks to enable SSO (which is very complex to organise and maintain)

---

**auxwww** (Thu 25 Jul 2024 02:00) - *Upvotes: 4*
A - Only security team needs access to findings org wide - hence delegated account
C - Allow security team members access to delegated account for Security hub using Identity center of control tower
E - Each new account needs security hub for it's own users to access and also for aggregation across org

---

**zijo** (Mon 10 Jun 2024 18:05) - *Upvotes: 3*
Automatic enablement in AWS Security Hub refers to the feature that allows AWS Security Hub to be automatically enabled for new and existing AWS accounts that are part of an organization in AWS Organizations. This feature simplifies the process of onboarding multiple accounts into Security Hub, ensuring consistent security posture and compliance across the organization.

---

**seetpt** (Thu 02 May 2024 12:41) - *Upvotes: 3*
ACE is correct

---

**didek1986** (Tue 16 Apr 2024 10:31) - *Upvotes: 2*
ACF
E - ensures that all new accounts are automatically enrolled in Security Hub (same as F) but it does not address the requirement for specific users to view findings from their own accounts

---


<br/>

## Question 102

*Date: May 9, 2023, 1:21 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company runs applications in AWS accounts that are in an organization in AWS Organizations. The applications use Amazon EC2 instances and Amazon S3.

The company wants to detect potentially compromised EC2 instances, suspicious network activity, and unusual API activity in its existing AWS accounts and in any AWS accounts that the company creates in the future. When the company detects one of these events, the company wants to use an existing Amazon Simple Notification Service (Amazon SNS) topic to send a notification to its operational support team for investigation and remediation.

Which solution will meet these requirements in accordance with AWS best practices?

**Options:**
- A. In the organization’s management account, configure an AWS account as the Amazon GuardDuty administrator account. In the GuardDuty administrator account, add the company’s existing AWS accounts to GuardDuty as members. In the GuardDuty administrator account, create an Amazon EventBridge rule with an event pattern to match GuardDuty events and to forward matching events to the SNS topic.
- B. In the organization’s management account, configure Amazon GuardDuty to add newly created AWS accounts by invitation and to send invitations to the existing AWS accounts. Create an AWS CloudFormation stack set that accepts the GuardDuty invitation and creates an Amazon EventBridge rule. Configure the rule with an event pattern to match GuardDuty events and to forward matching events to the SNS topic. Configure the CloudFormation stack set to deploy into all AWS accounts in the organization.
- C. In the organization’s management account, create an AWS CloudTrail organization trail. Activate the organization trail in all AWS accounts in the organization. Create an SCP that enables VPC Flow Logs in each account in the organization. Configure AWS Security Hub for the organization. Create an Amazon EventBridge rule with an event pattern to match Security Hub events and to forward matching events to the SNS topic.
- D. In the organization’s management account, configure an AWS account as the AWS CloudTrail administrator account. In the CloudTrail administrator account, create a CloudTrail organization trail. Add the company’s existing AWS accounts to the organization trail. Create an SCP that enables VPC Flow Logs in each account in the organization. Configure AWS Security Hub for the organization. Create an Amazon EventBridge rule with an event pattern to match Security Hub events and to forward matching events to the SNS topic.

> **Suggested Answer:** A
> **Community Vote:** A (87%), 13%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Just_Ninja** (Wed 12 Jul 2023 19:24) - *Upvotes: 13*
Dear Admin, Please Fix the Wrong response here!
It´s A:
This solution meets all the requirements:

Detect potentially compromised EC2 instances, suspicious network activity, and unusual API activity: Amazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior. It analyzes events from AWS CloudTrail, Amazon VPC Flow Logs, and DNS logs to detect such activities.

Send a notification to the operational support team: Creating an Amazon EventBridge rule that matches GuardDuty findings and then forwarding these to an SNS topic allows for the generation of notifications whenever suspicious activity is detected.

Cover future AWS accounts: By designating a GuardDuty administrator account in AWS Organizations, you can manage GuardDuty across all of your existing and future AWS accounts. This ensures that any new account created under the organization is automatically covered by GuardDuty.

---

**Mrflip** (Thu 09 Jan 2025 21:49) - *Upvotes: 1*
B is the right answer

---

**jamesf** (Mon 05 Aug 2024 16:22) - *Upvotes: 2*
keywords: compromised EC2 instances, suspicious network activity, and unusual API activity
= GuardDuty

---

**zijo** (Mon 10 Jun 2024 18:28) - *Upvotes: 2*
When you use GuardDuty with an AWS organization, the management account of that organization can designate any account within the organization as the delegated GuardDuty administrator account. For this administrator account, GuardDuty gets enabled automatically only in the designated AWS Region. This account also has the permission to enable and manage GuardDuty for all of the accounts in the organization within that Region. The administrator account can view the members of and add members to this AWS organization.

AWS GuardDuty can detect unusual API activity within existing AWS accounts in an AWS Organization. It monitors AWS CloudTrail event logs, which include records of all API calls made within your AWS environment. GuardDuty analyzes these logs to identify unusual or suspicious API activity that might indicate a potential security threat.

---

**zijo** (Mon 10 Jun 2024 18:23) - *Upvotes: 1*
A looks like a better choice.
When you use GuardDuty with an AWS organization, the management account of that organization can designate any account within the organization as the delegated GuardDuty administrator account. For this administrator account, GuardDuty gets enabled automatically only in the designated AWS Region. This account also has the permission to enable and manage GuardDuty for all of the accounts in the organization within that Region. The administrator account can view the members of and add members to this AWS organization.

---

**dkp** (Sun 14 Apr 2024 07:03) - *Upvotes: 2*
answer A

---

**Mordans** (Sat 13 Apr 2024 10:44) - *Upvotes: 1*
If GuardDuty is indeed set up at the organization level (which is supported and encouraged by AWS for simplicity and coverage), then Option A becomes a very strong choice. It provides centralized management and automatic, seamless inclusion of all organization accounts in security monitoring without requiring manual intervention for each new account.

---

**stoy123** (Tue 26 Mar 2024 09:46) - *Upvotes: 1*
Definitely B

---

**thanhnv142** (Sun 04 Feb 2024 04:59) - *Upvotes: 3*
A is correct: <detect potentially compromised EC2 instances, suspicious network activity, and unusual API activity> means AWS GuardDuty
B: dont have to invite other accounts because all accounts are in an org in AWS org.
C and D: no mention of GuardDuty

---

**a54b16f** (Fri 12 Jan 2024 14:46) - *Upvotes: 3*
invitation is used to handle users OUTSIDE the organization.

---


<br/>

## Question 103

*Date: May 14, 2023, 11:52 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company’s DevOps engineer is working in a multi-account environment. The company uses AWS Transit Gateway to route all outbound traffic through a network operations account. In the network operations account, all account traffic passes through a firewall appliance for inspection before the traffic goes to an internet gateway.

The firewall appliance sends logs to Amazon CloudWatch Logs and includes event severities of CRITICAL, HIGH, MEDIUM, LOW, and INFO. The security team wants to receive an alert if any CRITICAL events occur.

What should the DevOps engineer do to meet these requirements?

**Options:**
- A. Create an Amazon CloudWatch Synthetics canary to monitor the firewall state. If the firewall reaches a CRITICAL state or logs a CRITICAL event, use a CloudWatch alarm to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the security team’s email address to the topic.
- B. Create an Amazon CloudWatch metric filter by using a search for CRITICAL events. Publish a custom metric for the finding. Use a CloudWatch alarm based on the custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the security team’s email address to the topic.
- C. Enable Amazon GuardDuty in the network operations account. Configure GuardDuty to monitor flow logs. Create an Amazon EventBridge event rule that is invoked by GuardDuty events that are CRITICAL. Define an Amazon Simple Notification Service (Amazon SNS) topic as a target. Subscribe the security team’s email address to the topic.
- D. Use AWS Firewall Manager to apply consistent policies across all accounts. Create an Amazon EventBridge event rule that is invoked by Firewall Manager events that are CRITICAL. Define an Amazon Simple Notification Service (Amazon SNS) topic as a target. Subscribe the security team’s email address to the topic.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**2pk** (Tue 14 Nov 2023 17:50) - *Upvotes: 7*
The logs from the firewall appliance are already being sent to Amazon CloudWatch Logs. So , The best approach to meet the given requirements is to create an Amazon CloudWatch metric filter by using a search for CRITICAL events

---

**habros** (Tue 09 Jan 2024 15:27) - *Upvotes: 5*
B. As the appliance pipes to CW Logs for consolidation. Define an alarm listening to the metric and should be okay.

D is ONLY CORRECT IF YOU ARE USING AWS FIREWALL MANAGER.

---

**seetpt** (Sat 02 Nov 2024 13:42) - *Upvotes: 1*
I think B

---

**dkp** (Mon 14 Oct 2024 07:05) - *Upvotes: 2*
answer B

---

**thanhnv142** (Sun 04 Aug 2024 04:02) - *Upvotes: 4*
B is correct: <firewall appliance sends logs to Amazon CloudWatch Logs> means we already have the log in CW logs, only need to create alarm on these log files and send to sends
A: No need to monitor the state of the firewall
C and D: no mention of CloudWatch Logs

---

**OrganizedChaos25** (Thu 16 Nov 2023 16:36) - *Upvotes: 1*
B is the correct answer

---

**devnv** (Tue 14 Nov 2023 12:52) - *Upvotes: 1*
B is correct

---


<br/>

## Question 104

*Date: May 14, 2023, 5:03 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is divided into teams. Each team has an AWS account, and all the accounts are in an organization in AWS Organizations. Each team must retain full administrative rights to its AWS account. Each team also must be allowed to access only AWS services that the company approves for use. AWS services must gain approval through a request and approval process.

How should a DevOps engineer configure the accounts to meet these requirements?

**Options:**
- A. Use AWS CloudFormation StackSets to provision IAM policies in each account to deny access to restricted AWS services. In each account, configure AWS Config rules that ensure that the policies are attached to IAM principals in the account.
- B. Use AWS Control Tower to provision the accounts into OUs within the organization. Configure AWS Control Tower to enable AWS IAM Identity Center (AWS Single Sign-On). Configure IAM Identity Center to provide administrative access. Include deny policies on user roles for restricted AWS services.
- C. Place all the accounts under a new top-level OU within the organization. Create an SCP that denies access to restricted AWS services. Attach the SCP to the OU.
- D. Create an SCP that allows access to only approved AWS services. Attach the SCP to the root OU of the organization. Remove the FullAWSAccess SCP from the root OU of the organization.

> **Suggested Answer:** D
> **Community Vote:** D (54%), C (44%), 2%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**lunt** (Fri 26 May 2023 18:19) - *Upvotes: 22*
A=local account admin can change this.
B=local admin has admin permissions. Complicated.
C=implicit permit on everything else = breaks requirements.
D= As they want to approve each service, its got to be white-list based SCP setup.
Answer is D.

---

**dkp** (Sun 14 Apr 2024 07:26) - *Upvotes: 6*
Ans D:
It is easier to allow approved services than deny all the other services, considering the vast amount of AWS services. it's easier to whitelist than blacklisting all the remaining services.

---

**nickp84** (Thu 15 May 2025 13:55) - *Upvotes: 1*
D. SCP that allows only approved services at the root OU: This is risky because it affects all accounts in the organization, including potentially critical shared services or management accounts. It's better to scope restrictions to a specific OU.

---

**ce0df07** (Thu 06 Feb 2025 00:26) - *Upvotes: 2*
Not Option D, Using allow-list SCP, since:
- Too restrictive
- More difficult to maintain
- Might block essential services
- Could break account functionality
- Requires constant updates

---

**teo2157** (Thu 23 Jan 2025 08:59) - *Upvotes: 3*
Going for C as removing the FullAWSAccess SCP from the root OU requires impacts directly in the Administrative Access and restrict necessary administrative actions required for account management and operations.

---

**auxwww** (Sun 13 Oct 2024 16:05) - *Upvotes: 2*
D is more straight forward

---

**hzaki** (Sun 25 Aug 2024 16:38) - *Upvotes: 2*
The answer is (D). The following SCP example from the AWS DOCUMENT allows accounts to create resource shares that share prefix lists
https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_ram.html

---

**jamesf** (Fri 02 Aug 2024 07:28) - *Upvotes: 2*
I prefer C than D.
As SCP more in Deny but not Allow
https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html

---

**auxwww** (Thu 25 Jul 2024 02:16) - *Upvotes: 3*
SCP - only deny not allow - So answer is C

---

**zsoni** (Thu 20 Jun 2024 02:36) - *Upvotes: 1*
The question is looking to use the Allow List Strategy using SCP. So the answer that best fits is D.

---


<br/>

## Question 105

*Date: May 14, 2023, 12:13 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer used an AWS CloudFormation custom resource to set up AD Connector. The AWS Lambda function ran and created AD Connector, but CloudFormation is not transitioning from CREATE_IN_PROGRESS to CREATE_COMPLETE.

Which action should the engineer take to resolve this issue?

**Options:**
- A. Ensure the Lambda function code has exited successfully.
- B. Ensure the Lambda function code returns a response to the pre-signed URL.
- C. Ensure the Lambda function IAM role has cloudformation:UpdateStack permissions for the stack ARN.
- D. Ensure the Lambda function IAM role has ds:ConnectDirectory permissions for the AWS account.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**haazybanj** (Fri 29 Dec 2023 18:25) - *Upvotes: 14*
B. Ensure the Lambda function code returns a response to the pre-signed URL.
Explanation:
When using a custom resource in CloudFormation, the AWS Lambda function responsible for handling the resource creation should send a response to the pre-signed URL provided by CloudFormation. This response signals the completion status of the custom resource creation process to CloudFormation.

In this case, since the Lambda function successfully created the AD Connector, the engineer should ensure that the Lambda function code includes the logic to send a response to the pre-signed URL. This response should indicate the success status and any relevant data, such as the ARN or other details of the created AD Connector.

---

**dkp** (Mon 14 Oct 2024 07:33) - *Upvotes: 1*
its B

---

**thanhnv142** (Sun 04 Aug 2024 13:34) - *Upvotes: 2*
B is correct: <but CloudFormation is not transitioning from CREATE_IN_PROGRESS to CREATE_COMPLETE> means ACF hasnot received a response code from Lambda
A, C and D: no mention of response code

---

**YR4591** (Sat 27 Apr 2024 12:13) - *Upvotes: 1*
Lambda should send a cfnresponsse to presign url

---

**BaburTurk** (Fri 12 Apr 2024 10:15) - *Upvotes: 3*
B is correct
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html

---

**MarDog** (Thu 21 Dec 2023 00:35) - *Upvotes: 2*
It's B.

---

**OrganizedChaos25** (Fri 17 Nov 2023 14:43) - *Upvotes: 2*
B is correct

---

**devnv** (Tue 14 Nov 2023 13:13) - *Upvotes: 2*
B is the right answer

---


<br/>

## Question 106

*Date: May 14, 2023, 12:16 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses AWS CodeCommit for source code control. Developers apply their changes to various feature branches and create pull requests to move those changes to the main branch when the changes are ready for production.

The developers should not be able to push changes directly to the main branch. The company applied the AWSCodeCommitPowerUser managed policy to the developers’ IAM role, and now these developers can push changes to the main branch directly on every repository in the AWS account.

What should the company do to restrict the developers’ ability to push changes to the main branch directly?

**Options:**
- A. Create an additional policy to include a Deny rule for the GitPush and PutFile actions. Include a restriction for the specific repositories in the policy statement with a condition that references the main branch.
- B. Remove the IAM policy, and add an AWSCodeCommitReadOnly managed policy. Add an Allow rule for the GitPush and PutFile actions for the specific repositories in the policy statement with a condition that references the main branch.
- C. Modify the IAM policy. Include a Deny rule for the GitPush and PutFile actions for the specific repositories in the policy statement with a condition that references the main branch.
- D. Create an additional policy to include an Allow rule for the GitPush and PutFile actions. Include a restriction for the specific repositories in the policy statement with a condition that references the feature branches.

> **Suggested Answer:** A
> **Community Vote:** A (94%), 6%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Just_Ninja** (Wed 12 Jul 2023 19:40) - *Upvotes: 17*
A is possible!
If you think C is correct, then you should know that a policy managed by AWS cannot be modified.

---

**jamesf** (Mon 05 Aug 2024 16:31) - *Upvotes: 1*
Not C as AWS managed policy cannot be modified

---

**zijo** (Tue 11 Jun 2024 20:56) - *Upvotes: 2*
AWS Managed Policies are read-only, meaning you cannot modify their contents. If you need a similar policy with slight modifications, you can copy the managed policy and create a customer-managed policy.

---

**dkp** (Sun 14 Apr 2024 07:37) - *Upvotes: 1*
it s A.

---

**thanhnv142** (Sun 04 Feb 2024 14:43) - *Upvotes: 3*
A is correct: <The developers should not be able to push changes directly to the main branch> means we should deny these permissions in IAM policy. <managed polic> means we should add another policy, not modify this one.
B: <Remove the IAM policy>: this is an managed policy, cannot remove it
C: Cannot modify a managed policy. We can only create another policy
D: This option would deny commiting code to every sub-branches, which is not correct

---

**giovanna_mag** (Tue 26 Dec 2023 13:48) - *Upvotes: 3*
A, AWS managed policy cannot be modified, additional policy must be attached with a DENY

---

**Blueee** (Wed 05 Jul 2023 13:07) - *Upvotes: 1*
A is correct

---

**rhinozD** (Fri 16 Jun 2023 15:42) - *Upvotes: 3*
AWSCodeCommitPowerUser is an AWS-managed policy.
So you need to add an additional policy to deny push to the main branch directly.

---

**Kodoma** (Tue 23 May 2023 22:57) - *Upvotes: 1*
A is correct

---

**Ryan1002** (Mon 22 May 2023 08:02) - *Upvotes: 2*
It`s A

---


<br/>

## Question 107

*Date: May 14, 2023, 12:20 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company manages a web application that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2 instances run in an Auto Scaling group across multiple Availability Zones. The application uses an Amazon RDS for MySQL DB instance to store the data. The company has configured Amazon Route 53 with an alias record that points to the ALB.

A new company guideline requires a geographically isolated disaster recovery (DR) site with an RTO of 4 hours and an RPO of 15 minutes.

Which DR strategy will meet these requirements with the LEAST change to the application stack?

**Options:**
- A. Launch a replica environment of everything except Amazon RDS in a different Availability Zone. Create an RDS read replica in the new Availability Zone, and configure the new stack to point to the local RDS DB instance. Add the new stack to the Route 53 record set by using a health check to configure a failover routing policy.
- B. Launch a replica environment of everything except Amazon RDS in a different AWS Region. Create an RDS read replica in the new Region, and configure the new stack to point to the local RDS DB instance. Add the new stack to the Route 53 record set by using a health check to configure a latency routing policy.
- C. Launch a replica environment of everything except Amazon RDS in a different AWS Region. In the event of an outage, copy and restore the latest RDS snapshot from the primary Region to the DR Region. Adjust the Route 53 record set to point to the ALB in the DR Region.
- D. Launch a replica environment of everything except Amazon RDS in a different AWS Region. Create an RDS read replica in the new Region, and configure the new environment to point to the local RDS DB instance. Add the new stack to the Route 53 record set by using a health check to configure a failover routing policy. In the event of an outage, promote the read replica to primary.

> **Suggested Answer:** D
> **Community Vote:** D (90%), 10%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**YR4591** (Sat 27 Apr 2024 12:18) - *Upvotes: 5*
D is correct. Failover policy will route traffic to the ALB in the backup region.

---

**hayjaykay** (Sat 30 Aug 2025 09:02) - *Upvotes: 1*
Since it has to be geographically isolated, it has to be in a different region. The answer is D.

---

**youonebe** (Sat 21 Dec 2024 15:13) - *Upvotes: 1*
D is correct. My answer was C, but GPT told me this:
This process does not guarantee a quick recovery within the 4-hour RTO. Restoring from a snapshot would take significant time, and it doesn't provide continuous replication of data to minimize RPO. This option does not meet the RTO requirement because the recovery process (snapshot restore) will take too long. It also doesn’t address continuous data replication, leading to a higher potential RPO.

---

**dkp** (Mon 14 Oct 2024 07:49) - *Upvotes: 2*
geographically isolated location applies to option D

---

**WhyIronMan** (Mon 30 Sep 2024 10:46) - *Upvotes: 3*
Answer is D.

A. It did not cover the whole scenario. there is a need to promote the read replica otherwise the application in the region will not be able to write in that rds. Also, another Region means geographically isolated, while other Az will not solve the problem.

Details are everything during an investigation...

---

**WhyIronMan** (Mon 30 Sep 2024 10:45) - *Upvotes: 1*
Answer is D.

A. It did not cover the whole scenario. there is a need to promote the read replica otherwise the application in the region will not be able to write in that rds. Also, another Region means geographically isolated, while other Az will not solve the problem.

Details are everything during an investigation...

---

**jojom19980** (Mon 19 Aug 2024 11:08) - *Upvotes: 2*
they mentioned geographically not regional , the cost will be more if we make it regional so we can go with the AZ DR

---

**WhyIronMan** (Mon 30 Sep 2024 10:44) - *Upvotes: 1*
A did not cover the whole scenario. there is a need to promote the read replica otherwise the application in the region will not be able to write in that rds. Also, another Region means geographically isolated.
Details are everything during an investigation...

---

**thanhnv142** (Sun 04 Aug 2024 13:51) - *Upvotes: 3*
D is correct: < configured Amazon Route 53> and <requires a geographically isolated disaster recovery (DR) site> means fail-over routing and the DR site should be in another region
A: <Amazon RDS in a different Availability Zone>: We need to setup the DB in a different region, not in a different AZ, which is still in the same region
B and C: no mention of fail-over

---

**sarlos** (Wed 03 Jul 2024 00:28) - *Upvotes: 2*
D is the answer

---


<br/>

## Question 108

*Date: May 14, 2023, 12:22 p.m.
Disclaimers:
- ExamTopics website is not rel*

A large enterprise is deploying a web application on AWS. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The application stores data in an Amazon RDS for Oracle DB instance and Amazon DynamoDB. There are separate environments for development, testing, and production.

What is the MOST secure and flexible way to obtain password credentials during deployment?

**Options:**
- A. Retrieve an access key from an AWS Systems Manager SecureString parameter to access AWS services. Retrieve the database credentials from a Systems Manager SecureString parameter.
- B. Launch the EC2 instances with an EC2 IAM role to access AWS services. Retrieve the database credentials from AWS Secrets Manager.
- C. Retrieve an access key from an AWS Systems Manager plaintext parameter to access AWS services. Retrieve the database credentials from a Systems Manager SecureString parameter.
- D. Launch the EC2 instances with an EC2 IAM role to access AWS services. Store the database passwords in an encrypted config file with the application artifacts.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Mon 29 Jul 2024 06:39) - *Upvotes: 2*
Keywords: MOST secure

---

**zijo** (Wed 12 Jun 2024 21:01) - *Upvotes: 2*
This step is important for applications running on EC2 instances to retrieve passwords from AWS Secrets Manager.
Create an IAM role with the necessary permissions to access AWS Secrets Manager.
Attach this IAM role to your EC2 instance.

---

**c3518fc** (Fri 19 Apr 2024 16:21) - *Upvotes: 3*
The most secure and flexible way to obtain password credentials during deployment in the given scenario is to use AWS Secrets Manager. AWS Secrets Manager is a service that allows you to securely store, retrieve, and rotate credentials, such as passwords, API keys, and other sensitive data.

---

**dkp** (Sun 14 Apr 2024 07:53) - *Upvotes: 2*
B seems more relevant

---

**WhyIronMan** (Sun 31 Mar 2024 10:48) - *Upvotes: 2*
B. EC2 Role + Secrets Mananger

---

**thanhnv142** (Sun 04 Feb 2024 14:54) - *Upvotes: 4*
B is correct: <obtain password credentials> means we should consider AWS SSM and secret manager. However, <the MOST secure > means we should opt for secret manager, which is more costly but more secure
A, C and D: no mention of secret manager

---

**sarlos** (Wed 03 Jan 2024 02:26) - *Upvotes: 1*
why not A?

---

**thanhnv142** (Sun 04 Feb 2024 14:55) - *Upvotes: 3*
<obtain password credentials> means we should consider AWS SSM and secret manager. However, <the MOST secure > means we should opt for secret manager, which is more costly but more secure

---

**davdan99** (Tue 09 Jan 2024 10:40) - *Upvotes: 1*
We are not storing access keys for EC2 instances, instead we are using instance profile for that it is the best practice, and for database credentials it is correct to use Secret manager, it is more integrated with RDS, and other database services within AWS.

---

**giovanna_mag** (Tue 26 Dec 2023 13:53) - *Upvotes: 2*
I vote B

---


<br/>

## Question 109

*Date: May 9, 2023, 12:36 p.m.
Disclaimers:
- ExamTopics website is not rel*

The security team depends on AWS CloudTrail to detect sensitive security issues in the company’s AWS account. The DevOps engineer needs a solution to auto-remediate CloudTrail being turned off in an AWS account.

What solution ensures the LEAST amount of downtime for the CloudTrail log deliveries?

**Options:**
- A. Create an Amazon EventBridge rule for the CloudTrail StopLogging event. Create an AWS Lambda function that uses the AWS SDK to call StartLogging on the ARN of the resource in which StopLogging was called. Add the Lambda function ARN as a target to the EventBridge rule.
- B. Deploy the AWS-managed CloudTrail-enabled AWS Config rule, set with a periodic interval of 1 hour. Create an Amazon EventBridge rule for AWS Config rules compliance change. Create an AWS Lambda function that uses the AWS SDK to call StartLogging on the ARN of the resource in which StopLogging was called. Add the Lambda function ARN as a target to the EventBridge rule.
- C. Create an Amazon EventBridge rule for a scheduled event every 5 minutes. Create an AWS Lambda function that uses the AWS SDK to call StartLogging on a CloudTrail trail in the AWS account. Add the Lambda function ARN as a target to the EventBridge rule.
- D. Launch a t2.nano instance with a script running every 5 minutes that uses the AWS SDK to query CloudTrail in the current account. If the CloudTrail trail is disabled, have the script re-enable the trail.

> **Suggested Answer:** A
> **Community Vote:** A (93%), 7%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**rhinozD** (Sat 16 Dec 2023 16:54) - *Upvotes: 15*
A.
old but gold link:
https://aws.amazon.com/blogs/mt/monitor-changes-and-auto-enable-logging-in-aws-cloudtrail/

---

**daburahjail** (Tue 19 Mar 2024 03:17) - *Upvotes: 2*
Good job, buddy

---

**c3518fc** (Sat 19 Oct 2024 16:36) - *Upvotes: 2*
This solution ensures the least amount of downtime for CloudTrail log deliveries when auto-remediating CloudTrail being turned off. Here's why:

Event-Driven Automation: By creating an Amazon EventBridge rule for the CloudTrail StopLogging event, the remediation process is triggered immediately when CloudTrail logging is stopped, minimizing the downtime.
Targeted Remediation: The Lambda function uses the AWS SDK to call StartLogging on the specific CloudTrail trail ARN where the StopLogging event occurred. This targeted approach ensures that logging is re-enabled for the affected trail without impacting other trails or introducing unnecessary overhead.
Low Latency: EventBridge rules and Lambda functions are designed to be highly responsive, ensuring that the remediation action is initiated with minimal delay after the StopLogging event occurs.

---

**WhyIronMan** (Mon 30 Sep 2024 10:53) - *Upvotes: 1*
A. is correct
Details are everything during an investigation

---

**thanhnv142** (Sun 04 Aug 2024 14:02) - *Upvotes: 2*
A is correct: <The DevOps engineer needs a solution to auto-remediate CloudTrail being turned off> means we should turn on it again if we detect that it is turn-off. AWS config rule or Eventbridge would be considered. < the LEAST amount of downtime> means we should choose A because this minimizes downtime
B: this option utilizes an AWS config rule, which is good. But it sets the rule with a periodic interval of 1 hours, which would introduce a lot of downtime
C: this option utilizes evenbridge, but the event to trigger eventbridge is undetermined
D: Should not use a custom script to do the task

---

**HugoFM** (Wed 29 May 2024 15:23) - *Upvotes: 2*
A its quicker and the solution is asking for the leeast amount of downtime

---

**YR4591** (Sat 27 Apr 2024 12:29) - *Upvotes: 1*
"LEAST amount of downtime" = A
cloudwatch event is near real time. Al the other options are not.

---

**RVivek** (Sun 03 Mar 2024 11:56) - *Upvotes: 1*
Both A and B will work. However the question mentions leatst Cloutrial down time. Option A is correct beacuse the remiation is triggred immeiately .
Option B can be delayed a it uns once in ever hour

---

**Seoyong** (Fri 23 Feb 2024 11:46) - *Upvotes: 1*
I don't think stoplogging is CloudTrail being turned off.
You can stop logging anytime - https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-turning-off-logging.html

But it doesn't means CloudTrail being turned off

---

**Seoyong** (Fri 16 Feb 2024 10:39) - *Upvotes: 1*
cloudtrail-enabled rule will check CloudTrail being turned off.

---


<br/>

## Question 110

*Date: May 8, 2023, 3:43 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses AWS CodeArtifact to centrally store Python packages. The CodeArtifact repository is configured with the following repository policy:



A development team is building a new project in an account that is in an organization in AWS Organizations. The development team wants to use a Python library that has already been stored in the CodeArtifact repository in the organization. The development team uses AWS CodePipeline and AWS CodeBuild to build the new application. The CodeBuild job that the development team uses to build the application is configured to run in a VPC. Because of compliance requirements, the VPC has no internet connectivity.

The development team creates the VPC endpoints for CodeArtifact and updates the CodeBuild buildspec.yaml file. However, the development team cannot download the Python library from the repository.

Which combination of steps should a DevOps engineer take so that the development team can use CodeArtifact? (Choose two.)

**Options:**
- A. Create an Amazon S3 gateway endpoint. Update the route tables for the subnets that are running the CodeBuild job.
- B. Update the repository policy’s Principal statement to include the ARN of the role that the CodeBuild project uses.
- C. Share the CodeArtifact repository with the organization by using AWS Resource Access Manager (AWS RAM).
- D. Update the role that the CodeBuild project uses so that the role has sufficient permissions to use the CodeArtifact repository.
- E. Specify the account that hosts the repository as the delegated administrator for CodeArtifact in the organization.

> **Suggested Answer:** AD
> **Community Vote:** AD (52%), BD (40%), 8%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**TroyMcLure** (Sun 28 May 2023 00:48) - *Upvotes: 13*
I guess the answer is AD because of this:
"AWS CodeArtifact operates in multiple Availability Zones and stores artifact data and metadata in Amazon S3 and Amazon DynamoDB. Your encrypted data is redundantly stored across multiple facilities and multiple devices in each facility, making it highly available and highly durable."
https://aws.amazon.com/codeartifact/features/
With no internet connectivity, a gateway endpoint becomes necessary to access S3.

---

**RVivek** (Thu 14 Sep 2023 11:08) - *Upvotes: 2*
A- incorrect because the question says Devops engineers careted VPC endpoints for CodeArtifact

---

**RVivek** (Wed 20 Sep 2023 01:31) - *Upvotes: 2*
AD even though Devops engineer created a CodeArtifcat still a S3 end point is required

---

**Venki_dev** (Tue 18 Jun 2024 18:56) - *Upvotes: 1*
note here says "An Amazon S3 endpoint is not needed when using Python or Swift package formats."
https://docs.aws.amazon.com/codeartifact/latest/ug/create-s3-gateway-endpoint.html

---

**Arnaud92** (Sun 04 Jun 2023 08:41) - *Upvotes: 8*
https://docs.aws.amazon.com/codeartifact/latest/ug/create-s3-gateway-endpoint.html

It clearly state that you need to create a S3 endpoint to use codeartifact in a private network.

---

**vortegon** (Wed 07 Feb 2024 18:38) - *Upvotes: 5*
An Amazon S3 endpoint is not needed when using Python or Swift package formats.

---

**syh_rapha** (Thu 11 Jul 2024 02:55) - *Upvotes: 2*
When this question was created, there was no exception for Python and Swift packages. You can check this using the Wayback machine: https://web.archive.org/web/20230521063821/https://docs.aws.amazon.com/codeartifact/latest/ug/create-s3-gateway-endpoint.html

Considering that it's very common to have outdated questions in the exam, I'd say this is one those cases. So yeah, I'll also go with AD (also because B is not needed since the repository policy is already allowing the entire org).

---

**Jowblow** (Mon 08 May 2023 15:43) - *Upvotes: 6*
Codeartifact uses s3 gateway endpoints to store packages. The key word here are no internet access.

---

**92a2133** (Mon 26 May 2025 17:28) - *Upvotes: 1*
Its CD and when I saw the most voted I was shocked with the answers

A. Even though CodeArtifact uses S3 it can also use DynamoDB, theres no mention of either so I immediately negated this answer
B. Principal is already set to allow any
E. Irrelevant

---

**nickp84** (Sun 11 May 2025 12:18) - *Upvotes: 2*
CodeArtifact does not store packages in user-managed S3 buckets, and it does not require S3 endpoint access. This is unnecessary for CodeArtifact access.

---


<br/>

## Question 111

*Date: May 9, 2023, 12:26 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses a series of individual Amazon CloudFormation templates to deploy its multi-Region applications. These templates must be deployed in a specific order. The company is making more changes to the templates than previously expected and wants to deploy new templates more efficiently. Additionally, the data engineering team must be notified of all changes to the templates.

What should the company do to accomplish these goals?

**Options:**
- A. Create an AWS Lambda function to deploy the CloudFormation templates in the required order. Use stack policies to alert the data engineering team.
- B. Host the CloudFormation templates in Amazon S3. Use Amazon S3 events to directly trigger CloudFormation updates and Amazon SNS notifications.
- C. Implement CloudFormation StackSets and use drift detection to trigger update alerts to the data engineering team.
- D. Leverage CloudFormation nested stacks and stack sets for deployments. Use Amazon SNS to notify the data engineering team.

> **Suggested Answer:** D
> **Community Vote:** D (94%), 6%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**emupsx1** (Wed 17 Jan 2024 13:03) - *Upvotes: 13*
The answer is D because:
A few hours ago, I just finished the DOP-C02 exam.
My score is 1000 points.
This question has come up, I choose D.

---

**BaburTurk** (Sat 09 Mar 2024 12:08) - *Upvotes: 9*
pics or it did not happen, troll bot account

---

**c3518fc** (Sat 19 Oct 2024 17:33) - *Upvotes: 5*
Here's why this solution is the best approach:

Nested Stacks: CloudFormation nested stacks allow you to break down complex templates into smaller, more manageable templates. You can create a root stack that references and manages multiple nested stacks. This approach simplifies the management and deployment of multiple interdependent templates in the correct order.
StackSets: CloudFormation StackSets allow you to create, update, or delete stacks across multiple AWS accounts and regions with a single operation. This addresses the requirement of deploying applications across multiple regions efficiently.
Amazon SNS: Amazon Simple Notification Service (SNS) can be used to send notifications to the data engineering team whenever changes are made to the CloudFormation templates or stacks.

---

**dkp** (Mon 14 Oct 2024 08:15) - *Upvotes: 1*
should be D

---

**WhyIronMan** (Mon 30 Sep 2024 10:59) - *Upvotes: 1*
Answer is D.

---

**jojom19980** (Mon 19 Aug 2024 11:50) - *Upvotes: 2*
the nested for order :
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-nested-stacks.html

---

**thanhnv142** (Sun 04 Aug 2024 14:26) - *Upvotes: 3*
D is correct: <uses a series of individual Amazon CloudFormation templates to deploy its multi-Region applications> and <wants to deploy new templates more efficiently> mean stacksets, which is template for multiple regions. <the data engineering team must be notified of all changes> means SNS
A and B: no mention of stacksets
C: no mention of SNS

---

**sarlos** (Wed 03 Jul 2024 01:45) - *Upvotes: 1*
D is the answer

---

**YR4591** (Sat 27 Apr 2024 12:41) - *Upvotes: 1*
It's D.
C is not correct since according to this link:
https://aws.amazon.com/blogs/mt/implementing-an-alarm-to-automatically-detect-drift-in-aws-cloudformation-stacks/
We need AWS config rule to detect drifts and to send event. There is no build in solution to notify drift detection like mentioned in C,

---

**daburahjail** (Tue 19 Mar 2024 03:24) - *Upvotes: 1*
C is for notifying changes on what has been deployed by Cloud Formation
D is for notifying changes made on the Cloud Formation template (the recipe) itself

---


<br/>

## Question 112

*Date: May 12, 2023, 8:14 a.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer has implemented a CI/CD pipeline to deploy an AWS CloudFormation template that provisions a web application. The web application consists of an Application Load Balancer (ALB), a target group, a launch template that uses an Amazon Linux 2 AMI, an Auto Scaling group of Amazon EC2 instances, a security group, and an Amazon RDS for MySQL database. The launch template includes user data that specifies a script to install and start the application.

The initial deployment of the application was successful. The DevOps engineer made changes to update the version of the application with the user data. The CI/CD pipeline has deployed a new version of the template. However, the health checks on the ALB are now failing. The health checks have marked all targets as unhealthy.

During investigation, the DevOps engineer notices that the CloudFormation stack has a status of UPDATE_COMPLETE. However, when the DevOps engineer connects to one of the EC2 instances and checks /var/log/messages, the DevOps engineer notices that the Apache web server failed to start successfully because of a configuration error.

How can the DevOps engineer ensure that the CloudFormation deployment will fail if the user data fails to successfully finish running?

**Options:**
- A. Use the cfn-signal helper script to signal success or failure to CloudFormation. Use the WaitOnResourceSignals update policy within the CloudFormation template. Set an appropriate timeout for the update policy.
- B. Create an Amazon CloudWatch alarm for the UnhealthyHostCount metric. Include an appropriate alarm threshold for the target group. Create an Amazon Simple Notification Service (Amazon SNS) topic as the target to signal success or failure to CloudFormation.
- C. Create a lifecycle hook on the Auto Scaling group by using the AWS::AutoScaling::LifecycleHook resource. Create an Amazon Simple Notification Service (Amazon SNS) topic as the target to signal success or failure to CloudFormation. Set an appropriate timeout on the lifecycle hook.
- D. Use the Amazon CloudWatch agent to stream the cloud-init logs. Create a subscription filter that includes an AWS Lambda function with an appropriate invocation timeout. Configure the Lambda function to use the SignalResource API operation to signal success or failure to CloudFormation.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Certified101** (Wed 19 Jul 2023 15:22) - *Upvotes: 6*
A is correct

---

**danish1234** (Tue 19 Aug 2025 12:22) - *Upvotes: 1*
https://docs.aws.amazon.com/AWSCloudFormation/latest/TemplateReference/cfn-signal.html

---

**zijo** (Thu 13 Jun 2024 21:09) - *Upvotes: 3*
To ensure that the CloudFormation deployment fails if the user data script does not successfully finish, you can use a combination of AWS CloudFormation's CreationPolicy, cfn-signal, and wait condition resources. These mechanisms can signal CloudFormation about the success or failure of the instance creation process, including the execution of user data scripts.

---

**dkp** (Sun 14 Apr 2024 08:21) - *Upvotes: 2*
A is correct

---

**thanhnv142** (Sun 04 Feb 2024 15:34) - *Upvotes: 3*
A is correct: <ensure that the CloudFormation deployment will fail if the user data fails to successfully finish running> means we need cfn-signal
B, C and D: no mention of cfn-signal

---

**sarlos** (Wed 03 Jan 2024 02:48) - *Upvotes: 3*
yes A.
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-signal.html

---

**svjl** (Wed 06 Dec 2023 15:27) - *Upvotes: 1*
The instance is running, and the logs are available. The configuration happens inside the instance by the userdata. How A is correct if the issue is beyond CF?

---

**svjl** (Wed 06 Dec 2023 15:29) - *Upvotes: 2*
Ok now make sense: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-signal.html

---

**OrganizedChaos25** (Wed 17 May 2023 13:57) - *Upvotes: 1*
A is correct

---

**devnv** (Mon 15 May 2023 02:34) - *Upvotes: 1*
A is the right answer

---


<br/>

## Question 113

*Date: May 14, 2023, 2:07 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has a data ingestion application that runs across multiple AWS accounts. The accounts are in an organization in AWS Organizations. The company needs to monitor the application and consolidate access to the application. Currently, the company is running the application on Amazon EC2 instances from several Auto Scaling groups. The EC2 instances have no access to the internet because the data is sensitive. Engineers have deployed the necessary VPC endpoints. The EC2 instances run a custom AMI that is built specifically for the application.

To maintain and troubleshoot the application, system administrators need the ability to log in to the EC2 instances. This access must be automated and controlled centrally. The company’s security team must receive a notification whenever the instances are accessed.

Which solution will meet these requirements?

**Options:**
- A. Create an Amazon EventBridge rule to send notifications to the security team whenever a user logs in to an EC2 instance. Use EC2 Instance Connect to log in to the instances. Deploy Auto Scaling groups by using AWS CloudFormation. Use the cfn-init helper script to deploy appropriate VPC routes for external access. Rebuild the custom AMI so that the custom AMI includes AWS Systems Manager Agent.
- B. Deploy a NAT gateway and a bastion host that has internet access. Create a security group that allows incoming traffic on all the EC2 instances from the bastion host. Install AWS Systems Manager Agent on all the EC2 instances. Use Auto Scaling group lifecycle hooks for monitoring and auditing access. Use Systems Manager Session Manager to log in to the instances. Send logs to a log group in Amazon CloudWatch Logs. Export data to Amazon S3 for auditing. Send notifications to the security team by using S3 event notifications.
- C. Use EC2 Image Builder to rebuild the custom AMI. Include the most recent version of AWS Systems Manager Agent in the image. Configure the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to all the EC2 instances. Use Systems Manager Session Manager to log in to the instances. Enable logging of session details to Amazon S3. Create an S3 event notification for new file uploads to send a message to the security team through an Amazon Simple Notification Service (Amazon SNS) topic.
- D. Use AWS Systems Manager Automation to build Systems Manager Agent into the custom AMI. Configure AWS Config to attach an SCP to the root organization account to allow the EC2 instances to connect to Systems Manager. Use Systems Manager Session Manager to log in to the instances. Enable logging of session details to Amazon S3. Create an S3 event notification for new file uploads to send a message to the security team through an Amazon Simple Notification Service (Amazon SNS) topic.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Blueee** (Wed 05 Jul 2023 13:45) - *Upvotes: 7*
C and D are left over choice due to no internet access for EC2

C is correct
By using EC2 Image Builder to rebuild the custom AMI and including the most recent version of AWS Systems Manager Agent in the image, you can configure the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to all the EC2 instances. This allows you to use Systems Manager Session Manager to log in to the instances. You can enable logging of session details to Amazon S3 and create an S3 event notification for new file uploads to send a message to the security team through an Amazon Simple Notification Service (Amazon SNS) topic2

---

**thanhnv142** (Sun 04 Feb 2024 15:49) - *Upvotes: 6*
C is correct: <The company needs to monitor the application and consolidate access to the application> means using SSM. We should install SSM agent on all EC2 instances. <The EC2 instances run a custom AMI that is built specifically for the application> means we should rebuild the image and integrate agent into the AMI. To rebuild, the best option is EC2 image builder. <The company’s security team must receive a notification whenever the instances are accessed.> means SNS
A: <Rebuild the custom AMI so that the custom AMI includes AWS Systems Manager Agent.>: no mention of using EC2 image builder and SNS
B: no mention of integrating SSM agents into the AMI and we cannot just send S3 noti to users <Send notifications to the security team by using S3 event notifications.>
D: no me ntion of using EC2 image builder to rebuild the AMI.

---

**Saudis** (Wed 13 Nov 2024 20:14) - *Upvotes: 1*
Ans is C because The keyword is access must be automated and controlled centrally

---

**jamesf** (Fri 02 Aug 2024 08:16) - *Upvotes: 2*
- AWS Systems Manager Agent
- Systems Manager Session Manager for login the instances
- enable logging of session details to s3
- s3 event notification to SNS.

---

**dkp** (Sun 14 Apr 2024 09:17) - *Upvotes: 1*
C is correct

---

**haazybanj** (Thu 27 Jul 2023 03:02) - *Upvotes: 2*
C

Option C offers a well-architected approach to addressing the requirements, providing both centralized access and logging, and automated login to EC2 instances for system administrators. Additionally, it ensures that the security team receives notifications for auditing and monitoring purposes.

---

**PhuocT** (Sat 20 May 2023 08:22) - *Upvotes: 3*
D is not a good option for the following reasons:

1. AWS Systems Manager Automation is not the ideal choice for building a custom AMI. Instead, EC2 Image Builder, as stated in option C, is an AWS service designed for building, testing, and maintaining Golden Amazon Machine Images (AMIs), making it a suitable choice for both building and managing custom AMIs.

2. The option D suggests attaching an SCP (Service Control Policy) to the root organization to allow EC2 instances to connect to Systems Manager. This approach is incorrect because SCPs are used to define permissions on an organizational level, rather than allowing specific access between resources like EC2 instances and Systems Manager. Attaching the AmazonSSMManagedInstanceCore role to EC2 instances as mentioned in option C is the correct method, which allows instances to communicate with Systems Manager.

---

**2pk** (Sun 14 May 2023 02:07) - *Upvotes: 2*
if someone know why D is not correct , pls post

---

**92a2133** (Mon 26 May 2025 17:45) - *Upvotes: 1*
because SCPs are not meant to be used to explicitly allow access to resources, its meant to deny permissions within an organization

---

**MarDog** (Tue 20 Jun 2023 23:56) - *Upvotes: 2*
Because I don't think AWS Config can be used to attach an SCP.

---


<br/>

## Question 114

*Date: May 17, 2023, 2:13 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses Amazon S3 to store proprietary information. The development team creates buckets for new projects on a daily basis. The security team wants to ensure that all existing and future buckets have encryption, logging, and versioning enabled. Additionally, no buckets should ever be publicly read or write accessible.

What should a DevOps engineer do to meet these requirements?

**Options:**
- A. Enable AWS CloudTrail and configure automatic remediation using AWS Lambda.
- B. Enable AWS Config rules and configure automatic remediation using AWS Systems Manager documents.
- C. Enable AWS Trusted Advisor and configure automatic remediation using Amazon EventBridge.
- D. Enable AWS Systems Manager and configure automatic remediation using Systems Manager documents.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**dkp** (Mon 14 Oct 2024 09:19) - *Upvotes: 2*
AWS config to remediate non-compliace

---

**thanhnv142** (Mon 05 Aug 2024 03:45) - *Upvotes: 4*
B is correct: <wants to ensure that all existing and future buckets have encryption, logging, and versioning enabled> means we need aws config.
A, C and D: no mention of AWS config

---

**yuliaqwerty** (Wed 10 Jul 2024 11:59) - *Upvotes: 1*
Answer B

---

**sarlos** (Wed 03 Jul 2024 01:56) - *Upvotes: 1*
yes B is correct

---

**Arnaud92** (Mon 04 Dec 2023 10:40) - *Upvotes: 4*
AWS Config allows you to remediate noncompliant resources that are evaluated by AWS Config Rules. AWS Config applies remediation using AWS Systems Manager Automation documents.

see https://docs.aws.amazon.com/config/latest/developerguide/remediation.html

---

**OrganizedChaos25** (Fri 17 Nov 2023 15:13) - *Upvotes: 3*
B is correct

---


<br/>

## Question 115

*Date: May 9, 2023, noon
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer is researching the least expensive way to implement an image batch processing cluster on AWS. The application cannot run in Docker containers and must run on Amazon EC2. The batch job stores checkpoint data on an NFS volume and can tolerate interruptions. Configuring the cluster software from a generic EC2 Linux image takes 30 minutes.

What is the MOST cost-effective solution?

**Options:**
- A. Use Amazon EFS for checkpoint data. To complete the job, use an EC2 Auto Scaling group and an On-Demand pricing model to provision EC2 instances temporarily.
- B. Use GlusterFS on EC2 instances for checkpoint data. To run the batch job, configure EC2 instances manually. When the job completes, shut down the instances manually.
- C. Use Amazon EFS for checkpoint data. Use EC2 Fleet to launch EC2 Spot Instances, and utilize user data to configure the EC2 Linux instance on startup.
- D. Use Amazon EFS for checkpoint data. Use EC2 Fleet to launch EC2 Spot Instances. Create a custom AMI for the cluster and use the latest AMI when creating instances.

> **Suggested Answer:** D
> **Community Vote:** D (88%), 8%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**ParagSanyashiv** (Fri 12 May 2023 09:33) - *Upvotes: 7*
D is more suitable, as it says to avoid 30min launch time.

---

**Mail1964** (Tue 16 May 2023 15:39) - *Upvotes: 3*
I assume you are saying D over C as D will make the EC2 instances operational quicker, while C would require 30 minutes to install the software before it can start to be used. resulting in it being more cost effective.

---

**jamesf** (Mon 29 Jul 2024 07:08) - *Upvotes: 2*
keywords: MOST cost-effective, a generic EC2 Linux image takes 30 minutes.
Mean take 30mins or longer time for EC2 booting and will cost more.

---

**WhyIronMan** (Sun 31 Mar 2024 12:52) - *Upvotes: 3*
D is the correct answer.
Make the calculations 30 min of bootstraping when you have multiple scale actions is a lot of time idle when you have many instances, so the money spent during a lot of spot request that wast time boostraping is larger than keeping a single AMI.

Details are everything during an investigation...

---

**Diego1414** (Wed 21 Feb 2024 22:32) - *Upvotes: 2*
Answer is C.
C is cheaper than D

---

**HayLLlHuK** (Sun 07 Apr 2024 13:17) - *Upvotes: 3*
"utilize user data to configure the EC2 Linux instance on startup" - it takes an addiction time to configure an instance.
it's better to use a custom AMI and have everything preinstalled

---

**thanhnv142** (Mon 05 Feb 2024 04:51) - *Upvotes: 3*
C is correct: <can tolerate interruptions> means EC2 spot instances.
A and B: no mention of spot instances
D: Create a custom AMI for the cluster and use the latest AMI when creating instances: this incurs more cost than option C, which incurs no cost for the configuration step

---

**GripZA** (Sun 20 Apr 2025 14:13) - *Upvotes: 1*
The 30min bootstrapping will incur runtime costs, which can be avoided by using the latest AMI already containing the files, installtions etc done by the bootstrapping.

---

**vmahilevskyi** (Sun 17 Mar 2024 15:48) - *Upvotes: 3*
As for me, extra 30 minutes for each EC2 launch seems like an extra cost comparing to one-time built AMI.
So D looks cheaper than C

---

**zain1258** (Thu 16 Nov 2023 19:02) - *Upvotes: 2*
It's D

---


<br/>

## Question 116

*Date: May 9, 2023, 11:52 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company recently migrated its legacy application from on-premises to AWS. The application is hosted on Amazon EC2 instances behind an Application Load Balancer, which is behind Amazon API Gateway. The company wants to ensure users experience minimal disruptions during any deployment of a new version of the application. The company also wants to ensure it can quickly roll back updates if there is an issue.

Which solution will meet these requirements with MINIMAL changes to the application?

**Options:**
- A. Introduce changes as a separate environment parallel to the existing one. Configure API Gateway to use a canary release deployment to send a small subset of user traffic to the new environment.
- B. Introduce changes as a separate environment parallel to the existing one. Update the application’s DNS alias records to point to the new environment.
- C. Introduce changes as a separate target group behind the existing Application Load Balancer. Configure API Gateway to route user traffic to the new target group in steps.
- D. Introduce changes as a separate target group behind the existing Application Load Balancer. Configure API Gateway to route all traffic to the Application Load Balancer, which then sends the traffic to the new target group.

> **Suggested Answer:** A
> **Community Vote:** A (84%), Other, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**haazybanj** (Thu 27 Jul 2023 03:14) - *Upvotes: 6*
Option A is also a valid approach that can meet the requirements with MINIMAL changes to the application.

In Option A, the changes are introduced as a separate environment parallel to the existing one. This new environment can be used to deploy the new version of the application. By configuring API Gateway to use a canary release deployment, a small subset of user traffic is directed to the new environment, while the majority of traffic continues to be routed to the existing environment hosting the current version of the application.

---

**jamesf** (Mon 29 Jul 2024 07:11) - *Upvotes: 1*
Keywords: minimal disruptions, MINIMAL changes

---

**zijo** (Fri 14 Jun 2024 18:46) - *Upvotes: 2*
I chose A because the question says MINIMAL changes to the application. Canary deployment needs minimal changes to the application as it requires only adding canary settings to the deployment stage. AWS API Gateway supports canary deployments, allowing you to route a percentage of your traffic to a new stage or version of your API.

---

**dkp** (Sun 14 Apr 2024 09:43) - *Upvotes: 2*
C:
Separate Target Group: By introducing a new target group behind the existing Application Load Balancer, you can direct traffic to this new target group without affecting the existing environment. This means the new version of the application can be tested in isolation.
Step-by-Step Traffic Routing: API Gateway allows you to gradually shift user traffic from one target group (existing version) to another (new version). This means you can start with a small percentage of traffic and gradually increase it, allowing you to monitor the new version's performance and stability.
Quick Rollback: If the new version has any issues, you can quickly revert the traffic to the original target group, ensuring minimal disruption to users.
Separate Environment with Canary Deployment: This introduces a completely separate environment, requiring additional infrastructure management and potentially more configuration changes depending on how the environments are set up.

---

**WhyIronMan** (Sun 31 Mar 2024 13:03) - *Upvotes: 3*
Agree with A. A parallel environment will allow the company to test the deployment, do smoke tests and all the basic stuff to check if the application is working fine. Then, they can start serving traffic to the users, allowing 10% of the users to go to the new environment and test.
The key is the word "disruptions" disruption can be considered a failure in the infrastructure, in the communications (networking) but NOT a Bug. Even do, switching 10% of the users to test is best than switching the entire loadbalancer to a new target group because in this scenarios is 100% of users affect against 10%

Details are everything during an investigation...

---

**dzn** (Wed 28 Feb 2024 03:29) - *Upvotes: 1*
A is not meet the "MINIMAL changes to the application" requirement. If application receives requests from a different ALB, the application will receive a different request value, such as HTTP headers, and may need to be modified application. Since D is the same ALB, it is unlikely that changes will be necessary.

---

**thanhnv142** (Mon 05 Feb 2024 06:41) - *Upvotes: 3*
A is correct: <users experience minimal disruptions during any deployment of a new version of the application.> and <ensure it can quickly roll back updates if there is an issue> means deploy in parallel: canary release or blue/green deployment
B, C and D: If there was a large bug with the a new version, users would experience huge service disruptions

---

**Ramdi1** (Wed 31 Jan 2024 13:47) - *Upvotes: 3*
Canary deployment is used to stop disruption hence I have voted A

---

**zolthar_z** (Wed 27 Dec 2023 20:34) - *Upvotes: 2*
Answer is D. Even API supports canary deployment it is only if the API can redirect the traffic between two stages, in this case the API sends the traffic directly to the ALB, and from the ALB yo can choose to which environment redirect the traffic

---

**Ffida2214** (Wed 13 Dec 2023 19:02) - *Upvotes: 1*
Why not Option D, as the deployment is API gateway-->ALB-->target groups(EC2). and question is saying that: The company wants to ensure users experience minimal disruptions during any deployment of a new version of the application, with minimal changes to application (it is not saying that we shouldn't change deployment steps)

---


<br/>

## Question 117

*Date: May 12, 2023, 9:42 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company is storing 100 GB of log data in .csv format in an Amazon S3 bucket. SQL developers want to query this data and generate graphs to visualize it. The SQL developers also need an efficient, automated way to store metadata from the .csv file.

Which combination of steps will meet these requirements with the LEAST amount of effort? (Choose three.)

**Options:**
- A. Filter the data through AWS X-Ray to visualize the data.
- B. Filter the data through Amazon QuickSight to visualize the data.
- C. Query the data with Amazon Athena.
- D. Query the data with Amazon Redshift.
- E. Use the AWS Glue Data Catalog as the persistent metadata store.
- F. Use Amazon DynamoDB as the persistent metadata store.

> **Suggested Answer:** BCE
> **Community Vote:** BCE (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**habros** (Tue 09 Jan 2024 16:34) - *Upvotes: 6*
BCE. Glue Data Catalog can crawl S3 buckets to store table metadata. Then call the data catalog directly in Athena. It will show the partitions of the data.

Athena does not deal with DynamoDB directly. Hence F is out.

---

**habros** (Tue 09 Jan 2024 16:35) - *Upvotes: 3*
https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html

---

**thanhnv142** (Mon 05 Aug 2024 05:48) - *Upvotes: 5*
BCE are correct: < query this data and generate graphs to visualize it> means athena and quicksight
A: irrelevant
D: too expensive
F: Dynamodb is primarily used for storing web session data and not for this purpose

---

**92a2133** (Tue 27 May 2025 14:21) - *Upvotes: 1*
if you get this wrong you need to revoke any AWS certs you already have...

---

**OrganizedChaos25** (Fri 17 Nov 2023 15:35) - *Upvotes: 3*
BCE are correct

---

**devnv** (Thu 16 Nov 2023 03:09) - *Upvotes: 1*
BCE are correct

---

**PhuocT** (Mon 13 Nov 2023 07:07) - *Upvotes: 1*
yep, agree with B,C and E.

---

**ParagSanyashiv** (Sun 12 Nov 2023 10:42) - *Upvotes: 2*
Agree with BCE

---


<br/>

## Question 118

*Date: May 9, 2023, 10:15 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company deploys its corporate infrastructure on AWS across multiple AWS Regions and Availability Zones. The infrastructure is deployed on Amazon EC2 instances and connects with AWS IoT Greengrass devices. The company deploys additional resources on on-premises servers that are located in the corporate headquarters.

The company wants to reduce the overhead involved in maintaining and updating its resources. The company’s DevOps team plans to use AWS Systems Manager to implement automated management and application of patches. The DevOps team confirms that Systems Manager is available in the Regions that the resources are deployed in. Systems Manager also is available in a Region near the corporate headquarters.

Which combination of steps must the DevOps team take to implement automated patch and configuration management across the company’s EC2 instances, IoT devices, and on-premises infrastructure? (Choose three.)

**Options:**
- A. Apply tags to all the EC2 instances, AWS IoT Greengrass devices, and on-premises servers. Use Systems Manager Session Manager to push patches to all the tagged devices.
- B. Use Systems Manager Run Command to schedule patching for the EC2 instances, AWS IoT Greengrass devices, and on-premises servers.
- C. Use Systems Manager Patch Manager to schedule patching for the EC2 instances, AWS IoT Greengrass devices, and on-premises servers as a Systems Manager maintenance window task.
- D. Configure Amazon EventBridge to monitor Systems Manager Patch Manager for updates to patch baselines. Associate Systems Manager Run Command with the event to initiate a patch action for all EC2 instances, AWS IoT Greengrass devices, and on-premises servers.
- E. Create an IAM instance profile for Systems Manager. Attach the instance profile to all the EC2 instances in the AWS account. For the AWS IoT Greengrass devices and on-premises servers, create an IAM service role for Systems Manager.
- F. Generate a managed-instance activation. Use the Activation Code and Activation ID to install Systems Manager Agent (SSM Agent) on each server in the on-premises environment. Update the AWS IoT Greengrass IAM token exchange role. Use the role to deploy SSM Agent on all the IoT devices.

> **Suggested Answer:** CEF
> **Community Vote:** CEF (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**4bed5ff** (Tue 23 May 2023 15:04) - *Upvotes: 8*
I also choose E instead of B.

Why E is correct: "Previously in this post, you created and deployed the SSM Agent component which would have created an IAM service role. Suppose the AWS IoT Greengrass documentation was followed to deploy the SSM agent. In that case, the name of the IAM service role should be SSMServiceRole."

Why is B wrong: B is redundant given that answer C calls out Systems Manager Patch Manager which itself uses Systems Manager Run Command. Furthermore Run Command is described here to be used to run automated scripts and not to schedule patching: "we’ll demonstrate how to use Session Manager to open remote login to an edge device, patch them using Patch Manager, and run automated scripts through Run Command"

Quotes above are from: https://aws.amazon.com/blogs/mt/how-to-centrally-manage-aws-iot-greengrass-devices-using-aws-systems-manager/?force_isolation=true

---

**thanhnv142** (Mon 05 Feb 2024 08:07) - *Upvotes: 7*
CEF:
- < implement automated patch> means Systems Manager Patch Manager
- < configuration management > means we need install system manager agent
- we need to configure sufficient permissions for SSM

---

**jamesf** (Mon 29 Jul 2024 07:17) - *Upvotes: 1*
Systems Manager Patch Manager, System Manager Agent, permission

---

**c3518fc** (Fri 19 Apr 2024 19:54) - *Upvotes: 3*
By following the combination of steps C, E, and F, the DevOps team can effectively implement automated patch and configuration management across the company's EC2 instances, IoT Greengrass devices, and on-premises infrastructure using AWS Systems Manager's capabilities and best practices.

---

**dkp** (Sun 14 Apr 2024 09:49) - *Upvotes: 1*
ans is CEF

---

**DanShone** (Sat 16 Mar 2024 17:30) - *Upvotes: 1*
CEF are correct

---

**OrganizedChaos25** (Wed 17 May 2023 14:36) - *Upvotes: 1*
CEF are correct

---

**2pk** (Sun 14 May 2023 02:29) - *Upvotes: 1*
Agreed with Parag CEF

---

**ParagSanyashiv** (Fri 12 May 2023 09:46) - *Upvotes: 1*
CEF make more sense.

---

**Jeanphi72** (Tue 09 May 2023 10:15) - *Upvotes: 1*
I disagree with the solution ... FEC for me

---


<br/>

## Question 119

*Date: May 14, 2023, 2:32 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company is testing a web application that runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company uses a blue/green deployment process with immutable instances when deploying new software.

During testing, users are being automatically logged out of the application at random times. Testers also report that, when a new version of the application is deployed, all users are logged out. The development team needs a solution to ensure users remain logged in across scaling events and application deployments.

What is the MOST operationally efficient way to ensure users remain logged in?

**Options:**
- A. Enable smart sessions on the load balancer and modify the application to check for an existing session.
- B. Enable session sharing on the load balancer and modify the application to read from the session store.
- C. Store user session information in an Amazon S3 bucket and modify the application to read session information from the bucket.
- D. Modify the application to store user session information in an Amazon ElastiCache cluster.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Mon 05 Feb 2024 08:34) - *Upvotes: 5*
D is correct: <During testing, users are being automatically logged out of the application at random times>: the cause is there is no data storage that stores user's session. We need a session data storage to store user session

---

**thanhnv142** (Mon 05 Feb 2024 09:01) - *Upvotes: 1*
A. Enable smart sessions on the load balance: there is no smart session on ALB
B. Enable session sharing on the load balancer: load balancer does not store session data
C. storing session data in S3 introduces latency

---

**jamesf** (Mon 05 Aug 2024 17:08) - *Upvotes: 1*
keywords: Amazon ElastiCache cluster

---

**dkp** (Sun 14 Apr 2024 10:01) - *Upvotes: 2*
D is correct

---

**WhyIronMan** (Sun 31 Mar 2024 13:24) - *Upvotes: 2*
D is the correct one

---

**a54b16f** (Fri 12 Jan 2024 15:33) - *Upvotes: 1*
https://aws.amazon.com/blogs/developer/elasticache-as-an-asp-net-session-store/

---

**EricZhang** (Mon 22 May 2023 07:15) - *Upvotes: 2*
Why not C? Compared to D C is serverless thus more operationally efficient.

---

**lunt** (Sat 27 May 2023 20:23) - *Upvotes: 2*
like comment by accident. S3 is an object store, its not a mounted FS as such, potential performance issues & consistency rule it out. Session data - keep it local, caching system like redis or DB. C actually requires a lot more work as who is now managing the sessions, the bucket, keeping all in sync?

---

**dzn** (Thu 29 Feb 2024 06:36) - *Upvotes: 3*
This is why many web application frameworks support Redis and Memcached session. Also S3 is expensive to read, latency.

---

**OrganizedChaos25** (Wed 17 May 2023 14:37) - *Upvotes: 4*
D is correct

---


<br/>

## Question 120

*Date: May 14, 2023, 2:40 a.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer needs to configure a blue/green deployment for an existing three-tier application. The application runs on Amazon EC2 instances and uses an Amazon RDS database. The EC2 instances run behind an Application Load Balancer (ALB) and are in an Auto Scaling group.

The DevOps engineer has created a launch template and an Auto Scaling group for the blue environment. The DevOps engineer also has created a launch template and an Auto Scaling group for the green environment. Each Auto Scaling group deploys to a matching blue or green target group. The target group also specifies which software, blue or green, gets loaded on the EC2 instances. The ALB can be configured to send traffic to the blue environment’s target group or the green environment’s target group. An Amazon Route 53 record for www.example.com points to the ALB.

The deployment must move traffic all at once between the software on the blue environment’s EC2 instances to the newly deployed software on the green environment’s EC2 instances.

What should the DevOps engineer do to meet these requirements?

**Options:**
- A. Start a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment’s EC2 instances. When the rolling restart is complete, use an AWS CLI command to update the ALB to send traffic to the green environment’s target group.
- B. Use an AWS CLI command to update the ALB to send traffic to the green environment’s target group. Then start a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment’s EC2 instances.
- C. Update the launch template to deploy the green environment’s software on the blue environment’s EC2 instances. Keep the target groups and Auto Scaling groups unchanged in both environments. Perform a rolling restart of the blue environment’s EC2 instances.
- D. Start a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment’s EC2 instances. When the rolling restart is complete, update the Route 53 DNS to point to the green environment’s endpoint on the ALB.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**PhuocT** (Sat 20 May 2023 08:38) - *Upvotes: 17*
A is correct, cannot be D, as there is only one ALB. The ALB can be configured to send traffic to the blue environment’s target group or the green environment’s target group. Traffic route to blue or green, must be configure at Load balancer, in this case, update the target group.

---

**jamesf** (Mon 29 Jul 2024 07:25) - *Upvotes: 3*
One Application Load Balancer (ALB)
One Auto Scaling Group (ASG) for Blue and one Auto Scaling Group (ASG) for Green

---

**dkp** (Sun 14 Apr 2024 10:10) - *Upvotes: 2*
its A.

---

**sirronido** (Wed 10 Apr 2024 13:20) - *Upvotes: 1*
B correct .......
option A reverses the order of operations, which goes against the recommended practice of updating the load balancer first to send traffic to the new environment before deploying the new software.

---

**thanhnv142** (Mon 05 Feb 2024 09:06) - *Upvotes: 4*
A is correct: <The deployment must move traffic all at once between the software on the blue environment’s EC2 instances to the newly deployed software on the green environment’s EC2 instances.> and <The ALB can be configured to send traffic to the blue environment’s target group or the green environment’s target group.>means we should do the traffic migration manually by config the ALB
B and C: no mention of migration step
D: should not use route 53 DNS, we need to configure the ALB

---

**rlf** (Wed 01 Nov 2023 12:41) - *Upvotes: 2*
Answer is A
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-rollingupdate

---

**kyuhobe** (Fri 18 Aug 2023 17:16) - *Upvotes: 1*
The client-side caches the results of DNS queries, so DNS switching lacks immediacy, and it's challenging to transition traffic all at once. Therefore, option D doesn't meet the requirement of moving traffic all at once and is not suitable.

---

**ixdb** (Wed 16 Aug 2023 10:25) - *Upvotes: 1*
What???? No one read the question carefully. There are two ALB.
The DevOps engineer has created a launch template and an Auto Scaling group for the blue environment. The DevOps engineer also has created a launch template and an Auto Scaling group for the green environment.

---

**ixdb** (Wed 16 Aug 2023 10:26) - *Upvotes: 2*
D is right.

---

**lluukkyy** (Thu 30 Nov 2023 01:25) - *Upvotes: 2*
No, you mixed it up. There is only one ALB and two ASGs. Option A is the answer as there is no need to touch Route53 in this scenario(it's pointing to the single ALB already).

---


<br/>

## Question 121

*Date: May 9, 2023, 9:52 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company is building a new pipeline by using AWS CodePipeline and AWS CodeBuild in a build account. The pipeline consists of two stages. The first stage is a CodeBuild job to build and package an AWS Lambda function. The second stage consists of deployment actions that operate on two different AWS accounts: a development environment account and a production environment account. The deployment stages use the AWS CloudFormation action that CodePipeline invokes to deploy the infrastructure that the Lambda function requires.

A DevOps engineer creates the CodePipeline pipeline and configures the pipeline to encrypt build artifacts by using the AWS Key Management Service (AWS KMS) AWS managed key for Amazon S3 (the aws/s3 key). The artifacts are stored in an S3 bucket. When the pipeline runs, the CloudFormation actions fail with an access denied error.

Which combination of actions must the DevOps engineer perform to resolve this error? (Choose two.)

**Options:**
- A. Create an S3 bucket in each AWS account for the artifacts. Allow the pipeline to write to the S3 buckets. Create a CodePipeline S3 action to copy the artifacts to the S3 bucket in each AWS account. Update the CloudFormation actions to reference the artifacts S3 bucket in the production account.
- B. Create a customer managed KMS key. Configure the KMS key policy to allow the IAM roles used by the CloudFormation action to perform decrypt operations. Modify the pipeline to use the customer managed KMS key to encrypt artifacts.
- C. Create an AWS managed KMS key. Configure the KMS key policy to allow the development account and the production account to perform decrypt operations. Modify the pipeline to use the KMS key to encrypt artifacts.
- D. In the development account and in the production account, create an IAM role for CodePipeline. Configure the roles with permissions to perform CloudFormation operations and with permissions to retrieve and decrypt objects from the artifacts S3 bucket. In the CodePipeline account, configure the CodePipeline CloudFormation action to use the roles.
- E. In the development account and in the production account, create an IAM role for CodePipeline. Configure the roles with permissions to perform CloudFormation operations and with permissions to retrieve and decrypt objects from the artifacts S3 bucket. In the CodePipeline account, modify the artifacts S3 bucket policy to allow the roles access. Configure the CodePipeline CloudFormation action to use the roles.

> **Suggested Answer:** BE
> **Community Vote:** BE (80%), 13%, 7%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**lunt** (Sat 27 May 2023 20:29) - *Upvotes: 14*
C = AWS KMS fundamentals. Cannot modify AWS managed KMS key policies. No Cross account access = will not work. Not sure why there is even a discussion on this. Associate level basics.

---

**svjl** (Thu 07 Dec 2023 15:28) - *Upvotes: 2*
You van modify the key policies, it is a managed key. What is wrong is change it to use for different account.
https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-modifying.html

---

**robertohyena** (Sun 10 Dec 2023 11:27) - *Upvotes: 2*
From your link: https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-modifying.html

When changing a key policy, keep in mind the following rules:
- You can view the key policy for an AWS managed key or a customer managed key, but you can only change the key policy for a customer managed key.
- The policies of AWS managed keys are created and managed by the AWS service that created the KMS key in your account.
- You cannot view or change the key policy for an AWS owned key.

---

**heff_bezos** (Mon 23 Sep 2024 08:21) - *Upvotes: 1*
From your link:
"You can add or remove IAM users, IAM roles, and AWS accounts in the key policy, and change the actions that are allowed or denied for those principals."
The answer is BE because you don't want to grant permissions to the KMS key for an ENTIRE account, you'd want to allow access for a particular role.

---

**youonebe** (Sat 21 Dec 2024 16:14) - *Upvotes: 1*
there is no need to modify the artifacts S3 bucket policy to allow the roles access

---

**jamesf** (Mon 29 Jul 2024 07:59) - *Upvotes: 2*
B - Cannot modify AWS managed KMS key policies.
E - Cross account access and we need bucket policies also to be updated, if its same account then we do not need bucket policies permissions

---

**xdkonorek2** (Wed 19 Jun 2024 18:50) - *Upvotes: 3*
BD,
try it yourself, create account with a bucket, create role with access to s3 operations, and trust policy for another account.
role assumed by another account has full access to s3 resources thereby it's not needed to set up resource policy on s3 bucket

---

**Venki_dev** (Sun 09 Jun 2024 07:04) - *Upvotes: 1*
Answer is BD ,
I have recently implemented similar solution, and my S3 bucket do not have any policy configured , my IAM role has required KMS key permission and it worked.

modifying the S3 bucket policy, but this is not necessary if the IAM roles are correctly configured and used by the CodePipeline CloudFormation action

---

**Venki_dev** (Wed 19 Jun 2024 03:23) - *Upvotes: 3*
I switch to BE ,

because its cross account access and we need bucket policies also to be updated, if its same account then we do not need bucket policies permissions

---

**c3518fc** (Mon 22 Apr 2024 01:48) - *Upvotes: 1*
Nobody is saying why we are modifying the artifacts in S3 in Option E in the Codecommit account. Doesn't seem to make sense to me.

---


<br/>

## Question 122

*Date: May 9, 2023, 9:41 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company is using an organization in AWS Organizations to manage multiple AWS accounts. The company’s development team wants to use AWS Lambda functions to meet resiliency requirements and is rewriting all applications to work with Lambda functions that are deployed in a VPC. The development team is using Amazon Elastic File System (Amazon EFS) as shared storage in Account A in the organization.

The company wants to continue to use Amazon EFS with Lambda. Company policy requires all serverless projects to be deployed in Account B.

A DevOps engineer needs to reconfigure an existing EFS file system to allow Lambda functions to access the data through an existing EFS access point.

Which combination of steps should the DevOps engineer take to meet these requirements? (Choose three.)

**Options:**
- A. Update the EFS file system policy to provide Account B with access to mount and write to the EFS file system in Account A.
- B. Create SCPs to set permission guardrails with fine-grained control for Amazon EFS.
- C. Create a new EFS file system in Account B. Use AWS Database Migration Service (AWS DMS) to keep data from Account A and Account B synchronized.
- D. Update the Lambda execution roles with permission to access the VPC and the EFS file system.
- E. Create a VPC peering connection to connect Account A to Account B.
- F. Configure the Lambda functions in Account B to assume an existing IAM role in Account A.

> **Suggested Answer:** ADE
> **Community Vote:** ADE (75%), AEF (19%), 6%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**OrganizedChaos25** (Wed 17 May 2023 14:52) - *Upvotes: 13*
I got ADE

---

**learnwithaniket** (Fri 17 Nov 2023 01:54) - *Upvotes: 7*
Initially, I thought of A,E,F. But after reading the docs I came to conclusion A,D,E is correct answer.
E: https://docs.aws.amazon.com/lambda/latest/dg/configuration-filesystem.html#configuration-filesystem-cross-account
A,D: https://docs.aws.amazon.com/lambda/latest/dg/configuration-filesystem.html#configuration-filesystem-permissions

---

**jamesf** (Mon 29 Jul 2024 08:05) - *Upvotes: 2*
Should be ADE
VPC peering required.

---

**dkp** (Sun 14 Apr 2024 10:26) - *Upvotes: 3*
A,D,E is correct

---

**DanShone** (Sat 16 Mar 2024 17:23) - *Upvotes: 3*
A,D,E is correct

---

**kyuhuck** (Wed 21 Feb 2024 02:21) - *Upvotes: 1*
1.need to update the file system plocy on efs to allow mounting the file system into account b
2.need vpc peering between account account a and account b as the pre-requisite
3.need to assume cross-account iam role to descibe the mounts so that a specific mount can be chosen

---

**thanhnv142** (Mon 05 Feb 2024 10:59) - *Upvotes: 5*
ADE are correct: <The company wants to continue to use Amazon EFS with Lambda. Company policy requires all serverless projects to be deployed in Account B.> means we need assign relevant IAM policies to lambda in account b
B: no mention of policy
C: no mention of policy
F: <assume an existing IAM role in Account A>: What role?

---

**a54b16f** (Mon 15 Jan 2024 19:52) - *Upvotes: 5*
NOT F: account B will mount EFS and would read/write as a local folder. There is no way/no need to assume role. Option D would assign permission that allow account B to read/write the EFS.

---

**zain1258** (Wed 15 Nov 2023 21:34) - *Upvotes: 3*
It's ADE.

---

**hzhang** (Fri 10 Nov 2023 13:22) - *Upvotes: 2*
D only works if both lamda function and EFS are in the same account.

---


<br/>

## Question 123

*Date: May 16, 2023, 2:51 a.m.
Disclaimers:
- ExamTopics website is not rel*

A media company has several thousand Amazon EC2 instances in an AWS account. The company is using Slack and a shared email inbox for team communications and important updates. A DevOps engineer needs to send all AWS-scheduled EC2 maintenance notifications to the Slack channel and the shared inbox. The solution must include the instances’ Name and Owner tags.

Which solution will meet these requirements?

**Options:**
- A. Integrate AWS Trusted Advisor with AWS Config. Configure a custom AWS Config rule to invoke an AWS Lambda function to publish notifications to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe a Slack channel endpoint and the shared inbox to the topic.
- B. Use Amazon EventBridge to monitor for AWS Health events. Configure the maintenance events to target an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe an AWS Lambda function to the SNS topic to send notifications to the Slack channel and the shared inbox.
- C. Create an AWS Lambda function that sends EC2 maintenance notifications to the Slack channel and the shared inbox. Monitor EC2 health events by using Amazon CloudWatch metrics. Configure a CloudWatch alarm that invokes the Lambda function when a maintenance notification is received.
- D. Configure AWS Support integration with AWS CloudTrail. Create a CloudTrail lookup event to invoke an AWS Lambda function to pass EC2 maintenance notifications to Amazon Simple Notification Service (Amazon SNS). Configure Amazon SNS to target the Slack channel and the shared inbox.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**dkp** (Mon 14 Oct 2024 10:29) - *Upvotes: 2*
B is the answer

---

**thanhnv142** (Mon 05 Aug 2024 10:10) - *Upvotes: 2*
B is correct: <eeds to send all AWS-scheduled EC2 maintenance notifications to the Slack channel and the shared inbox> means SNS
C: no mention of SNS
A: AWS trusted advisor has nothing to do here
D: AWS Support is support plan. It has nothing to do here. so as AWS cloud trail

---

**yuliaqwerty** (Wed 10 Jul 2024 19:19) - *Upvotes: 1*
Answer is B

---

**sarlos** (Thu 04 Jul 2024 01:30) - *Upvotes: 1*
Yes it's B

---

**n_d1** (Sun 21 Jan 2024 13:48) - *Upvotes: 4*
https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html

---

**Certified101** (Sat 20 Jan 2024 14:00) - *Upvotes: 2*
B is correct

---

**OrganizedChaos25** (Fri 17 Nov 2023 15:53) - *Upvotes: 2*
B is the answer I got

---

**devnv** (Thu 16 Nov 2023 03:51) - *Upvotes: 1*
B is correct

---


<br/>

## Question 124

*Date: May 16, 2023, 2:58 a.m.
Disclaimers:
- ExamTopics website is not rel*

An AWS CodePipeline pipeline has implemented a code release process. The pipeline is integrated with AWS CodeDeploy to deploy versions of an application to multiple Amazon EC2 instances for each CodePipeline stage.

During a recent deployment, the pipeline failed due to a CodeDeploy issue. The DevOps team wants to improve monitoring and notifications during deployment to decrease resolution times.

What should the DevOps engineer do to create notifications when issues are discovered?

**Options:**
- A. Implement Amazon CloudWatch Logs for CodePipeline and CodeDeploy, create an AWS Config rule to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues.
- B. Implement Amazon EventBridge for CodePipeline and CodeDeploy, create an AWS Lambda function to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues.
- C. Implement AWS CloudTrail to record CodePipeline and CodeDeploy API call information, create an AWS Lambda function to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues.
- D. Implement Amazon EventBridge for CodePipeline and CodeDeploy, create an Amazon Inspector assessment target to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Mon 05 Aug 2024 10:17) - *Upvotes: 5*
B is correct: <monitoring and notifications during deployment> means eventbridge and SNS
A: cloudwatchlog has nothing to do here. This is use for continuous monitoring of AWS services
C: cloudtrail is for account activities monitoring
D: Inspector is for threat detection

---

**dkp** (Mon 14 Oct 2024 10:32) - *Upvotes: 2*
B is correc

---

**YR4591** (Sun 28 Apr 2024 11:46) - *Upvotes: 2*
Its B, They want to monitor issued DURING deployment, means near real time, so cloudwatch event will do the work.

C is wrong for to reasons, first, cloudtrail alone can't trigger lambda without an event. Second, cloud trail logs are update in 5 minutes intervals, which means monitoring for the code deploy will not be during deployment.

---

**haazybanj** (Sat 30 Dec 2023 03:22) - *Upvotes: 3*
B. Implement Amazon EventBridge for CodePipeline and CodeDeploy, create an AWS Lambda function to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues.

Explanation:
Amazon EventBridge provides a serverless event bus that integrates with various AWS services. By implementing EventBridge for CodePipeline and CodeDeploy, the engineer can capture deployment events and trigger actions based on those events. Creating an AWS Lambda function allows for evaluating code deployment issues and performing custom actions. Additionally, creating an Amazon SNS topic provides a means to notify stakeholders of any deployment issues detected.

---

**OrganizedChaos25** (Fri 17 Nov 2023 15:55) - *Upvotes: 3*
B is correct

---

**devnv** (Thu 16 Nov 2023 03:58) - *Upvotes: 1*
Yes it's B

---


<br/>

## Question 125

*Date: May 13, 2023, 11:33 p.m.
Disclaimers:
- ExamTopics website is not rel*

A global company manages multiple AWS accounts by using AWS Control Tower. The company hosts internal applications and public applications.

Each application team in the company has its own AWS account for application hosting. The accounts are consolidated in an organization in AWS Organizations. One of the AWS Control Tower member accounts serves as a centralized DevOps account with CI/CD pipelines that application teams use to deploy applications to their respective target AWS accounts. An IAM role for deployment exists in the centralized DevOps account.

An application team is attempting to deploy its application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster in an application AWS account. An IAM role for deployment exists in the application AWS account. The deployment is through an AWS CodeBuild project that is set up in the centralized DevOps account. The CodeBuild project uses an IAM service role for CodeBuild. The deployment is failing with an Unauthorized error during attempts to connect to the cross-account EKS cluster from CodeBuild.

Which solution will resolve this error?

**Options:**
- A. Configure the application account’s deployment IAM role to have a trust relationship with the centralized DevOps account. Configure the trust relationship to allow the sts:AssumeRole action. Configure the application account’s deployment IAM role to have the required access to the EKS cluster. Configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions.
- B. Configure the centralized DevOps account’s deployment IAM role to have a trust relationship with the application account. Configure the trust relationship to allow the sts:AssumeRole action. Configure the centralized DevOps account’s deployment IAM role to allow the required access to CodeBuild.
- C. Configure the centralized DevOps account’s deployment IAM role to have a trust relationship with the application account. Configure the trust relationship to allow the sts:AssumeRoleWithSAML action. Configure the centralized DevOps account’s deployment IAM role to allow the required access to CodeBuild.
- D. Configure the application account’s deployment IAM role to have a trust relationship with the AWS Control Tower management account. Configure the trust relationship to allow the sts:AssumeRole action. Configure the application account’s deployment IAM role to have the required access to the EKS cluster. Configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Certified101** (Thu 13 Jul 2023 09:54) - *Upvotes: 10*
A. Configure the application account’s deployment IAM role to have a trust relationship with the centralized DevOps account. Configure the trust relationship to allow the sts:AssumeRole action. Configure the application account’s deployment IAM role to have the required access to the EKS cluster. Configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions.

Options B, C, and D are not correct because the centralized DevOps account’s deployment IAM role doesn't need to trust the application account, it's the other way around. The sts:AssumeRoleWithSAML action in option C is used for federation from a SAML 2.0 compliant identity provider and is not necessary in this scenario. Lastly, there's no need to have a trust relationship with the AWS Control Tower management account as in option D, as the interaction is directly between the DevOps account and the application account.

---

**thanhnv142** (Mon 05 Feb 2024 13:35) - *Upvotes: 6*
A is correct: <Unauthorized error during attempts to connect> means we need to setup relevant permissions and policies
- A is correct because < AWS CodeBuild project that is set up in the centralized DevOps account>, so we should setup trust relationship on the account that has resources, which is the application account and allow codebuild from centralized account assume it
B and C are wrong: we need to setup trust from the app account, not the centralized account.
D: this option mentions control Tower, which is irrelevant

---

**jamesf** (Fri 02 Aug 2024 09:01) - *Upvotes: 2*
A. Configure the application account’s deployment IAM role to have a trust relationship with the centralized DevOps account.
- setup trust relationship on the account that has resources, which is the application account

Configure the trust relationship to allow the sts:AssumeRole action.
- allow CodeBuild from centralized account assume it
- CodeBuild is configured in Centralized DevOps account but not in application account.

Configure the application account’s deployment IAM role to have the required access to the EKS cluster. Configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions.
- the application account has access to the resources

---

**tartarus23** (Tue 20 Jun 2023 21:45) - *Upvotes: 3*
(A) This solution addresses the Unauthorized error by allowing the DevOps account to assume the IAM role in the application account that has the necessary permissions to access the EKS cluster. The other options don't provide the necessary cross-account permissions or correctly configure the roles for accessing EKS.

---

**walkwolf3** (Thu 08 Jun 2023 14:37) - *Upvotes: 2*
B is correct.
Unauthorized error happened from CodeBuild in Dev account to EKS cluster in application account, instead of reverse direction.

---

**zain1258** (Wed 15 Nov 2023 21:14) - *Upvotes: 2*
CodeBuild is configured in Centralized DevOps account not in application account.

---

**2pk** (Tue 23 May 2023 23:16) - *Upvotes: 3*
I'd like to add more, don't get the source and destination mixed up. Because the Application team must deploy resources in the Dev account. So, the source should be the Application team and the destination should be the Dev team.

---

**PhuocT** (Tue 23 May 2023 15:05) - *Upvotes: 1*
A is correct

---

**ParagSanyashiv** (Sun 14 May 2023 10:57) - *Upvotes: 1*
A is correct Answer

---

**2pk** (Sat 13 May 2023 23:33) - *Upvotes: 4*
Answer is A.
In the source AWS account, the IAM role used by the CI/CD pipeline should have permissions to access the source code repository, build artifacts, and any other resources required for the build process.
In the destination AWS accounts, the IAM role used for deployment should have permissions to access the AWS resources required for deploying the application, such as EC2 instances, RDS databases, S3 buckets, etc. The exact permissions required will depend on the specific resources being used by the application.
the IAM role used for deployment in the destination accounts should also have permissions to assume the IAM role for deployment in the centralized DevOps account. This is typically done using an IAM role trust policy that allows the destination account to assume the DevOps account role.

---


<br/>

## Question 126

*Date: May 13, 2023, 11:58 p.m.
Disclaimers:
- ExamTopics website is not rel*

A highly regulated company has a policy that DevOps engineers should not log in to their Amazon EC2 instances except in emergencies. If a DevOps engineer does log in, the security team must be notified within 15 minutes of the occurrence.

Which solution will meet these requirements?

**Options:**
- A. Install the Amazon Inspector agent on each EC2 instance. Subscribe to Amazon EventBridge notifications. Invoke an AWS Lambda function to check if a message is about user logins. If it is, send a notification to the security team using Amazon SNS.
- B. Install the Amazon CloudWatch agent on each EC2 instance. Configure the agent to push all logs to Amazon CloudWatch Logs and set up a CloudWatch metric filter that searches for user logins. If a login is found, send a notification to the security team using Amazon SNS.
- C. Set up AWS CloudTrail with Amazon CloudWatch Logs. Subscribe CloudWatch Logs to Amazon Kinesis. Attach AWS Lambda to Kinesis to parse and determine if a log contains a user login. If it does, send a notification to the security team using Amazon SNS.
- D. Set up a script on each Amazon EC2 instance to push all logs to Amazon S3. Set up an S3 event to invoke an AWS Lambda function, which invokes an Amazon Athena query to run. The Athena query checks for logins and sends the output to the security team using Amazon SNS.

> **Suggested Answer:** B
> **Community Vote:** B (95%), 5%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Mon 05 Feb 2024 13:39) - *Upvotes: 6*
B is correct: <the security team must be notified > means SNS
A: irrelevant, inspector is for vulnerability scanning
C: cloud trail is for monitoring account activities
D: This options uses manual script, which is irrelevant

---

**zijo** (Thu 27 Jun 2024 19:14) - *Upvotes: 1*
B is the cheapest and correct solution
CloudTrail does not capture:
SSH logins to Linux instances.
RDP logins to Windows instances.
Commands executed on the instances.
Local file access or modifications within the instances.

---

**haazybanj** (Sat 22 Jul 2023 11:43) - *Upvotes: 1*
While Option B can provide valuable insights into user logins and send notifications to the security team, it might not guarantee that the security team is notified within 15 minutes of a login occurrence. The time it takes for the CloudWatch metric filter to process and detect the login event, along with the potential delays in the SNS notification, could result in notifications being sent beyond the required 15-minute timeframe.

On the other hand, Option C, which uses AWS CloudTrail with Amazon CloudWatch Logs and Amazon Kinesis, allows real-time processing and immediate notifications when a user login event is detected. This makes Option C more suitable for meeting the specific requirement of notifying the security team within 15 minutes of a login occurrence.

---

**RVivek** (Thu 14 Sep 2023 16:17) - *Upvotes: 10*
Cloud Trail will track calls to AWS API, but not the OS login in an EC2. That can be checked only using Cloud watch logs

---

**n_d1** (Fri 21 Jul 2023 15:03) - *Upvotes: 3*
https://aws.amazon.com/blogs/security/how-to-monitor-and-visualize-failed-ssh-access-attempts-to-amazon-ec2-linux-instances/

---

**ProfXsamson** (Tue 11 Jul 2023 03:47) - *Upvotes: 4*
B,
Eventhough it's not stated in some questions, the cheapest solution to a problem is always AWS favorite.

---

**gdtypk** (Tue 30 May 2023 23:17) - *Upvotes: 2*
Isn't it possible to get login events with CloudTrail?

---

**Mail1964** (Tue 23 May 2023 11:12) - *Upvotes: 3*
Subtle difference Cloudtrail is "near" realtime - You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, an Amazon Kinesis Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems.

---

**devnv** (Tue 16 May 2023 03:07) - *Upvotes: 1*
B is the right answer

---

**2pk** (Sat 13 May 2023 23:58) - *Upvotes: 1*
i think its C, Both B&C solutions are valid and can meet the requirement of notifying the security team within 15 minutes of a DevOps engineer logging into an EC2 instance.

However, there are some differences in how quickly each solution can detect and notify the security team of a login event.

The CloudTrail-based solution can detect a login event more quickly than the CloudWatch-based solution because CloudTrail captures API events in near-real-time, while CloudWatch logs may have a delay of a few minutes before they appear in the log group. Therefore, the CloudTrail-based solution is more likely to meet the 15-minute notification requirement.

---


<br/>

## Question 127

*Date: May 14, 2023, 12:08 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company updated the AWS CloudFormation template for a critical business application. The stack update process failed due to an error in the updated template, and AWS CloudFormation automatically began the stack rollback process. Later, a DevOps engineer discovered that the application was still unavailable and that the stack was in the UPDATE_ROLLBACK_FAILED state.

Which combination of actions should the DevOps engineer perform so that the stack rollback can complete successfully? (Choose two.)

**Options:**
- A. Attach the AWSCloudFormationFullAccess IAM policy to the AWS CloudFormation role.
- B. Automatically recover the stack resources by using AWS CloudFormation drift detection.
- C. Issue a ContinueUpdateRollback command from the AWS CloudFormation console or the AWS CLI.
- D. Manually adjust the resources to match the expectations of the stack.
- E. Update the existing AWS CloudFormation stack by using the original template.

> **Suggested Answer:** CD
> **Community Vote:** CD (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**2pk** (Sun 14 May 2023 00:08) - *Upvotes: 10*
yes C & D
C. Issue a ContinueUpdateRollback command from the AWS CloudFormation console or the AWS CLI: This command allows CloudFormation to continue the rollback process from the point where it was paused. By using this command, CloudFormation will attempt to restore the resources to their previous state and delete any resources that were created during the update.

D. Manually adjust the resources to match the expectations of the stack: This involves identifying and correcting the root cause of the update failure, which could involve changing the resource configuration or resolving any dependencies or inconsistencies in the stack.

---

**heff_bezos** (Mon 23 Sep 2024 09:50) - *Upvotes: 2*
D.
"In most cases, you must fix the error that causes the update rollback to fail before you can continue to roll back your stack. In other cases, you can continue to roll back the update without any changes, for example when a stack operation times out."

https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-continueupdaterollback.html

---

**thanhnv142** (Mon 05 Feb 2024 13:44) - *Upvotes: 3*
C and D are correct: < UPDATE_ROLLBACK_FAILED state> means we are left with ContinueUpdateRollback command
A: irrelevant, AWSCloudFormationFullAccess IAM policy is used to access ACF, not to fix this
B: AWS CloudFormation drift detection is to check if the stack has been updated unexpectedly, not to fix irrelevant
E: The original template didnt work, so update the stack using it wont work

---

**yuliaqwerty** (Wed 10 Jan 2024 20:31) - *Upvotes: 2*
Agree with C and D

---

**sarlos** (Thu 04 Jan 2024 02:43) - *Upvotes: 1*
C and D are right

---

**Certified101** (Thu 20 Jul 2023 13:37) - *Upvotes: 3*
Yes it's C & D

---

**tartarus23** (Tue 20 Jun 2023 21:42) - *Upvotes: 4*
(C) This command will try to roll back the stack to the previously working state after you have resolved the issues that caused the rollback failure.

(D) Sometimes a stack update can fail because the current state of a resource differs from the state expected by AWS CloudFormation (e.g., a resource that AWS CloudFormation is trying to modify or delete is locked by another process). Manually resolving these issues, by stopping the conflicting process or by modifying the resource to match the expected state, will allow the stack update or rollback to proceed.

---

**devnv** (Tue 16 May 2023 03:12) - *Upvotes: 2*
Yes it's C & D

---


<br/>

## Question 128

*Date: May 16, 2023, 3:16 a.m.
Disclaimers:
- ExamTopics website is not rel*

A development team manually builds an artifact locally and then places it in an Amazon S3 bucket. The application has a local cache that must be cleared when a deployment occurs. The team runs a command to do this, downloads the artifact from Amazon S3, and unzips the artifact to complete the deployment.

A DevOps team wants to migrate to a CI/CD process and build in checks to stop and roll back the deployment when a failure occurs. This requires the team to track the progression of the deployment.

Which combination of actions will accomplish this? (Choose three.)

**Options:**
- A. Allow developers to check the code into a code repository. Using Amazon EventBridge, on every pull into the main branch, invoke an AWS Lambda function to build the artifact and store it in Amazon S3.
- B. Create a custom script to clear the cache. Specify the script in the BeforeInstall lifecycle hook in the AppSpec file.
- C. Create user data for each Amazon EC2 instance that contains the clear cache script. Once deployed, test the application. If it is not successful, deploy it again.
- D. Set up AWS CodePipeline to deploy the application. Allow developers to check the code into a code repository as a source for the pipeline.
- E. Use AWS CodeBuild to build the artifact and place it in Amazon S3. Use AWS CodeDeploy to deploy the artifact to Amazon EC2 instances.
- F. Use AWS Systems Manager to fetch the artifact from Amazon S3 and deploy it to all the instances.

> **Suggested Answer:** BDE
> **Community Vote:** BDE (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**tartarus23** (Wed 20 Dec 2023 22:41) - *Upvotes: 8*
(B) This would help ensure that the local cache is cleared before the new version of the application is installed. AppSpec (Application Specification) file is a unique file to AWS CodeDeploy. It defines the deployment actions you want AWS CodeDeploy to execute.

(D) This would allow you to automate the build and deployment processes. AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.

(E) This would allow you to automate the build process and ensure that the application is built in a consistent environment. AWS CodeBuild is a fully managed build service that compiles source code, runs tests, and produces software packages that are ready to deploy. AWS CodeDeploy automates software deployments to a variety of compute services including Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers.

---

**Certified101** (Sat 13 Jan 2024 11:09) - *Upvotes: 5*
BDE combination will do all requirments

---

**thanhnv142** (Tue 06 Aug 2024 09:20) - *Upvotes: 3*
BDE are correct: < migrate to a CI/CD process > means codepipeline, code build and code deploy
A,C and F: no mention of the above three

---

**sarlos** (Thu 04 Jul 2024 01:47) - *Upvotes: 1*
BDE is rigtht

---

**devnv** (Thu 16 Nov 2023 04:16) - *Upvotes: 2*
Yes it's BD&E

---


<br/>

## Question 129

*Date: May 13, 2023, 11:20 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer is working on a project that is hosted on Amazon Linux and has failed a security review. The DevOps manager has been asked to review the company buildspec.yaml file for an AWS CodeBuild project and provide recommendations. The buildspec.yaml file is configured as follows:



What changes should be recommended to comply with AWS security best practices? (Choose three.)

**Options:**
- A. Add a post-build command to remove the temporary files from the container before termination to ensure they cannot be seen by other CodeBuild users.
- B. Update the CodeBuild project role with the necessary permissions and then remove the AWS credentials from the environment variable.
- C. Store the DB_PASSWORD as a SecureString value in AWS Systems Manager Parameter Store and then remove the DB_PASSWORD from the environment variables.
- D. Move the environment variables to the ‘db-deploy-bucket’ Amazon S3 bucket, add a prebuild stage to download, then export the variables.
- E. Use AWS Systems Manager run command versus scp and ssh commands directly to the instance.
- F. Scramble the environment variables using XOR followed by Base64, add a section to install, and then run XOR and Base64 to the build phase.

> **Suggested Answer:** BCE
> **Community Vote:** BCE (82%), ABC (18%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**WhyIronMan** (Sun 31 Mar 2024 16:02) - *Upvotes: 11*
BCE is correct
A is WRONG. CodeBuild does not keep files for next builds in that way, once the build is done, the files will be deleted.

---

**sb333** (Sun 23 Jul 2023 21:42) - *Upvotes: 5*
BCE are the correct answers.

---

**heff_bezos** (Mon 23 Sep 2024 10:03) - *Upvotes: 2*
Code Build is a managed service. There's no way for other users to see what's in the container.

---

**jamesf** (Fri 02 Aug 2024 09:18) - *Upvotes: 1*
Prefer BCE

Option A incorrect as
- CodeBuild does not keep files for next builds in that way, once the build is done, the files will be deleted.
- and don't think have such "CodeBuild users"

---

**ericphl** (Mon 22 Jul 2024 10:50) - *Upvotes: 1*
ABC seems right.

---

**ajeeshb** (Sat 06 Jul 2024 22:13) - *Upvotes: 1*
A - Cleans up temp files that stores the my.cnf and the instance key files
B - Removes hardcoded AWS credentials
C - Securely stores DB password

---

**Diego1414** (Thu 22 Feb 2024 21:34) - *Upvotes: 2*
ABC seems appropriate, since the emphasis is on security.

---

**thanhnv142** (Tue 06 Feb 2024 16:09) - *Upvotes: 2*
ABC are correct: security best practices are related to removing credentials and sensitive data
- A remove temporary files is important because they might contain sensitive data
- B: <remove the AWS credentials> is removing the access key
- C: <remove the DB_PASSWORD> means removing hardcoded DB_PASSWORD
All other options dont relate to senstive data or password

---

**sarlos** (Thu 04 Jan 2024 02:52) - *Upvotes: 1*
its BCE
https://stackoverflow.com/questions/76854227/i-want-to-copy-files-to-aws-ec2-using-buildspec-yml-file-the-22-port-is-open-fo

---

**zain1258** (Wed 15 Nov 2023 21:02) - *Upvotes: 4*
It's BCE.
A is wrong. I don't think there is any concept of `CodeBuild users`.

---


<br/>

## Question 130

*Date: June 20, 2023, 9:12 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has a legacy application. A DevOps engineer needs to automate the process of building the deployable artifact for the legacy application. The solution must store the deployable artifact in an existing Amazon S3 bucket for future deployments to reference.

Which solution will meet these requirements in the MOST operationally efficient way?

**Options:**
- A. Create a custom Docker image that contains all the dependencies for the legacy application. Store the custom Docker image in a new Amazon Elastic Container Registry (Amazon ECR) repository. Configure a new AWS CodeBuild project to use the custom Docker image to build the deployable artifact and to save the artifact to the S3 bucket.
- B. Launch a new Amazon EC2 instance. Install all the dependencies for the legacy application on the EC2 instance. Use the EC2 instance to build the deployable artifact and to save the artifact to the S3 bucket.
- C. Create a custom EC2 Image Builder image. Install all the dependencies for the legacy application on the image. Launch a new Amazon EC2 instance from the image. Use the new EC2 instance to build the deployable artifact and to save the artifact to the S3 bucket.
- D. Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with an AWS Fargate profile that runs in multiple Availability Zones. Create a custom Docker image that contains all the dependencies for the legacy application. Store the custom Docker image in a new Amazon Elastic Container Registry (Amazon ECR) repository. Use the custom Docker image inside the EKS cluster to build the deployable artifact and to save the artifact to the S3 bucket.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**haazybanj** (Thu 27 Jul 2023 18:44) - *Upvotes: 8*
The most operationally efficient solution for automating the process of building the deployable artifact for the legacy application and storing it in an existing Amazon S3 bucket is:

A. This solution leverages containerization with Docker, which allows for consistent and isolated builds, making it easier to manage application dependencies. The use of AWS CodeBuild allows for scalable and automated builds using the custom Docker image, making the process efficient and reliable. The deployable artifact can then be saved to the existing S3 bucket for future reference and deployments.

---

**thanhnv142** (Tue 06 Feb 2024 16:13) - *Upvotes: 6*
A is correct: <needs to automate the process of building the deployable artifact for the legacy application> means codebuild
BCD dont mention codebuild, only A mentions

---

**jamesf** (Fri 02 Aug 2024 09:21) - *Upvotes: 1*
keywords: CodeBuild for Reusable artifacts

---

**habros** (Sun 09 Jul 2023 16:52) - *Upvotes: 3*
Reusable artifacts = A.

---

**FunkyFresco** (Mon 26 Jun 2023 13:01) - *Upvotes: 2*
Option A makes more sense to me.

---

**tartarus23** (Tue 20 Jun 2023 21:12) - *Upvotes: 3*
(A) This approach is the most operationally efficient because it leverages the benefits of containerization, such as isolation and reproducibility, as well as AWS managed services. AWS CodeBuild is a fully managed build service that can compile your source code, run tests, and produce deployable software packages. By using a custom Docker image that includes all dependencies, you can ensure that the environment in which your code is built is consistent. Using Amazon ECR to store Docker images lets you easily deploy the images to any environment. Also, you can directly upload the build artifacts to Amazon S3 from AWS CodeBuild, which is beneficial for version control and archival purposes.

---


<br/>

## Question 131

*Date: June 20, 2023, 9:05 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company builds a container image in an AWS CodeBuild project by running Docker commands. After the container image is built, the CodeBuild project uploads the container image to an Amazon S3 bucket. The CodeBuild project has an IAM service role that has permissions to access the S3 bucket.

A DevOps engineer needs to replace the S3 bucket with an Amazon Elastic Container Registry (Amazon ECR) repository to store the container images. The DevOps engineer creates an ECR private image repository in the same AWS Region of the CodeBuild project. The DevOps engineer adjusts the IAM service role with the permissions that are necessary to work with the new ECR repository. The DevOps engineer also places new repository information into the docker build command and the docker push command that are used in the buildspec.yml file.

When the CodeBuild project runs a build job, the job fails when the job tries to access the ECR repository.

Which solution will resolve the issue of failed access to the ECR repository?

**Options:**
- A. Update the buildspec.yml file to log in to the ECR repository by using the aws ecr get-login-password AWS CLI command to obtain an authentication token. Update the docker login command to use the authentication token to access the ECR repository.
- B. Add an environment variable of type SECRETS_MANAGER to the CodeBuild project. In the environment variable, include the ARN of the CodeBuild project's IAM service role. Update the buildspec.yml file to use the new environment variable to log in with the docker login command to access the ECR repository.
- C. Update the ECR repository to be a public image repository. Add an ECR repository policy that allows the IAM service role to have access.
- D. Update the buildspec.yml file to use the AWS CLI to assume the IAM service role for ECR operations. Add an ECR repository policy that allows the IAM service role to have access.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**tartarus23** (Wed 20 Dec 2023 22:05) - *Upvotes: 8*
(A) When Docker communicates with an Amazon Elastic Container Registry (ECR) repository, it requires authentication. You can authenticate your Docker client to the Amazon ECR registry with the help of the AWS CLI (Command Line Interface). Specifically, you can use the "aws ecr get-login-password" command to get an authorization token and then use Docker's "docker login" command with that token to authenticate to the registry. You would need to perform these steps in your buildspec.yml file before attempting to push or pull images from/to the ECR repository.

---

**haazybanj** (Sat 27 Jan 2024 19:46) - *Upvotes: 5*
A:
When using Amazon ECR, you need to authenticate Docker to the ECR registry before pushing or pulling container images. The authentication token can be obtained using the aws ecr get-login-password AWS CLI command. The obtained token needs to be used with the docker login command to authenticate Docker to the ECR repository.

By following this approach, the CodeBuild project will have the necessary credentials to access the ECR repository, and the build job will be able to push the container image to the ECR repository successfully.

---

**thanhnv142** (Tue 06 Aug 2024 15:24) - *Upvotes: 4*
A is correct: <the job fails when the job tries to access the ECR repository.> This means there is problem when accessing the repo. <adjusts the IAM service role with the permissions that are necessary to work with the new ECR repository> means have got sufficient permission. Need token to access with aws ecr get-login-password command
BCD no mention of ecr get-login-password

---

**ixdb** (Sat 17 Feb 2024 04:29) - *Upvotes: 3*
A is right.

---

**CirusD** (Sat 27 Jan 2024 00:16) - *Upvotes: 3*
A..version: 0.2

phases:
pre_build:
commands:
- $(aws ecr get-login --no-include-email --region region-name)
build:
commands:
- docker build -t repository-name .
- docker tag repository-name:latest repository-uri:latest
post_build:
commands:
- docker push repository-uri:latest

---


<br/>

## Question 132

*Date: June 20, 2023, 9 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company manually provisions IAM access for its employees. The company wants to replace the manual process with an automated process. The company has an existing Active Directory system configured with an external SAML 2.0 identity provider (IdP).

The company wants employees to use their existing corporate credentials to access AWS. The groups from the existing Active Directory system must be available for permission management in AWS Identity and Access Management (IAM). A DevOps engineer has completed the initial configuration of AWS IAM Identity Center (AWS Single Sign-On) in the company’s AWS account.

What should the DevOps engineer do next to meet the requirements?

**Options:**
- A. Configure an external IdP as an identity source. Configure automatic provisioning of users and groups by using the SCIM protocol.
- B. Configure AWS Directory Service as an identity source. Configure automatic provisioning of users and groups by using the SAML protocol.
- C. Configure an AD Connector as an identity source. Configure automatic provisioning of users and groups by using the SCIM protocol.
- D. Configure an external IdP as an identity source Configure automatic provisioning of users and groups by using the SAML protocol.

> **Suggested Answer:** A
> **Community Vote:** A (93%), 7%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**tartarus23** (Tue 20 Jun 2023 21:00) - *Upvotes: 8*
(A) AWS SSO (Single Sign-On) integrates with external identity providers using SAML 2.0, and it can automatically synchronize users and groups from a connected directory using the SCIM (System for Cross-domain Identity Management) protocol. Thus, the DevOps engineer should configure the external IdP as an identity source and then configure automatic provisioning of users and groups by using the SCIM protocol. This will ensure the groups from the existing Active Directory system are available for permission management in AWS Identity and Access Management (IAM) and that employees can use their existing corporate credentials to access AWS.

---

**jamesf** (Fri 02 Aug 2024 09:43) - *Upvotes: 6*
For Note: SAML (Security Assertion Markup Language) is primarily used for authentication and authorization
while SCIM (System for Cross-domain Identity Management) is a protocol used for automating user provisioning and deprovisioning across different systems and domains

---

**thanhnv142** (Tue 06 Feb 2024 16:33) - *Upvotes: 4*
A is correct: <The company wants employees to use their existing corporate credentials to access AWS> means we need to assign the existing IdP as an identity source
B: <Configure AWS Directory Service as an identity source> is irrelevant
C: < Configure an AD Connector as an identity source>: AD connector is use for connecting AWS active directory with that of on-prem. This question requires AWS identity Center
D: <provisioning of users and groups by using the SAML protocol.>: SAML is an authenticate protocol. SCIM is the protocol for Idp connection

---

**zolthar_z** (Thu 30 Nov 2023 14:52) - *Upvotes: 2*
A: Explanation: What is the difference between SCIM and SSO? SSO (single-sign on) is a way to authenticate (sign in), and SCIM is a way to provision (create an account).

---

**XP_2600** (Tue 15 Aug 2023 22:15) - *Upvotes: 1*
This is quoted from aws documentationThe SAML protocol however does not provide a way to query the IdP to learn about users and groups. Therefore, you must make IAM Identity Center aware of those users and groups by provisioning them into IAM Identity Center.

https://docs.aws.amazon.com/singlesignon/latest/userguide/scim-profile-saml.html

---

**CirusD** (Wed 26 Jul 2023 23:19) - *Upvotes: 1*
Answer is A : AWS Single Sign-On (AWS SSO) can be integrated with an external SAML 2.0 identity provider (IdP). AWS SSO also supports automatic provisioning (auto-provisioning) of user and group information using the System for Cross-domain Identity Management (SCIM) protocol.

---

**sb333** (Mon 24 Jul 2023 18:00) - *Upvotes: 2*
Answer A is correct. It is SCIM that can provision users and groups in AWS. Of course the IdP needs to support SCIM (AWS has a list of IdPs that use SCIM). Answer D is not correct as SAML is an authentication protocol (cannot be used to provision users in AWS).

https://docs.aws.amazon.com/singlesignon/latest/userguide/scim-profile-saml.html
https://docs.aws.amazon.com/singlesignon/latest/userguide/supported-idps.html

---

**haazybanj** (Sat 22 Jul 2023 11:59) - *Upvotes: 1*
The AWS IAM Identity Center (AWS Single Sign-On) has been configured initially. Now, to automate the provisioning of users and groups from the external IdP into AWS IAM, the engineer should choose the SCIM protocol. SCIM is specifically designed for automatic user provisioning, making it the appropriate choice for this scenario.

Option D (Configure an external IdP as an identity source and use the SAML protocol) could work, but it does not address the requirement for automatic provisioning of users and groups. The use of SCIM (Option A) is preferred for automated user and group provisioning, as it is designed for this purpose.

---

**Snape** (Mon 17 Jul 2023 04:17) - *Upvotes: 1*
The company already has an external SAML 2.0 IdP, so the DevOps engineer should configure this IdP as an identity source in AWS Single Sign-On. Vs in option A would require to configure new identity source

---

**habros** (Sun 09 Jul 2023 17:04) - *Upvotes: 1*
A. SCIM is the automated way to provision users. You do it in AAD/AD and it propagates automatically into AWS SSO.

---


<br/>

## Question 133

*Date: June 20, 2023, 8:53 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is using AWS to run digital workloads. Each application team in the company has its own AWS account for application hosting. The accounts are consolidated in an organization in AWS Organizations.

The company wants to enforce security standards across the entire organization. To avoid noncompliance because of security misconfiguration, the company has enforced the use of AWS CloudFormation. A production support team can modify resources in the production environment by using the AWS Management Console to troubleshoot and resolve application-related issues.

A DevOps engineer must implement a solution to identify in near real time any AWS service misconfiguration that results in noncompliance. The solution must automatically remediate the issue within 15 minutes of identification. The solution also must track noncompliant resources and events in a centralized dashboard with accurate timestamps.

Which solution will meet these requirements with the LEAST development overhead?

**Options:**
- A. Use CloudFormation drift detection to identify noncompliant resources. Use drift detection events from CloudFormation to invoke an AWS Lambda function for remediation. Configure the Lambda function to publish logs to an Amazon CloudWatch Logs log group. Configure an Amazon CloudWatch dashboard to use the log group for tracking.
- B. Turn on AWS CloudTrail in the AWS accounts. Analyze CloudTrail logs by using Amazon Athena to identify noncompliant resources. Use AWS Step Functions to track query results on Athena for drift detection and to invoke an AWS Lambda function for remediation. For tracking, set up an Amazon QuickSight dashboard that uses Athena as the data source.
- C. Turn on the configuration recorder in AWS Config in all the AWS accounts to identify noncompliant resources. Enable AWS Security Hub with the --no-enable-default-standards option in all the AWS accounts. Set up AWS Config managed rules and custom rules. Set up automatic remediation by using AWS Config conformance packs. For tracking, set up a dashboard on Security Hub in a designated Security Hub administrator account.
- D. Turn on AWS CloudTrail in the AWS accounts. Analyze CloudTrail logs by using Amazon CloudWatch Logs to identify noncompliant resources. Use CloudWatch Logs filters for drift detection. Use Amazon EventBridge to invoke the Lambda function for remediation. Stream filtered CloudWatch logs to Amazon OpenSearch Service. Set up a dashboard on OpenSearch Service for tracking.

> **Suggested Answer:** C
> **Community Vote:** C (89%), 11%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**92a2133** (Thu 29 May 2025 17:38) - *Upvotes: 1*
"complaince"

automatically default to a AWS config answer

---

**92a2133** (Thu 29 May 2025 17:39) - *Upvotes: 1*
compliance* sorry just waking up

---

**heff_bezos** (Mon 23 Sep 2024 20:06) - *Upvotes: 1*
I don't think it is A because the question is asking the LEAST development overhead. Configuring Lambdas to remediate and send logs is development. It is much easier to use the built in features of AWS Config and SecurityHub

---

**heff_bezos** (Mon 23 Sep 2024 20:08) - *Upvotes: 1*
Lambda functions also have a execution limit of 15 minutes. If a remediation task were to take longer than that, it would fail.

---

**zijo** (Tue 02 Jul 2024 17:22) - *Upvotes: 3*
C is the better solution. AWS CloudFormation drift detection helps identify whether the actual configuration of your AWS resources matches their expected configuration as defined in the CloudFormation stack template. While it is a powerful tool for maintaining compliance and consistency, it alone cannot fully prevent noncompliance due to security misconfigurations. Thats where you need AWS config to continuously monitor service configurations and even use aggregator to collect all aws config data from all member accounts in aws organization to Security Hub to provide a centralized dashboard.

---

**Gomer** (Sat 22 Jun 2024 01:04) - *Upvotes: 2*
Leaning towards "A" unless someone can convince me otherwise. Why?:
I have a problem with this step in "C": "Turn on the configuration recorder in AWS Config in all the AWS accounts to identify noncompliant resources."
The fact is your not going to detect any "drift" by turning on the recorder AFTER the accounts are noncompliant.
AWS Config rules (canned or custom) and Conformance Packs can do a lot, but it's definitely duplicating settings any security settings allready defined CloudFormation stacks.
I lean towards "A" because "To avoid noncompliance because of security misconfiguration, the company has enforced the use of AWS CloudFormation".
Therefore CloudFormation stacks are is where the security settings are defined, and thereby CloudFormation is implied to be part of the detection and remediation process.
CloudFormation drift detection can be automated, and one can just "automatically remediate the issue within 15 minutes of identification" by just doing a stack refresh. Easy peasy.

---

**ajeeshb** (Sat 06 Jul 2024 22:44) - *Upvotes: 1*
Correct, A is the answer.

---

**dkp** (Sun 14 Apr 2024 11:21) - *Upvotes: 1*
answer is C with minimal overhead

---

**tristan_07** (Mon 18 Mar 2024 18:37) - *Upvotes: 1*
Both Option A and C work. However, considering Option C involves a lot 'all the AWS accounts,' it undoubtedly increases development overhead

---

**thanhnv142** (Tue 06 Feb 2024 16:49) - *Upvotes: 1*
A is correct: drift detection is the best for this scenario, which utilizes AWS cloudformation
B and D: using cloudtraid is for monitoring account activities
C: AWS Config conformance packs cannot make remediation actions. It needs to trigger AWS SSM automation document

---


<br/>

## Question 134

*Date: June 20, 2023, 8:48 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses AWS Organizations to manage its AWS accounts. The organization root has an OU that is named Environments. The Environments OU has two child OUs that are named Development and Production, respectively.

The Environments OU and the child OUs have the default FullAWSAccess policy in place. A DevOps engineer plans to remove the FullAWSAccess policy from the Development OU and replace the policy with a policy that allows all actions on Amazon EC2 resources.

What will be the outcome of this policy replacement?

**Options:**
- A. All users in the Development OU will be allowed all API actions on all resources.
- B. All users in the Development OU will be allowed all API actions on EC2 resources. All other API actions will be denied.
- C. All users in the Development OU will be denied all API actions on all resources.
- D. All users in the Development OU will be denied all API actions on EC2 resources. All other API actions will be allowed.

> **Suggested Answer:** B
> **Community Vote:** B (81%), A (19%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**d262e67** (Sun 31 Dec 2023 06:05) - *Upvotes: 15*
The key point is that "SCP inheritance works differently for Allow and Deny policies". Allowed policies are only inherited if the children don't have any Allow policy. Once they have an allow policy, only actions defined in that policy will be allowed and no "Allow" policy will be inherited from the parent(s) OUs. What inherits is the implicit Deny policy which is a hidden policy sitting above all.

Check the tables in this link:
https://aws.amazon.com/blogs/security/get-more-out-of-service-control-policies-in-a-multi-account-environment/

---

**MalonJay** (Tue 07 May 2024 23:16) - *Upvotes: 1*
Very good link about SCPs.

---

**devakram** (Thu 11 Apr 2024 11:56) - *Upvotes: 7*
I've just tested in my AWS account with the same scenario. I removed the SCP from the dev env and kept the EC2 policy, which by that I was denied access to all other operations except EC2.

---

**auxwww** (Mon 14 Oct 2024 14:27) - *Upvotes: 1*
Best explanation I found in this forum

From: learnwithaniket
"For a permission to be allowed for a specific account, there must be an explicit Allow statement at every level from the root through each OU in the direct path to the account (including the target account itself). This is why when you enable SCPs, AWS Organizations attaches an AWS managed SCP policy named FullAWSAccess which allows all services and actions. If this policy is removed and not replaced at any level of the organization, all OUs and accounts under that level would be blocked from taking any actions.
https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html"

---

**HayLLlHuK** (Mon 08 Apr 2024 13:07) - *Upvotes: 2*
Note: Adding an SCP with full AWS access doesn’t give all the principals in an account access to everything. SCPs don’t grant permissions; they are used to filter permissions. Principals still need a policy within the account that grants them access.

---

**DanShone** (Sat 16 Mar 2024 17:10) - *Upvotes: 1*
A - Inherited SCPs cannot be removed so FullAWSAccess will still apply

---

**devakram** (Thu 11 Apr 2024 11:55) - *Upvotes: 2*
no, I've just tested it in my account now, and B is the true answer. Although there were inherited SCPs coming from root and env which still showed in the SCP page for that OU, after detaching the allow all SCP, I was denied access on any other API except EC2.

---

**thanhnv142** (Tue 06 Feb 2024 16:59) - *Upvotes: 2*
B is correct: SCP have allow statement and this matchs

---

**sarlos** (Wed 24 Jan 2024 04:00) - *Upvotes: 1*
a is the answer

---

**1123lluu** (Sat 02 Dec 2023 04:49) - *Upvotes: 1*
should be B, see example in here: https://aws.amazon.com/blogs/security/get-more-out-of-service-control-policies-in-a-multi-account-environment/

---


<br/>

## Question 135

*Date: Sept. 1, 2023, 3:07 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is examining its disaster recovery capability and wants the ability to switch over its daily operations to a secondary AWS Region. The company uses AWS CodeCommit as a source control tool in the primary Region.

A DevOps engineer must provide the capability for the company to develop code in the secondary Region. If the company needs to use the secondary Region, developers can add an additional remote URL to their local Git configuration.

Which solution will meet these requirements?

**Options:**
- A. Create a CodeCommit repository in the secondary Region. Create an AWS CodeBuild project to perform a Git mirror operation of the primary Region's CodeCommit repository to the secondary Region's CodeCommit repository. Create an AWS Lambda function that invokes the CodeBuild project. Create an Amazon EventBridge rule that reacts to merge events in the primary Region's CodeCommit repository. Configure the EventBridge rule to invoke the Lambda function.
- B. Create an Amazon S3 bucket in the secondary Region. Create an AWS Fargate task to perform a Git mirror operation of the primary Region's CodeCommit repository and copy the result to the S3 bucket. Create an AWS Lambda function that initiates the Fargate task. Create an Amazon EventBridge rule that reacts to merge events in the CodeCommit repository. Configure the EventBridge rule to invoke the Lambda function.
- C. Create an AWS CodeArtifact repository in the secondary Region. Create an AWS CodePipeline pipeline that uses the primary Region’s CodeCommit repository for the source action. Create a cross-Region stage in the pipeline that packages the CodeCommit repository contents and stores the contents in the CodeArtifact repository when a pull request is merged into the CodeCommit repository.
- D. Create an AWS Cloud9 environment and a CodeCommit repository in the secondary Region. Configure the primary Region's CodeCommit repository as a remote repository in the AWS Cloud9 environment. Connect the secondary Region's CodeCommit repository to the AWS Cloud9 environment.

> **Suggested Answer:** A
> **Community Vote:** A (87%), 13%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**GripZA** (Sun 20 Apr 2025 16:37) - *Upvotes: 1*
the goal is to enable code development in a secondary Region in case of a failover.

[A] provides:
- a mirrored CodeCommit repo in the secondary Region.
- automated sync via Git mirroring (a Git feature that fully syncs repositories).
- automation through CodeBuild, Lambda, and EventBridge to keep the secondary repo up to date.

Developers can simply add the secondary repo as a remote (git remote add backup URL) and push/pull as needed

problem with D:
- cloud9 is a development env (IDE), not a repository mirroring solution
- having both repos in the Cloud9 env doesn’t keep them synchronized automatically.

This doesn’t scale or allow other devs to use the secondary repo unless they also set up custom workflows

---

**zijo** (Tue 02 Jul 2024 20:27) - *Upvotes: 1*
Why is not D a solution? If the developers in the secondary region can configure primary region's codecommit repository as a remote repository in the AWS Cloud9 environment they can do development and do all git functions remote.

---

**Gomer** (Sat 22 Jun 2024 06:52) - *Upvotes: 1*
A: (NO) "Create an AWS CodeBuild project to perform a Git mirror operation of the primary Region's CodeCommit repository to the secondary Region's CodeCommit repository."
CodeBuild doesn't have the ability to do a "git mirror" operation itself. All online examples have CodeCommit actions calling Lambda (directly or through EventWatch) which calls fargate (or EC2) which does the actual git mirror

A: (NO) "Create an AWS Lambda function that invokes the CodeBuild project.
The is exactly the reverse from online examples

B: (NO) "Create an AWS Fargate task to perform a Git mirror operation of the primary Region's CodeCommit repository and copy the result to the S3 bucket."
Does it really make sense to use a "git" mirror operations copy from a CodeCommit repo to an S3 bucket? All online examples using "git" "mirror" have CodeCommit repo as remote target.

---

**Gomer** (Sat 22 Jun 2024 06:54) - *Upvotes: 1*
The specific requirement here isn't "disaster recovery capability" and "ability to switch over its daily operations to a secondary AWS Region." That is just being investigated.
The specific requirement is to "provide the capability for the company to develop code in the secondary Region."
"If the company needs to use the secondary Region, developers can add an additional remote URL to their local Git configuration."

To me this sounds to like the specific requirement here is only to provide developers with a complete remote development environment (not to provide a DR solution)
If that is true, then using Cloud9 web development environment (includes git, etc.) with same local CodeCommit repo is acceptiable
I'm not a developer, but the specific criteria wording and logic make me lean towards "C"

---

**Gomer** (Sat 22 Jun 2024 07:01) - *Upvotes: 2*
Actually, I meant to say I lean towards "D" (using Cloud9 as remote development environment)

---

**Gomer** (Sat 22 Jun 2024 07:24) - *Upvotes: 1*
Also want to add that "D" would would work fine if you presume that the "git" "mirror" is also being done (though additional undefined step in the solution). Nothing says "D" is the complete solution. The ONLY requirement here is to provide developers a remote environment to develop in.

---

**Gomer** (Sat 22 Jun 2024 06:55) - *Upvotes: 2*
Flow:

AWS Example: CodeCommit(action) ----------------> Lambda -> Fargate task ("git clone --mirror" local repo, "git remote set-url --push origin" destination repo) -> CodeCommit(remote repo)

Solution "B": CodeCommit(action) -> EventBridge -> Lambda -> Fargate task ("git clone --mirror" local repo, "git remote set-url --push origin" destination repo) -> S3(remote bucket)

References:
https://aws.amazon.com/blogs/devops/replicate-aws-codecommit-repository-between-regions-using-aws-fargate/
https://aws.amazon.com/cloud9/

---

**thanhnv142** (Wed 07 Feb 2024 04:11) - *Upvotes: 3*
A is correct: <A DevOps engineer must provide the capability for the company to develop code in the secondary Region> means code commit

---

**thanhnv142** (Fri 09 Feb 2024 14:24) - *Upvotes: 2*
A is correct: < develop code in the secondary Region>: code commit cannot automatically clone cross-region.Must use a tool to do this duplication task
B: Using S3 as a secondary repo is incorrect
C and D: no mention of using codecommit as the secondary repo

---

**yuliaqwerty** (Wed 10 Jan 2024 20:49) - *Upvotes: 2*
Agree answer is A

---


<br/>

## Question 136

*Date: Sept. 1, 2023, 11 a.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps team is merging code revisions for an application that uses an Amazon RDS Multi-AZ DB cluster for its production database. The DevOps team uses continuous integration to periodically verify that the application works. The DevOps team needs to test the changes before the changes are deployed to the production database.

Which solution will meet these requirements?

**Options:**
- A. Use a buildspec file in AWS CodeBuild to restore the DB cluster from a snapshot of the production database, run integration tests, and drop the restored database after verification.
- B. Deploy the application to production. Configure an audit log of data control language (DCL) operations to capture database activities to perform if verification fails.
- C. Create a snapshot of the DB cluster before deploying the application. Use the Update requires:Replacement property on the DB instance in AWS CloudFormation to deploy the application and apply the changes.
- D. Ensure that the DB cluster is a Multi-AZ deployment. Deploy the application with the updates. Fail over to the standby instance if verification fails.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Ramdi1** (Sat 10 Feb 2024 16:14) - *Upvotes: 5*
A is the solution which will allow testing without any such consequence

---

**jamesf** (Mon 29 Jul 2024 09:06) - *Upvotes: 2*
A correct as allow testing before real deployment.
https://aws.amazon.com/blogs/devops/enhancing-automated-database-continuous-integration-with-aws-codebuild-and-amazon-rds-database-snapshot/

---

**thanhnv142** (Fri 09 Feb 2024 14:43) - *Upvotes: 4*
A is correct: This option allow testing before real deployment
B: < Deploy the application to production> : this would not allow testing before changes are made
C: <Create a snapshot of the DB cluster before deploying the application>: This means the same as B - would not allow testing before changes are made
D: <Ensure that the DB cluster is a Multi-AZ deployment. Deploy the application with the updates> - This deploy the app before testing, so it is incoorect

---

**thanhnv142** (Wed 07 Feb 2024 04:13) - *Upvotes: 1*
A is correct: <The DevOps team uses continuous integration to periodically verify that the application works> and <The DevOps team needs to test the changes before the changes are deployed to the production database> means codebuild

---

**Dushank** (Sat 23 Sep 2023 09:30) - *Upvotes: 1*
This solution meets all of the company's requirements:

It allows the DevOps team to test the changes before they are deployed to the production database.
It is automated: the CodeBuild buildspec file will automatically restore the DB cluster from a snapshot, run the integration tests, and drop the restored database after verification.
It is reliable: the CodeBuild buildspec file will ensure that the integration tests are run against a copy of the production database.

---

**ixdb** (Fri 01 Sep 2023 15:08) - *Upvotes: 4*
A is right. All others will change the prod db.

---

**traveller37** (Fri 01 Sep 2023 11:00) - *Upvotes: 2*
I think it is A

https://aws.amazon.com/blogs/devops/enhancing-automated-database-continuous-integration-with-aws-codebuild-and-amazon-rds-database-snapshot/

---


<br/>

## Question 137

*Date: Sept. 1, 2023, 11:02 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company manages a multi-tenant environment in its VPC and has configured Amazon GuardDuty for the corresponding AWS account. The company sends all GuardDuty findings to AWS Security Hub.

Traffic from suspicious sources is generating a large number of findings. A DevOps engineer needs to implement a solution to automatically deny traffic across the entire VPC when GuardDuty discovers a new suspicious source.

Which solution will meet these requirements?

**Options:**
- A. Create a GuardDuty threat list. Configure GuardDuty to reference the list. Create an AWS Lambda function that will update the threat list. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty.
- B. Configure an AWS WAF web ACL that includes a custom rule group. Create an AWS Lambda function that will create a block rule in the custom rule group. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty.
- C. Configure a firewall in AWS Network Firewall. Create an AWS Lambda function that will create a Drop action rule in the firewall policy. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty.
- D. Create an AWS Lambda function that will create a GuardDuty suppression rule. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty.

> **Suggested Answer:** C
> **Community Vote:** C (92%), 4%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**traveller37** (Fri 01 Sep 2023 11:02) - *Upvotes: 14*
I think C:
https://aws.amazon.com/blogs/security/automatically-block-suspicious-traffic-with-aws-network-firewall-and-amazon-guardduty/

---

**traveller37** (Sat 02 Sep 2023 15:41) - *Upvotes: 1*
Sorry i means B

---

**denccc** (Sun 05 Nov 2023 14:37) - *Upvotes: 1*
You mean C?

---

**RVivek** (Fri 22 Sep 2023 23:48) - *Upvotes: 11*
C is correct . Only Network Firewall can block traffic at VPC level.
A only updates the list , no blocking action
B- WAF and Web ACL can block only HTTPS traffic for a API/VPC endpoint/ Cloudfron distribution not for enire VPC

---

**jamesf** (Mon 29 Jul 2024 09:10) - *Upvotes: 2*
C, AWS Network Firewall can block traffic at VPC level.
https://aws.amazon.com/blogs/security/automatically-block-suspicious-traffic-with-aws-network-firewall-and-amazon-guardduty/

---

**zijo** (Tue 02 Jul 2024 22:37) - *Upvotes: 1*
B blocks traffic at the http/https web traffic layer not for VPC layer

---

**thanhnv142** (Wed 07 Feb 2024 04:16) - *Upvotes: 3*
C is correct: <a solution to automatically deny traffic> means network FW.
A: irrelevant
B: We need network fw, not WAF
D: irrelevant

---

**yorkicurke** (Sat 25 Nov 2023 21:16) - *Upvotes: 1*
hmmm
is this the last question as of now(25th Nov 23)

---

**Dushank** (Sat 23 Sep 2023 09:38) - *Upvotes: 7*
Here's the rationale for choosing this option:

AWS Network Firewall:
AWS Network Firewall is designed to provide centralized network traffic inspection and filtering. It's a suitable choice for implementing network-level controls.

Lambda Function for Automation:
Creating a Lambda function to trigger the creation of a Drop action rule in the firewall policy allows for automated response based on Security Hub findings. This enables you to take immediate action when suspicious sources are detected.

Specific Action (Drop):
The Drop action rule is effective for denying traffic from suspicious sources, effectively controlling access and preventing unwanted traffic.

This approach aligns well with the requirement to automatically deny traffic when GuardDuty identifies a new suspicious source, enhancing security in the multi-tenant VPC environment.

---

**RVivek** (Wed 20 Sep 2023 14:35) - *Upvotes: 1*
A only will upadte threat list. the requirement is to block the taffic.
B is corerect. Also it is event driven immditae action

---


<br/>

## Question 138

*Date: Nov. 22, 2023, 2:15 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses AWS Secrets Manager to store a set of sensitive API keys that an AWS Lambda function uses. When the Lambda function is invoked the Lambda function retrieves the API keys and makes an API call to an external service. The Secrets Manager secret is encrypted with the default AWS Key Management Service (AWS KMS) key.

A DevOps engineer needs to update the infrastructure to ensure that only the Lambda function’s execution role can access the values in Secrets Manager. The solution must apply the principle of least privilege.

Which combination of steps will meet these requirements? (Choose two.)

**Options:**
- A. Update the default KMS key for Secrets Manager to allow only the Lambda function’s execution role to decrypt
- B. Create a KMS customer managed key that trusts Secrets Manager and allows the Lambda function's execution role to decrypt. Update Secrets Manager to use the new customer managed key
- C. Create a KMS customer managed key that trusts Secrets Manager and allows the account's root principal to decrypt. Update Secrets Manager to use the new customer managed key
- D. Ensure that the Lambda function’s execution role has the KMS permissions scoped on the resource level. Configure the permissions so that the KMS key can encrypt the Secrets Manager secret
- E. Remove all KMS permissions from the Lambda function’s execution role

> **Suggested Answer:** BD
> **Community Vote:** BD (95%), 5%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Wed 07 Feb 2024 04:21) - *Upvotes: 5*
B and D are correct: <update the infrastructure to ensure that only the Lambda function’s execution role> means we need to ensure that lambda's IAM role has sufficient permissions and KMS policy allows Lambda's IAM role
A: cannot update default key
C: <allows the account's root principal to decrypt> this against the principal of least privilege
E: irrelevant

---

**heff_bezos** (Mon 23 Sep 2024 20:43) - *Upvotes: 1*
If default keys are the same as the AWS managed keys, then the answer is B. You cannot modify the "default" key's policy to allow access only from the Lambda execution role.

---

**jamesf** (Mon 29 Jul 2024 09:17) - *Upvotes: 2*
I go for BD

---

**4555894** (Sat 09 Mar 2024 18:37) - *Upvotes: 2*
The requirement is to update the infrastructure to ensure that only the Lambda function’s execution
role can access the values in Secrets Manager. The solution must apply the principle of least
privilege, which means granting the minimum permissions necessary to perform a task.

---

**hotblooded** (Sun 04 Feb 2024 10:49) - *Upvotes: 1*
{
"Version": "2012-10-17",
"Id": "key-consolepolicy-2",
"Statement": [
{
"Sid": "Allow use of the key",
"Effect": "Allow",
"Principal": {"AWS": [
"arn:aws:iam::111122223333:role/KeyCreatorRole"
]},
"Action": [
"kms:Encrypt",
"kms:Decrypt",
"kms:ReEncrypt*",
"kms:GenerateDataKey*",
"kms:DescribeKey"
],
"Resource": here arn of secret manager
}
]
}

I think A is correct answer , why to create CMK as customer is using default KMS

---

**zolthar_z** (Thu 30 Nov 2023 15:30) - *Upvotes: 4*
I think B:D

---

**radev** (Thu 23 Nov 2023 10:25) - *Upvotes: 4*
B, D
A is incorrect because updating the default KMS key for Secrets Manager to allow only the Lambda function's execution role to decrypt would grant access to all other resources using the default key, which violates the principle of least privilege.

C is incorrect because allowing the account's root principal to decrypt the secret would grant unnecessary access to the secret, which violates the principle of least privilege.

E is incorrect because removing all KMS permissions from the Lambda function's execution role would prevent the Lambda function from decrypting the secret, which is required for it to function properly.

---

**hotblooded** (Sun 04 Feb 2024 10:42) - *Upvotes: 1*
{
"Version": "2012-10-17",
"Id": "key-consolepolicy-2",
"Statement": [
{
"Sid": "Allow use of the key",
"Effect": "Allow",
"Principal": {"AWS": [
"arn:aws:iam::111122223333:role/KeyCreatorRole"
]},
"Action": [
"kms:Encrypt",
"kms:Decrypt",
"kms:ReEncrypt*",
"kms:GenerateDataKey*",
"kms:DescribeKey"
],
"Resource": here arn of secret manager
}
]
}

I think A is correct answer , why to create CMK as customer is using default KMS

---

**hotblooded** (Sun 04 Feb 2024 10:46) - *Upvotes: 1*
Or we can ad below condition also

"Condition": {
"StringEquals": {
"kms:CallerAccount": "111122223333",
"kms:ViaService": "secretsmanager.us-west-2.amazonaws.com"
}
}

---

**vandergun** (Wed 22 Nov 2023 14:15) - *Upvotes: 2*
I vote B,D

---


<br/>

## Question 139

*Date: Nov. 23, 2023, 1:29 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company's DevOps engineer is creating an AWS Lambda function to process notifications from an Amazon Simple Notification Service (Amazon SNS) topic. The Lambda function will process the notification messages and will write the contents of the notification messages to an Amazon RDS Multi-AZ DB instance.

During testing, a database administrator accidentally shut down the DB instance. While the database was down the company lost several of the SNS notification messages that were delivered during that time.

The DevOps engineer needs to prevent the loss of notification messages in the future.

Which solutions will meet this requirement? (Choose two.)

**Options:**
- A. Replace the RDS Multi-AZ DB instance with an Amazon DynamoDB table.
- B. Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination of the Lambda function.
- C. Configure an Amazon Simple Queue Service (Amazon SQS) dead-letter queue for the SNS topic.
- D. Subscribe an Amazon Simple Queue Service (Amazon SQS) queue to the SNS topic. Configure the Lambda function to process messages from the SQS queue.
- E. Replace the SNS topic with an Amazon EventBridge event bus. Configure an EventBridge rule on the new event bus to invoke the Lambda function for each event.

> **Suggested Answer:** CD
> **Community Vote:** CD (87%), 13%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**vandergun** (Thu 23 Nov 2023 13:29) - *Upvotes: 11*
The two solutions that will meet the requirement of preventing the loss of notification messages in the future are:

D. Subscribe an Amazon Simple Queue Service (Amazon SQS) queue to the SNS topic. Configure the Lambda function to process messages from the SQS queue.

This solution will ensure that notification messages are delivered to the SQS queue even if the Lambda function is unavailable or the RDS DB instance is down. The Lambda function can then process the messages from the SQS queue at its own pace.

C. Configure an Amazon Simple Queue Service (Amazon SQS) dead-letter queue for the SNS topic.

This solution will ensure that notification messages that cannot be delivered to the RDS DB instance are not lost. Instead, they will be moved to a dead-letter queue. The DevOps engineer can then manually process the messages from the dead-letter queue.

---

**zolthar_z** (Thu 30 Nov 2023 15:34) - *Upvotes: 5*
C:D , D is a best practice for this scenario, C because you can send failed SNS o SQS Dead letter queue, https://docs.aws.amazon.com/sns/latest/dg/sns-dead-letter-queues.html

---

**ce0df07** (Sat 08 Feb 2025 00:58) - *Upvotes: 1*
C does not make sense. Adding a DLQ to an SNS topic will capture messages that failed to be sent to the consumer, due to an issue with SNS. We are looking to capture messages that couldn't be processed due to a issue at the consumer side (RDS database to be exact, but the Lambda function will be aware of this).
Hence there are two solutions:
1. configure a SQS between the SNS topic and the Lambda function
2. configure a DLQ as a destination for failure for the Lambda function (raise an exceuption in the Lambda code)

---

**CHRIS12722222** (Sun 15 Dec 2024 08:51) - *Upvotes: 1*
Correct answer

https://www.youtube.com/watch?v=rYFAdRCibyc

---

**weixing** (Thu 10 Oct 2024 08:34) - *Upvotes: 1*
BD
C. Dead-letter queues can only be added to SNS subscriptions, not to topics.

---

**h432ng** (Mon 22 Jul 2024 15:03) - *Upvotes: 2*
AD.

C is wrong, "Configuring an Amazon SNS dead-letter queue for a subscription" not for SNS topic
A is correct, with Dynamodb, admin can no longer "accidentally shut down the DB instance."

A fixes the root cause. With D an SQS is there, no need for DLQ for SNS. If lambda process data from SQS, what is SNS DLQ help here?

---

**thanhnv142** (Wed 07 Feb 2024 04:27) - *Upvotes: 4*
C and D are correct: <. While the database was down the company lost several of the SNS notification messages that were delivered during that time> means dead-letter queue in SQS and output SNS to SQS to store dead-letter queue

---

**zain1258** (Tue 28 Nov 2023 15:52) - *Upvotes: 1*
B & D are correct

---

**Gomer** (Mon 24 Jun 2024 21:55) - *Upvotes: 2*
Here's what I get when you break it down graphically between CD and BC:

CD: SNS > SQS|DLQ) > Lambda > RDS
BD: SNS > SQS > Lambda > SQS > RDS
The DLQ is just there to handle any SNS messages that have errors and can't be processed. There is no way you want/need two SQS queues in series (on either side of the Lambda). The ONLY thing you need to add for the requirements is queue to hold stuff while DB is down. The DLQ just makes sure even an messed up message data is captured for later review. Only C&D make any sense here.

---


<br/>

## Question 140

*Date: Nov. 26, 2023, 7:14 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an application that runs on Amazon EC2 instances. The company uses an AWS CodePipeline pipeline to deploy the application into multiple AWS Regions. The pipeline is configured with a stage for each Region. Each stage contains an AWS CloudFormation action for each Region.

When the pipeline deploys the application to a Region, the company wants to confirm that the application is in a healthy state before the pipeline moves on to the next Region. Amazon Route 53 record sets are configured for the application in each Region. A DevOps engineer creates a Route 53 health check that is based on an Amazon CloudWatch alarm for each Region where the application is deployed.

What should the DevOps engineer do next to meet the requirements?

**Options:**
- A. Create an AWS Step Functions workflow to check the state of the CloudWatch alarm. Configure the Step Functions workflow to exit with an error if the alarm is in the ALARM state. Create a new stage in the pipeline between each Region deployment stage. In each new stage, include an action to invoke the Step Functions workflow.
- B. Configure an AWS CodeDeploy application to deploy a CloudFormation template with automatic rollback. Configure the CloudWatch alarm as the instance health check for the CodeDeploy application. Remove the CloudFormation actions from the pipeline. Create a CodeDeploy action in the pipeline stage for each Region.
- C. Create a new pipeline stage for each Region where the application is deployed. Configure a CloudWatch alarm action for the new stage to check the state of the CloudWatch alarm and to exit with an error if the alarm is in the ALARM state
- D. Configure the CloudWatch agent on the EC2 instances to report the application status to the Route 53 health check. Create a new pipeline stage for each Region where the application is deployed. Configure a CloudWatch alarm action to exit with an error if the CloudWatch alarm is in the ALARM state.

> **Suggested Answer:** A
> **Community Vote:** A (94%), 6%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**heff_bezos** (Mon 23 Sep 2024 22:15) - *Upvotes: 2*
There are no such things as cloudwatch alarm actions. The only things alarms can do is send notifications to an SNS topic. You can perform actions by using EventBridge (CloudWatch Log events) or Step Functions.

---

**zijo** (Tue 09 Jul 2024 22:43) - *Upvotes: 1*
D seems to be simple solution for me
The CloudWatch agent on EC2 instances can be configured to report the application status, and this information can then be used by Route 53 health checks.
Create Route 53 health checks that are based on the CloudWatch alarms. When you create a health check in Route 53, you can specify that the health check should be based on the state of a CloudWatch alarm. Route 53 health checks can be configured to treat the CloudWatch alarm state as Healthy or Unhealthy.

---

**govindrk** (Fri 09 Feb 2024 13:27) - *Upvotes: 1*
D is correct - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/monitoring-cloudwatch.html

---

**thanhnv142** (Wed 07 Feb 2024 04:49) - *Upvotes: 3*
A is correct: <confirm that the application is in a healthy state before the pipeline moves on to the next Region.> means we need a new stage
B and C: no mention of creating a new stage
D: irrelevant

---

**a54b16f** (Mon 15 Jan 2024 20:29) - *Upvotes: 4*
Exact scenario for Step usage: different routing options based on choices

---

**zolthar_z** (Thu 30 Nov 2023 15:38) - *Upvotes: 4*
A: https://dev.to/aws-builders/dynamic-build-orchestration-using-codepipeline-codebuild-and-step-functions-2kpa

---

**zain1258** (Tue 28 Nov 2023 16:01) - *Upvotes: 3*
A is correct answer

---

**TheAWSRhino** (Sun 26 Nov 2023 19:14) - *Upvotes: 3*
A - 'If the state machine execution reaches a terminal status of FAILED, TIMED_OUT, or ABORTED, the action execution fails.' https://docs.aws.amazon.com/codepipeline/latest/userguide/action-reference-StepFunctions.html

Can't be D because you can't update a Route53 healhcheck via the Cloudwatch agent

---


<br/>

## Question 141

*Date: Nov. 23, 2023, 9:25 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company plans to use Amazon CloudWatch to monitor its Amazon EC2 instances. The company needs to stop EC2 instances when the average of the NetworkPacketsIn metric is less than 5 for at least 3 hours in a 12-hour time window. The company must evaluate the metric every hour. The EC2 instances must continue to run if there is missing data for the NetworkPacketsIn metric during the evaluation period.

A DevOps engineer creates a CloudWatch alarm for the NetworkPacketsIn metric. The DevOps engineer configures a threshold value of 5 and an evaluation period of 1 hour.

Which set of additional actions should the DevOps engineer take to meet these requirements?

**Options:**
- A. Configure the Datapoints to Alarm value to be 3 out of 12. Configure the alarm to treat missing data as breaching the threshold. Add an AWS Systems Manager action to stop the instance when the alarm enters the ALARM state.
- B. Configure the Datapoints to Alarm value to be 3 out of 12. Configure the alarm to treat missing data as not breaching the threshold. Add an EC2 action to stop the instance when the alarm enters the ALARM state.
- C. Configure the Datapoints to Alarm value to be 9 out of 12. Configure the alarm to treat missing data as breaching the threshold. Add an EC2 action to stop the instance when the alarm enters the ALARM state.
- D. Configure the Datapoints to Alarm value to be 9 out of 12. Configure the alarm to treat missing data as not breaching the threshold. Add an AWS Systems Manager action to stop the instance when the alarm enters the ALARM state.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**zolthar_z** (Thu 30 Nov 2023 15:43) - *Upvotes: 7*
B: This is the reason https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html#AddingStopActions

---

**zijo** (Wed 21 Aug 2024 18:08) - *Upvotes: 1*
In CloudWatch alarms, datapoints are the individual metric values collected during each period. Here the period is 1 hour and 1 datapoint every hour collected. So 3 datapoints out of 12 is the alrm state because the network threshold has to be less than 5 for atleast 3 hours to Alarm state. The DevOps Engineer sets evaluation for every hour to look for the threshold value of 5. So if 1 hour has no data it is not breaching threshold. If there The alarm evaluates these datapoints against the conditions you set to determine whether it should trigger an action which is stopping an EC2 instance. My explanation for B

---

**PrasannaBalaji** (Sat 30 Dec 2023 04:48) - *Upvotes: 4*
B is correct

---

**tom_cat** (Fri 24 Nov 2023 19:36) - *Upvotes: 3*
I think B

---

**vandergun** (Thu 23 Nov 2023 15:06) - *Upvotes: 4*
B should be corrected

---

**vandergun** (Thu 23 Nov 2023 09:25) - *Upvotes: 3*
B should be corrected

---


<br/>

## Question 142

*Date: Nov. 23, 2023, 9:27 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company manages 500 AWS accounts that are in an organization in AWS Organizations. The company discovers many unattached Amazon Elastic Block Store (Amazon EBS) volumes in all the accounts. The company wants to automatically tag the unattached EBS volumes for investigation.

A DevOps engineer needs to deploy an AWS Lambda function to all the AWS accounts. The Lambda function must run every 30 minutes to tag all the EBS volumes that have been unattached for a period of 7 days or more.

Which solution will meet these requirements in the MOST operationally efficient manner?

**Options:**
- A. Configure a delegated administrator account for the organization. Create an AWS CloudFormation template that contains the Lambda function. Use CloudFormation StackSets to deploy the CloudFormation template from the delegated administrator account to all the member accounts in the organization. Create an Amazon EventBridge event bus in the delegated administrator account to invoke the Lambda function in each member account every 30 minutes.
- B. Create a cross-account IAM role in the organization's member accounts. Attach the AWSLambda_FullAccess policy and the AWSCloudFormationFullAccess policy to the role. Create an AWS CloudFormation template that contains the Lambda function and an Amazon EventBridge scheduled rule to invoke the Lambda function every 30 minutes. Create a custom script in the organization’s management account that assumes the role and deploys the CloudFormation template to the member accounts.
- C. Configure a delegated administrator account for the organization. Create an AWS CloudFormation template that contains the Lambda function and an Amazon EventBridge scheduled rule to invoke the Lambda function every 30 minutes. Use CloudFormation StackSets to deploy the CloudFormation template from the delegated administrator account to all the member accounts in the organization
- D. Create a cross-account IAM role in the organization's member accounts. Attach the AmazonS3FullAccess policy and the AWSCodeDeployDeployerAccess policy to the role. Use AWS CodeDeploy to assume the role to deploy the Lambda function from the organization's management account. Configure an Amazon EventBridge scheduled rule in the member accounts to invoke the Lambda function every 30 minutes.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Wed 07 Aug 2024 13:28) - *Upvotes: 5*
C is correct: <The Lambda function must run every 30 minutes to tag all the EBS volumes>: we should use a combination of eventbridge and Lambda
A: <. Create an Amazon EventBridge event bus in the delegated administrator account to invoke the Lambda function>: event bridge should be in each member account to monitor event, not in the delegated admin's account
B and D: These options create an IAM role in every member account, which is incorrect

---

**DanShone** (Mon 16 Sep 2024 15:59) - *Upvotes: 3*
C make the most sense

---

**a54b16f** (Fri 12 Jul 2024 17:46) - *Upvotes: 3*
NOT A: you don't want to run it for every user accounts

---

**yuliaqwerty** (Thu 11 Jul 2024 10:32) - *Upvotes: 3*
Agree with C

---

**davdan99** (Tue 09 Jul 2024 20:05) - *Upvotes: 2*
Why no A?

---

**zain1258** (Tue 28 May 2024 15:09) - *Upvotes: 3*
C is correct

---

**tom_cat** (Fri 24 May 2024 18:35) - *Upvotes: 2*
C makes sense

---

**vandergun** (Thu 23 May 2024 08:27) - *Upvotes: 2*
I vote C

---


<br/>

## Question 143

*Date: Nov. 23, 2023, 9:28 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company's production environment uses an AWS CodeDeploy blue/green deployment to deploy an application. The deployment incudes Amazon EC2 Auto Scaling groups that launch instances that run Amazon Linux 2.

A working appspec.yml file exists in the code repository and contains the following text:



A DevOps engineer needs to ensure that a script downloads and installs a license file onto the instances before the replacement instances start to handle request traffic. The DevOps engineer adds a hooks section to the appspec.yml file.

Which hook should the DevOps engineer use to run the script that downloads and installs the license file?

**Options:**
- A. AfterBlockTraffic
- B. BeforeBlockTraffic
- C. BeforeInstall
- D. DownloadBundle

> **Suggested Answer:** C
> **Community Vote:** C (95%), 5%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Wed 07 Aug 2024 13:47) - *Upvotes: 7*
C is correct: For blue/green deployment, Before install is one of several hooks that come before <the replacement instances> start to handle request traffic.
A and B: these hooks come after the replacement instances start to handle request traffic. They are hooks from the original instance, which are two of 3 last steps.
D: There is no such hook in blue/green deployment

---

**tom_cat** (Fri 24 May 2024 18:47) - *Upvotes: 5*
A & B are not available for replacement instances - https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-availability
D - "Reserved for CodeDeploy operations. Cannot be used to run scripts."

---

**ET2025** (Wed 09 Jul 2025 23:57) - *Upvotes: 1*
AfterBlockTraffic - A is correct

---

**DanShone** (Mon 16 Sep 2024 15:57) - *Upvotes: 2*
C is correct

---

**twogyt** (Fri 19 Jul 2024 08:51) - *Upvotes: 2*
is C: A DevOps engineer needs to ensure that a script downloads and "installs" a license file onto the instances "before" the replacement instances start to handle request traffic

---

**vandergun** (Thu 23 May 2024 08:28) - *Upvotes: 3*
C should be correct

---


<br/>

## Question 144

*Date: Nov. 23, 2023, 9:30 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an application that includes AWS Lambda functions. The Lambda functions run Python code that is stored in an AWS CodeCommit repository. The company has recently experienced failures in the production environment because of an error in the Python code. An engineer has written unit tests for the Lambda functions to help avoid releasing any future defects into the production environment.

The company's DevOps team needs to implement a solution to integrate the unit tests into an existing AWS CodePipeline pipeline. The solution must produce reports about the unit tests for the company to view.

Which solution will meet these requirements?

**Options:**
- A. Associate the CodeCommit repository with Amazon CodeGuru Reviewer. Create a new AWS CodeBuild project. In the CodePipeline pipeline, configure a test stage that uses the new CodeBuild project. Create a buildspec.yml file in the CodeCommit repository. In the buildspec yml file, define the actions to run a CodeGuru review.
- B. Create a new AWS CodeBuild project. In the CodePipeline pipeline, configure a test stage that uses the new CodeBuild project. Create a CodeBuild report group. Create a buildspec.yml file in the CodeCommit repository. In the buildspec.yml file, define the actions to run the unit tests with an output of JUNITXML in the build phase section. Configure the test reports to be uploaded to the new CodeBuild report group.
- C. Create a new AWS CodeArtifact repository. Create a new AWS CodeBuild project. In the CodePipeline pipeline, configure a test stage that uses the new CodeBuild project. Create an appspec.yml file in the original CodeCommit repository. In the appspec.yml file, define the actions to run the unit tests with an output of CUCUMBERJSON in the build phase section. Configure the tests reports to be sent to the new CodeArtifact repository.
- D. Create a new AWS CodeBuild project. In the CodePipeline pipeline, configure a test stage that uses the new CodeBuild project. Create a new Amazon S3 bucket. Create a buildspec.yml file in the CodeCommit repository. In the buildspec yml file, define the actions to run the unit tests with an output of HTML in the phases section. In the reports section, upload the test reports to the S3 bucket.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Wed 07 Aug 2024 13:56) - *Upvotes: 6*
B is correct: for unit test, we need codebuild
A: codeguru is for code analysis, not unit test
C: This option mentions pushing reports to CodeArtifact repository, which is incorrect
D: This option push reports to S3, which is incorrect. We should upload report to codebuild report group

---

**zain1258** (Tue 28 May 2024 15:15) - *Upvotes: 3*
B is correct

---

**KobraKai** (Mon 27 May 2024 13:40) - *Upvotes: 3*
I think B as per link:
https://docs.aws.amazon.com/codebuild/latest/userguide/test-reporting.html

---

**tom_cat** (Fri 24 May 2024 18:53) - *Upvotes: 3*
I think it should be B

---

**vandergun** (Thu 23 May 2024 08:30) - *Upvotes: 4*
B is corrected

---


<br/>

## Question 145

A company manages multiple AWS accounts in AWS Organizations. The company’s security policy states that AWS account root user credentials for member accounts must not be used. The company monitors access to the root user credentials.

A recent alert shows that the root user in a member account launched an Amazon EC2 instance. A DevOps engineer must create an SCP at the organization's root level that will prevent the root user in member accounts from making any AWS service API calls.

Which SCP will meet these requirements?

**Options:**
- A.
- B.
- C.
- D.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Wed 07 Feb 2024 15:09) - *Upvotes: 9*
C is correct: < will prevent the root user in member accounts> this means deny action
A and D: irrelevant (mention allow statement)
B: scp does not have principal element. only condition

---

**tom_cat** (Fri 24 Nov 2023 20:00) - *Upvotes: 8*
I believe it should be C
https://docs.aws.amazon.com/organizations/latest/userguide/best-practices_member-acct.html#bp_member-acct_use-scp

---

**Gomer** (Wed 26 Jun 2024 00:23) - *Upvotes: 1*
A slightly more consise version of "C" is a "strongly recommended" control to deny root access in member accounts. See the example:
https://docs.aws.amazon.com/controltower/latest/controlreference/strongly-recommended-controls.html#disallow-root-auser-actions

---

**c3518fc** (Tue 23 Apr 2024 18:10) - *Upvotes: 1*
https://docs.aws.amazon.com/organizations/latest/userguide/best-practices_member-acct.html#bp_member-acct_use-scp

---

**DanShone** (Sat 16 Mar 2024 16:53) - *Upvotes: 1*
C is correct

---

**manman7** (Sat 09 Dec 2023 08:47) - *Upvotes: 4*
It's C, based on the documentation :
https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_general.html#example-scp-root-user

---

**zain1258** (Tue 28 Nov 2023 16:16) - *Upvotes: 2*
C looks correct

---


<br/>

## Question 146

*Date: Nov. 23, 2023, 1:26 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses AWS and has a VPC that contains critical compute infrastructure with predictable traffic patterns. The company has configured VPC flow logs that are published to a log group in Amazon CloudWatch Logs.

The company's DevOps team needs to configure a monitoring solution for the VPC flow logs to identify anomalies in network traffic to the VPC over time. If the monitoring solution detects an anomaly, the company needs the ability to initiate a response to the anomaly.

How should the DevOps team configure the monitoring solution to meet these requirements?

**Options:**
- A. Create an Amazon Kinesis data stream. Subscribe the log group to the data stream. Configure Amazon Kinesis Data Analytics to detect log anomalies in the data stream. Create an AWS Lambda function to use as the output of the data stream. Configure the Lambda function to write to the default Amazon EventBridge event bus in the event of an anomaly finding.
- B. Create an Amazon Kinesis Data Firehose delivery stream that delivers events to an Amazon S3 bucket. Subscribe the log group to the delivery stream. Configure Amazon Lookout for Metrics to monitor the data in the S3 bucket for anomalies. Create an AWS Lambda function to run in response to Lookout for Metrics anomaly findings. Configure the Lambda function to publish to the default Amazon EventBridge event bus.
- C. Create an AWS Lambda function to detect anomalies. Configure the Lambda function to publish an event to the default Amazon EventBridge event bus if the Lambda function detects an anomaly. Subscribe the Lambda function to the log group.
- D. Create an Amazon Kinesis data stream. Subscribe the log group to the data stream. Create an AWS Lambda function to detect log anomalies. Configure the Lambda function to write to the default Amazon EventBridge event bus if the Lambda function detects an anomaly. Set the Lambda function as the processor for the data stream.

> **Suggested Answer:** B
> **Community Vote:** B (69%), A (31%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**giovanna_mag** (Tue 26 Dec 2023 20:14) - *Upvotes: 7*
I think it's B, Amazon Lookout for metrics can detect anomalies from S3 bucket and trigger Lambda
https://aws.amazon.com/lookout-for-metrics/

---

**ce0df07** (Sat 08 Feb 2025 01:45) - *Upvotes: 1*
Using KDA for anomaly detection, means you can use the built-in RCF (Random Cut Forest) ML algorithm.
Option B: Firehose and S3 adds latency. Further, Amazon Lookout for Metrics is more suitable for business metrics than network traffic.
Options C&D require you to implement your own Lambda function, which means more error prone and more maintenance.

---

**jamesf** (Sat 03 Aug 2024 10:17) - *Upvotes: 4*
- Data Streaming: Use Amazon Kinesis Data Firehose to deliver VPC flow logs from CloudWatch Logs to an Amazon S3 bucket.
- Anomaly Detection: Amazon Lookout for Metrics will monitor the data in the S3 bucket and automatically detect anomalies in the network traffic.
- Event Response: When Lookout for Metrics detects an anomaly, it triggers an AWS Lambda function. The Lambda function will then publish an event to the Amazon EventBridge event bus, which can further initiate automated responses, notifications, or alerts.

---

**trungtd** (Wed 10 Jul 2024 01:27) - *Upvotes: 2*
Although option A uses Kinesis Data Analytics for anomaly detection, setting up and maintaining custom analytics and anomaly detection logic is more complex and less efficient compared to using a managed service like Lookout for Metrics.

---

**xdkonorek2** (Wed 03 Jul 2024 21:43) - *Upvotes: 2*
A is wrong because kinesis data analytics output must be either kinesis data stream or firehose, can't be lambda directly so there is a missing component

---

**Gomer** (Wed 26 Jun 2024 02:39) - *Upvotes: 3*
I've reviewed most of the comments, and it seems like everyone is just repeating themselves. I've "googled" and looked at the references. I found examples of both kinesis data streams, kinesis data analytics and firehose. The one step in "A" I have a problem with is "Create an AWS Lambda function to use as the output of the data stream." How can Lambda be an output of a data stream "over time"? I don't think you can identify an anomaly "over time" unless you've got persistent storage for the data (which can be reparsed as necessary to compare past with present). I'm leaning towards "B" unless someone can convince me otherwise (and not by just repeating what others have already said).

---

**tsangckl** (Wed 19 Jun 2024 07:24) - *Upvotes: 2*
Option B involves using Amazon Lookout for Metrics, which is not designed for real-time anomaly detection.

---

**Gomer** (Wed 26 Jun 2024 02:50) - *Upvotes: 1*
I see the "over time" requirement as implying some ability to parse the past with the present in order for ML to assess an anomaly. I don't see the words "real time" in the requirements. The "over time" requirement is not specific enough, but until there are more specifics, it would be reasonable to presume it means your trying to discover current anomalies by comparing traffic from against days, weeks or months ago.

---

**seetpt** (Thu 02 May 2024 13:52) - *Upvotes: 2*
i think B

---

**c3518fc** (Tue 23 Apr 2024 18:26) - *Upvotes: 3*
Lookout for Metrics automatically detects and diagnoses anomalies (outliers from the norm) in business and operational data. It’s a fully managed ML service, which uses specialized ML models to detect anomalies based on the characteristics of your data. You don’t need ML experience to use Lookout for Metrics.

Kinesis Data Analytics Studio provides an interactive notebook experience powered by Apache Zeppelin and Apache Flink to analyze streaming data. It also helps productionize your analytics application by building and deploying code as a Kinesis data analytics application straight from the notebook. https://aws.amazon.com/blogs/machine-learning/smart-city-traffic-anomaly-detection-using-amazon-lookout-for-metrics-and-amazon-kinesis-data-analytics-studio/

---


<br/>

## Question 147

*Date: Nov. 23, 2023, 10 a.m.
Disclaimers:
- ExamTopics website is not rel*

AnyCompany is using AWS Organizations to create and manage multiple AWS accounts. AnyCompany recently acquired a smaller company, Example Corp. During the acquisition process, Example Corp's single AWS account joined AnyCompany's management account through an Organizations invitation. AnyCompany moved the new member account under an OU that is dedicated to Example Corp.

AnyCompany's DevOps engineer has an IAM user that assumes a role that is named OrganizationAccountAccessRole to access member accounts. This role is configured with a full access policy. When the DevOps engineer tries to use the AWS Management Console to assume the role in Example Corp's new member account, the DevOps engineer receives the following error message: "Invalid information in one or more fields. Check your information or contact your administrator."

Which solution will give the DevOps engineer access to the new member account?

**Options:**
- A. In the management account, grant the DevOps engineer's IAM user permission to assume the OrganizationAccountAccessRole IAM role in the new member account.
- B. In the management account, create a new SCP. In the SCP, grant the DevOps engineer's IAM user full access to all resources in the new member account. Attach the SCP to the OU that contains the new member account.
- C. In the new member account, create a new IAM role that is named OrganizationAccountAccessRole. Attach the AdministratorAccess AWS managed policy to the role. In the role's trust policy, grant the management account permission to assume the role.
- D. In the new member account, edit the trust policy for the OrganizationAccountAccessRole IAM role. Grant the management account permission to assume the role.

> **Suggested Answer:** C
> **Community Vote:** C (63%), D (35%), 2%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Wed 07 Feb 2024 15:23) - *Upvotes: 6*
C is correct: <assume the role in Example Corp's new member account> means this role has not been properly configured (or even not created)
A: only mention assuming the role, not create it.
B: scp has nothing to do here
D: only mention create trust relationship

---

**radev** (Sat 25 Nov 2023 11:59) - *Upvotes: 6*
https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_access.html#orgs_manage_accounts_create-cross-account-role

---

**nickp84** (Thu 15 May 2025 16:52) - *Upvotes: 1*
sorry for this reason:
n the Example Corp account, manually create an IAM role named OrganizationAccountAccessRole.
Attach the AdministratorAccess AWS managed policy to it.
Set the trust policy to allow the management account to assume the role:
{
"Version": "2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Principal": {
"AWS": "arn:aws:iam::<management-account-id>:root"
},
"Action": "sts:AssumeRole"
}
]
}
C is correct — because the account was invited, not created, and therefore the role must be manually created.

---

**nickp84** (Thu 15 May 2025 16:49) - *Upvotes: 1*
option C:Creating a new role with the correct trust policy and permissions would allow the DevOps engineer to assume it.
If the original OrganizationAccountAccessRole was deleted or misconfigured, this could be a fallback.

---

**AC2021** (Fri 24 Jan 2025 01:38) - *Upvotes: 2*
The role is already there. Why create a new one?

---

**ce0df07** (Sat 08 Feb 2025 01:57) - *Upvotes: 1*
The role would not be automatically created in accounts that are added through invitation (as opposed to accounts created within the organization).

---

**Simba84** (Sun 22 Dec 2024 09:08) - *Upvotes: 4*
Correct Answer is D
Role Trust Policy Issue:
When a new account is invited and joins an AWS Organization, the OrganizationAccountAccessRole is typically created automatically.
This role allows the management account to access member accounts, but its trust policy must explicitly grant the management account permission to assume the role.
If this trust policy is not configured correctly, the management account cannot assume the role, leading to the error message.

---

**ce0df07** (Sat 08 Feb 2025 01:58) - *Upvotes: 1*
Incorrect. See https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_access.html#orgs_manage_accounts_create-cross-account-role
"When you create a member account using AWS Organizations, Organizations automatically creates an IAM role in the account that grants administrator access to the management account. For invited member accounts, you must manually create the role."

---

**eugene2owl** (Tue 03 Dec 2024 17:03) - *Upvotes: 5*
I've spent like 30 mins, and now I've got the most full explanation.

Correct answer is "C"
While "D" is NOT FULLY describing what needs to be done (so it's wrong).

The thing you need to know to answer this question is the following:
* if account is generated (meaning NEW account CREATED) within the Org, then this account will automatically have a proper role "OrganizationAccountAccessRole"
* if account is invited (meaning EXISTING account ADDED) to Org, then this account will NOT have such role

Question says, that Management Account tries to assume a role called "OrganizationAccountAccessRole" from member account, but it gets an error saying like "there is no such thing which you request".

So to fix an error you need:
1) Create a IAM Role "OrganizationAccountAccessRole" in a member account
2) Give it FullAccess Policy
3) Allow Management Account to assume this role via its Trust Relationship

---

**hamzaBennis** (Fri 15 Nov 2024 09:36) - *Upvotes: 2*
member accounts that you invite to join your organization do not automatically get an administrator role created.
https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_create-cross-account-role.html

---


<br/>

## Question 148

*Date: Nov. 23, 2023, 10:02 a.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer is designing an application that integrates with a legacy REST API. The application has an AWS Lambda function that reads records from an Amazon Kinesis data stream. The Lambda function sends the records to the legacy REST API.

Approximately 10% of the records that the Lambda function sends from the Kinesis data stream have data errors and must be processed manually. The Lambda function event source configuration has an Amazon Simple Queue Service (Amazon SQS) dead-letter queue as an on-failure destination. The DevOps engineer has configured the Lambda function to process records in batches and has implemented retries in case of failure.

During testing, the DevOps engineer notices that the dead-letter queue contains many records that have no data errors and that already have been processed by the legacy REST API. The DevOps engineer needs to configure the Lambda function's event source options to reduce the number of errorless records that are sent to the dead-letter queue.

Which solution will meet these requirements?

**Options:**
- A. Increase the retry attempts.
- B. Configure the setting to split the batch when an error occurs.
- C. Increase the concurrent batches per shard.
- D. Decrease the maximum age of record.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**c3518fc** (Tue 23 Apr 2024 19:16) - *Upvotes: 11*
When consuming records from a Kinesis data stream using AWS Lambda, the function can process records in batches. By default, if any record in the batch fails to process, the entire batch is sent to the dead-letter queue.
To avoid sending errorless records to the dead-letter queue, the Lambda function's event source options should be configured to split the batch when an error occurs. This setting is called batchWindow and can be configured in the event source mapping for the Lambda function.
When batchWindow is set to TRIM_HORIZON, the Lambda function will split the batch at the first record that causes an error and send only the failed records to the dead-letter queue. The remaining errorless records in the batch will continue to be processed by the function.

---

**Gomer** (Wed 26 Jun 2024 06:14) - *Upvotes: 1*
Seemingly very good explanation, though I had trouble finding any references other than this:
"BisectBatchOnFunctionError" "If the function returns an error, split the batch in two and retry. The default value is false."
aws lambda update-event-source-mapping --bisect-batch-on-function-error [...]

---

**zolthar_z** (Thu 30 Nov 2023 16:53) - *Upvotes: 7*
B: https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html#services-kinesis-eventsourcemapping

---

**tinyshare** (Sat 10 Aug 2024 23:51) - *Upvotes: 2*
B:
https://aws.amazon.com/blogs/big-data/best-practices-for-consuming-amazon-kinesis-data-streams-using-aws-lambda/

---

**thanhnv142** (Wed 07 Feb 2024 15:46) - *Upvotes: 3*
B is correct: <(Amazon SQS) dead-letter queue as an on-failure destination>: split the batch into 2 parts: success ones and error ones. error ones come to dead queue

---

**zain1258** (Thu 23 Nov 2023 19:41) - *Upvotes: 4*
B is correct

---

**vandergun** (Thu 23 Nov 2023 10:02) - *Upvotes: 5*
B is corrected

---


<br/>

## Question 149

*Date: Dec. 29, 2023, 12:53 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has microservices running in AWS Lambda that read data from Amazon DynamoDB. The Lambda code is manually deployed by developers after successful testing. The company now needs the tests and deployments be automated and run in the cloud. Additionally, traffic to the new versions of each microservice should be incrementally shifted over time after deployment.

What solution meets all the requirements, ensuring the MOST developer velocity?

**Options:**
- A. Create an AWS CodePipeline configuration and set up a post-commit hook to trigger the pipeline after tests have passed. Use AWS CodeDeploy and create a Canary deployment configuration that specifies the percentage of traffic and interval.
- B. Create an AWS CodeBuild configuration that triggers when the test code is pushed. Use AWS CloudFormation to trigger an AWS CodePipeline configuration that deploys the new Lambda versions and specifies the traffic shift percentage and interval.
- C. Create an AWS CodePipeline configuration and set up the source code step to trigger when code is pushed. Set up the build step to use AWS CodeBuild to run the tests. Set up an AWS CodeDeploy configuration to deploy, then select the CodeDeployDefault.LambdaLinear10PercentEvery3Minutes option.
- D. Use the AWS CLI to set up a post-commit hook that uploads the code to an Amazon S3 bucket after tests have passed Set up an S3 event trigger that runs a Lambda function that deploys the new version. Use an interval in the Lambda function to deploy the code over time at the required percentage.

> **Suggested Answer:** C
> **Community Vote:** C (97%), 3%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**c3518fc** (Wed 23 Oct 2024 19:22) - *Upvotes: 6*
This solution provides the following benefits:

Automation: The entire process, from code push to testing and deployment, is automated, reducing manual effort and increasing developer velocity.
Integration: By using AWS CodePipeline, CodeBuild, and CodeDeploy, you leverage fully managed services that are designed to work together seamlessly.
Incremental Deployment: The CodeDeployDefault.LambdaLinear10PercentEvery3Minutes option ensures a smooth and controlled migration of traffic to the new versions of your microservices, minimizing the risk of downtime or disruption.

---

**PrasannaBalaji** (Sun 30 Jun 2024 04:15) - *Upvotes: 6*
Agree C is correct

---

**92a2133** (Mon 02 Jun 2025 14:21) - *Upvotes: 2*
Any mention of automated testing and the answer must include CodeBuild

---

**DanShone** (Mon 16 Sep 2024 15:45) - *Upvotes: 3*
C is correct

---

**Shasha1** (Fri 06 Sep 2024 14:54) - *Upvotes: 3*
C
There is no 'pre-commit' hook option in the Lambda deployment hook (Canary); only 'before allowing traffic' and 'after allowing traffic' options are available. Therefore, the 'LambdaLinear10PercentEvery3Minutes' option, which is a canary deployment method, enables a linear deployment strategy, gradually shifting traffic to the new versions at a rate of 10% every 3 minutes.
https://medium.com/@Da_vidgf/canary-deployments-in-serverless-applications-b0f47fa9b409

---

**Chelseajcole** (Mon 12 Aug 2024 16:56) - *Upvotes: 1*
if it is canary, why not a?

---

**thanhnv142** (Wed 07 Aug 2024 14:48) - *Upvotes: 1*
A is correct: canary deployment

---

**GripZA** (Sun 20 Apr 2025 18:56) - *Upvotes: 1*
canary won't meet requirements here since it will shift all traffic after time completes, whereas linear the amount of the traffic routed to the new version will be incremented according to the provided percentage and interval.

---

**twogyt** (Wed 17 Jul 2024 17:05) - *Upvotes: 4*
c is correct

---

**a54b16f** (Fri 12 Jul 2024 18:07) - *Upvotes: 4*
B is wrong, why would you trigger a pipeline when TEST code is pushed

---


<br/>

## Question 150

*Date: Dec. 29, 2023, 2:57 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is building a web and mobile application that uses a serverless architecture powered by AWS Lambda and Amazon API Gateway. The company wants to fully automate the backend Lambda deployment based on code that is pushed to the appropriate environment branch in an AWS CodeCommit repository.

The deployment must have the following:

• Separate environment pipelines for testing and production
• Automatic deployment that occurs for test environments only

Which steps should be taken to meet these requirements?

**Options:**
- A. Configure a new AWS CodePipeline service. Create a CodeCommit repository for each environment. Set up CodePipeline to retrieve the source code from the appropriate repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation.
- B. Create two AWS CodePipeline configurations for test and production environments. Configure the production pipeline to have a manual approval step. Create a CodeCommit repository for each environment. Set up each CodePipeline to retrieve the source code from the appropriate repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation.
- C. Create two AWS CodePipeline configurations for test and production environments. Configure the production pipeline to have a manual approval step. Create one CodeCommit repository with a branch for each environment. Set up each CodePipeline to retrieve the source code from the appropriate branch in the repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation.
- D. Create an AWS CodeBuild configuration for test and production environments. Configure the production pipeline to have a manual approval step. Create one CodeCommit repository with a branch for each environment. Push the Lambda function code to an Amazon S3 bucket. Set up the deployment step to deploy the Lambda functions from the S3 bucket.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Wed 07 Aug 2024 14:53) - *Upvotes: 6*
C is correct: <Separate environment pipelines for testing and production> means codepipeline <code that is pushed to the appropriate environment branch in an AWS CodeCommit repository.> means code CodeCommit
A: no mention of creating Separate env for test and dev
B: <Create a CodeCommit repository for each environment> should not do this. We should create a branch for each env
D: no mention of code pipelines

---

**c3518fc** (Wed 23 Oct 2024 19:29) - *Upvotes: 3*
By creating two CodePipeline configurations, using a single CodeCommit repository with branches for each environment, and deploying Lambda functions with CloudFormation, this solution meets the requirements while following best practices for source code management, continuous delivery, and infrastructure as code.

---

**PrasannaBalaji** (Sun 30 Jun 2024 04:19) - *Upvotes: 3*
C is correct
First, A&B both are in-correct: As a basic policy - do not create a repo for the same code for multiple environments. Always create a branch from the same repo. The strategy is wrong for A&B.
Now C&D: D uses Lambda function with s3, whereas C uses code pipeline to store and build. Using code pipeline is a smart choice rather than using S3 as a code pipeline that offers better branching strategy and controls. I will go with ‘C”.

---

**csG13** (Sat 29 Jun 2024 13:57) - *Upvotes: 2*
It’s C - unique env and also distinct resources in aws codepipeline would result to pull from both repos on every update of either repo.

---


<br/>

## Question 151

*Date: Dec. 29, 2023, 3:36 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer wants to find a solution to migrate an application from on premises to AWS. The application is running on Linux and needs to run on specific versions of Apache Tomcat, HAProxy, and Varnish Cache to function properly. The application's operating system-level parameters require tuning. The solution must include a way to automate the deployment of new application versions. The infrastructure should be scalable and faulty servers should be replaced automatically.

Which solution should the DevOps engineer use?

**Options:**
- A. Upload the application as a Docker image that contains all the necessary software to Amazon ECR. Create an Amazon ECS cluster using an AWS Fargate launch type and an Auto Scaling group. Create an AWS CodePipeline pipeline that uses Amazon ECR as a source and Amazon ECS as a deployment provider.
- B. Upload the application code to an AWS CodeCommit repository with a saved configuration file to configure and install the software. Create an AWS Elastic Beanstalk web server tier and a load balanced-type environment that uses the Tomcat solution stack. Create an AWS CodePipeline pipeline that uses CodeCommit as a source and Elastic Beanstalk as a deployment provider.
- C. Upload the application code to an AWS CodeCommit repository with a set of .ebextensions files to configure and install the software. Create an AWS Elastic Beanstalk worker tier environment that uses the Tomcat solution stack. Create an AWS CodePipeline pipeline that uses CodeCommit as a source and Elastic Beanstalk as a deployment provider.
- D. Upload the application code to an AWS CodeCommit repository with an appspec.yml file to configure and install the necessary software. Create an AWS CodeDeploy deployment group associated with an Amazon EC2 Auto Scaling group. Create an AWS CodePipeline pipeline that uses CodeCommit as a source and CodeDeploy as a deployment provider.

> **Suggested Answer:** D
> **Community Vote:** D (90%), 8%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**csG13** (Fri 29 Dec 2023 15:36) - *Upvotes: 7*
I go with D, simply because with Fargate you have very limited access to the OS.

---

**CloudHandsOn** (Thu 21 Mar 2024 13:40) - *Upvotes: 6*
While option A is a strong candidate due to its serverless nature and ease of deployment, option D is the most suitable solution given the need for specific software versions, OS-level tuning, and the requirement for a scalable and fault-tolerant infrastructure. Option D provides the necessary control over the software environment and infrastructure configuration, along with the benefits of automation and scalability.

---

**Jonalb** (Thu 10 Jul 2025 12:18) - *Upvotes: 1*
CodeCommit + CodeDeploy + EC2 Auto Scaling

---

**ce0df07** (Sat 08 Feb 2025 07:37) - *Upvotes: 1*
Option B is clearly the preferred option:
- Uses Elastic Beanstalk web server tier with load balancing
- Provides managed Tomcat platform
- Supports configuration files for software installation
- Includes auto-scaling and health monitoring
- Uses CodePipeline for automated deployments
- Appropriate for web applications
Option A: Containerization created unnecessary overhead and it will turn up more expensive
Option C: Worker tier is for background processes, not suitable for web applications
Option D:
- More manual configuration required
- Less managed service features
- No built-in platform support
- More complex to maintain

---

**zijo** (Thu 02 Jan 2025 23:08) - *Upvotes: 2*
AWS Fargate does not allow direct tuning of Linux OS-level parameters because it is a fully managed, serverless compute engine for containers. With Fargate, AWS abstracts the underlying infrastructure, including the operating system, and does not expose granular OS-level configurations.

---

**jamesf** (Tue 06 Aug 2024 05:25) - *Upvotes: 2*
keywords:
- specific versions of Apache Tomcat, HAProxy, and Varnish Cache
- operating system-level parameters

Not A as Fargate is serverless

---

**didek1986** (Wed 17 Apr 2024 12:23) - *Upvotes: 5*
Key point: The application's operating system-level parameters require tuning

---

**dkp** (Sat 13 Apr 2024 11:30) - *Upvotes: 3*
ill go with D

---

**devakram** (Sat 13 Apr 2024 10:19) - *Upvotes: 3*
I doubt this question will come up since both A and D are correct: https://aws.amazon.com/blogs/containers/announcing-additional-linux-controls-for-amazon-ecs-tasks-on-aws-fargate/

they need to add some words to the question tat force you to choose either A or D

---

**jamesf** (Mon 29 Jul 2024 10:54) - *Upvotes: 1*
Me too, I still go for D but seen like new update for AWS Fargate have some change

---


<br/>

## Question 152

*Date: Dec. 29, 2023, 1:03 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer is using AWS CodeDeploy across a fleet of Amazon EC2 instances in an EC2 Auto Scaling group. The associated CodeDeploy deployment group, which is integrated with EC2 Auto Scaling, is configured to perform in-place deployments with CodeDeployDefault.OneAtATime. During an ongoing new deployment, the engineer discovers that, although the overall deployment finished successfully, two out of five instances have the previous application revision deployed. The other three instances have the newest application revision.

What is likely causing this issue?

**Options:**
- A. The two affected instances failed to fetch the new deployment.
- B. A failed AfterInstall lifecycle event hook caused the CodeDeploy agent to roll back to the previous version on the affected instances.
- C. The CodeDeploy agent was not installed in two affected instances.
- D. EC2 Auto Scaling launched two new instances while the new deployment had not yet finished, causing the previous version to be deployed on the affected instances.

> **Suggested Answer:** D
> **Community Vote:** D (89%), 11%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Fri 09 Aug 2024 14:51) - *Upvotes: 6*
D is correct: In-place deployment search for all available agents at the time of the deployment and update the app version. If there are new instances launched after the search, they would be omitted and they fetch the lastest app version available, which is the previous revision
A and B: If this happened, the other three would be affected as well
C: If code deploy agents were not installed, no version would be installed on the two instances

---

**promo286** (Fri 26 Jul 2024 16:40) - *Upvotes: 5*
D. EC2 Auto Scaling launched two new instances while the new deployment had not yet finished, causing the previous version to be deployed on the affected instances.

Explanation:

In an EC2 Auto Scaling group, when a deployment is in progress and new instances are launched, they may receive the previous version of the application if the deployment has not yet completed. This is because the new instances join the Auto Scaling group and need to fetch the latest revision during the deployment process. If the deployment has not finished when the new instances are launched, they will fetch the current revision available in the Auto Scaling group, which might be the previous version.

---

**govindrk** (Mon 12 Aug 2024 11:28) - *Upvotes: 1*
A. The explanation provided for D summarizes A- The two affected instances failed to fetch the new deployment.

---

**nickp84** (Mon 12 May 2025 17:52) - *Upvotes: 1*
B. Failed lifecycle event: Would cause a rollback, but that would typically be marked as a deployment failure, not success.

---

**youonebe** (Sun 22 Dec 2024 22:38) - *Upvotes: 2*
B is correct.

The AfterInstall lifecycle event is executed after the application revision is installed on the EC2 instance. If an error occurs during this phase, the CodeDeploy agent can trigger a rollback to the previous application version. Given that the deployment was successful for three instances and not for the others, it’s possible that the AfterInstall hook failed on the two instances that still have the previous version. In this case, the CodeDeploy agent would have automatically rolled back to the last successful application revision for those two instances.

why D is wrong? new instances launched by Auto Scaling would have the current revision applied during the deployment process, not the previous revision. Therefore, it’s unlikely that the new instances would have the previous version unless something else occurred.

---

**kyuhuck** (Sat 17 Aug 2024 18:26) - *Upvotes: 1*
, B. A failed AfterInstall lifecycle event hook caused the CodeDeploy agent to roll back to the previous version on the affected instances seems to be the most plausible explanation. It accounts for the scenario where the deployment was successful overall, but specific instances reverted to the previous application revision due to issues encountered during post-installation steps. It's important for the DevOps engineer to review the deployment logs, especially focusing on lifecycle event hooks and their outcomes, to confirm this hypothesis and take corrective actions.

---

**a54b16f** (Fri 12 Jul 2024 18:15) - *Upvotes: 4*
only D makes sense

---

**csG13** (Sat 29 Jun 2024 14:39) - *Upvotes: 5*
Got to be D. The rest choices would impact the entire ASG and not only two out of the five intances.

---

**PrasannaBalaji** (Sat 29 Jun 2024 12:03) - *Upvotes: 3*
D is correct
https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-auto-scaling.html#troubleshooting-auto-scaling-provision-termination-loo

---


<br/>

## Question 153

*Date: Dec. 29, 2023, 1:05 p.m.
Disclaimers:
- ExamTopics website is not rel*

A security team is concerned that a developer can unintentionally attach an Elastic IP address to an Amazon EC2 instance in production. No developer should be allowed to attach an Elastic IP address to an instance. The security team must be notified if any production server has an Elastic IP address at any time.

How can this task be automated?

**Options:**
- A. Use Amazon Athena to query AWS CloudTrail logs to check for any associate-address attempts. Create an AWS Lambda function to disassociate the Elastic IP address from the instance, and alert the security team.
- B. Attach an IAM policy to the developers' IAM group to deny associate-address permissions. Create a custom AWS Config rule to check whether an Elastic IP address is associated with any instance tagged as production, and alert the security team.
- C. Ensure that all IAM groups associated with developers do not have associate-address permissions. Create a scheduled AWS Lambda function to check whether an Elastic IP address is associated with any instance tagged as production, and alert the security team if an instance has an Elastic IP address associated with it.
- D. Create an AWS Config rule to check that all production instances have EC2 IAM roles that include deny associate-address permissions. Verify whether there is an Elastic IP address associated with any instance, and alert the security team if an instance has an Elastic IP address associated with it.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Fri 09 Feb 2024 16:07) - *Upvotes: 7*
B is correct: < Attach an IAM policy to the developers' IAM group to deny associate-address permissions>: means we can deny all address-assosiate attempts
A: AWS CloudTrail logs is used for monitoring users' actions. Though it would reveal associate-address attempts, it would not trigger AWS lambda to disassosiate the IPs
C: <Ensure that all IAM groups associated with developers do not have associate-address permissions>: This is unnecessary and can be done more easily with option B.
D: <check that all production instances have EC2 IAM roles>: We dont need to check the role of the EC2, we need to handle the role of developers.

Summary: D is irrelevant while A and C, though can achive the requirements, consume more efforts and resources.

---

**zijo** (Fri 03 Jan 2025 02:04) - *Upvotes: 1*
AWS Config provides the eip-attached managed rule to evaluate whether all allocated Elastic IPs are associated with a resource.

---

**Gomer** (Wed 26 Jun 2024 23:13) - *Upvotes: 2*
For what it's worth:
{
"Statement": [
{
"Action": [
"ec2:AssociateAddress",
"ec2:DisassociateAddress"
],
"Effect": "Deny",
"Resource": "*"
}
]
}

---

**dkp** (Sat 13 Apr 2024 11:43) - *Upvotes: 3*
answer B

---

**a54b16f** (Fri 12 Jan 2024 19:19) - *Upvotes: 3*
so easy, almost copy/paste from the two requirements listed inside the question

---

**csG13** (Fri 29 Dec 2023 15:42) - *Upvotes: 4*
It's B, the only who meets the question criteria.

---

**PrasannaBalaji** (Fri 29 Dec 2023 13:05) - *Upvotes: 3*
B is correct

---


<br/>

## Question 154

*Date: Dec. 29, 2023, 1:09 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is using AWS Organizations to create separate AWS accounts for each of its departments. The company needs to automate the following tasks:

• Update the Linux AMIs with new patches periodically and generate a golden image
• Install a new version of Chef agents in the golden image, if available
• Provide the newly generated AMIs to the department's accounts

Which solution meets these requirements with the LEAST management overhead?

**Options:**
- A. Write a script to launch an Amazon EC2 instance from the previous golden image. Apply the patch updates. Install the new version of the Chef agent, generate a new golden image, and then modify the AMI permissions to share only the new image with the department's accounts.
- B. Use Amazon EC2 Image Builder to create an image pipeline that consists of the base Linux AMI and components to install the Chef agent. Use AWS Resource Access Manager to share EC2 Image Builder images with the department's accounts.
- C. Use an AWS Systems Manager Automation runbook to update the Linux AMI by using the previous image. Provide the URL for the script that will update the Chef agent. Use AWS Organizations to replace the previous golden image in the department's accounts.
- D. Use Amazon EC2 Image Builder to create an image pipeline that consists of the base Linux AMI and components to install the Chef agent. Create a parameter in AWS Systems Manager Parameter Store to store the new AMI ID that can be referenced by the department's accounts.

> **Suggested Answer:** B
> **Community Vote:** B (86%), 9%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Fri 09 Aug 2024 15:16) - *Upvotes: 5*
B is correct: <generate a golden image> means we need EC2 image builder for an automated pipeline to build a golden image
A and C: no mention of EC2 image builder
D: This option utilizes SSM to share the image, which is not correct. We need AWS resource sharing to share resources cross-account

---

**a54b16f** (Fri 12 Jul 2024 18:24) - *Upvotes: 5*
Only B provided solution for sharing AMI image

---

**Jonalb** (Thu 10 Jul 2025 12:22) - *Upvotes: 1*
B is correct: <generate a golden image> means we need EC2 image builder for an automated pipeline to build a golden image

---

**Srikantha** (Sun 30 Mar 2025 12:09) - *Upvotes: 2*
it automates the AMI creation process and provides a low-maintenance way for departments to access the latest AMI ID.

---

**c3518fc** (Wed 23 Oct 2024 19:54) - *Upvotes: 4*
By leveraging EC2 Image Builder and RAM, solution B provides a fully automated and centralized approach to creating, updating, and sharing golden images with the department's accounts, minimizing manual effort and management overhead.

---

**dkp** (Sun 13 Oct 2024 11:57) - *Upvotes: 3*
ill go with B

---

**DanShone** (Mon 16 Sep 2024 14:44) - *Upvotes: 3*
B - The function of Amazon EC2 Image Builder is to build you golden AMIs

---

**denccc** (Wed 17 Jul 2024 17:42) - *Upvotes: 2*
This should be B

---

**davdan99** (Wed 10 Jul 2024 11:27) - *Upvotes: 4*
Going for B

---

**d262e67** (Sun 30 Jun 2024 08:19) - *Upvotes: 5*
Builder Image to streamline the AMI baking process and use RAM to easily share the AMI among the whole organization or select accounts.

https://docs.aws.amazon.com/imagebuilder/latest/userguide/manage-shared-resources.html#manage-shared-resources-using

---


<br/>

## Question 155

*Date: Dec. 29, 2023, 1:12 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has a mission-critical application on AWS that uses automatic scaling. The company wants the deployment lifecycle to meet the following parameters:

• The application must be deployed one instance at a time to ensure the remaining fleet continues to serve traffic.
• The application is CPU intensive and must be closely monitored.
• The deployment must automatically roll back if the CPU utilization of the deployment instance exceeds 85%.

Which solution will meet these requirements?

**Options:**
- A. Use AWS CloudFormation to create an AWS Step Functions state machine and Auto Scaling lifecycle hooks to move to one instance at a time into a wait state. Use AWS Systems Manager automation to deploy the update to each instance and move it back into the Auto Scaling group using the heartbeat timeout.
- B. Use AWS CodeDeploy with Amazon EC2 Auto Scaling Configure an alarm tied to the CPU utilization metric. Use the CodeDeployDefault OneAtAtime configuration as a deployment strategy. Configure automatic rollbacks within the deployment group to roll back the deployment if the alarm thresholds are breached.
- C. Use AWS Elastic Beanstalk for load balancing and AWS Auto Scaling. Configure an alarm tied to the CPU utilization metric. Configure rolling deployments with a fixed batch size of one instance. Enable enhanced health to monitor the status of the deployment and roll back based on the alarm previously created.
- D. Use AWS Systems Manager to perform a blue/green deployment with Amazon EC2 Auto Scaling. Configure an alarm tied to the CPU utilization metric. Deploy updates one at a time. Configure automatic rollbacks within the Auto Scaling group to roll back the deployment if the alarm thresholds are breached.

> **Suggested Answer:** B
> **Community Vote:** B (95%), 5%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Fri 09 Feb 2024 17:22) - *Upvotes: 5*
B is correct: < must be deployed one instance at a time> means codedeploy, which provides this option
A: AWS Step Functions state machine does not provide deployment Functions
C: Beanstalk does not work with EC2
D: AWS SSM does not provides deployment Functions

---

**tinyshare** (Mon 25 Nov 2024 06:16) - *Upvotes: 1*
Why not C? Beanstalk can use CPU utilization metric for Auto Scaling.

---

**tinyshare** (Mon 25 Nov 2024 06:30) - *Upvotes: 3*
Sorry still B
Although Beanstalk can use CPU utilization to auto scale
Beanstalk can not use CPU utilization to roll back. The roll back is a manual re-deployment of the latest working version.

---

**jamesf** (Mon 29 Jul 2024 11:08) - *Upvotes: 2*
B correct - CodeDeployDefault OneAtAtime configuration (One at a time, but not AllAtATime)

---

**Gomer** (Thu 27 Jun 2024 00:27) - *Upvotes: 3*
"B"
"You can now monitor and automatically react to changes in your AWS CodeDeploy deployments using Amazon CloudWatch alarms."
"Using CloudWatch with CodeDeploy, you can monitor metrics for Amazon EC2 instances or Auto Scaling groups being managed by CodeDeploy and then invoke an action if the metric you are tracking crosses a certain threshold for a defined period of time."
"You can monitor metrics such as instance CPU utilization."
"If the alarm is activated, CloudWatch initiates actions such as sending a notification to Amazon Simple Notification Service, stopping a CodeDeploy deployment, or changing the state of an instance (e.g. reboot, terminate, recover)."
"CodeDeploy now also lets you automatically roll back a deployment when a deployment fails or when a CloudWatch alarm is activated."
https://aws.amazon.com/about-aws/whats-new/2016/09/aws-codedeploy-introduces-deployment-monitoring-with-amazon-cloudwatch-alarms-and-automatic-deployment-rollback/

---

**c3518fc** (Tue 23 Apr 2024 19:56) - *Upvotes: 3*
By using AWS CodeDeploy with Amazon EC2 Auto Scaling, configuring the CodeDeployDefault.OneAtAtime deployment strategy, and setting up automatic rollbacks based on a CloudWatch alarm for CPU utilization, this solution meets all the specified requirements. It ensures a controlled deployment process, monitors the CPU-intensive application, and automatically rolls back the deployment if the CPU utilization threshold is breached, providing a reliable and automated deployment lifecycle for the mission-critical application.

---

**dkp** (Sat 13 Apr 2024 12:03) - *Upvotes: 3*
answer B

---

**davdan99** (Wed 10 Jan 2024 12:37) - *Upvotes: 3*
Why not BeansTalk?

---

**tinyshare** (Mon 25 Nov 2024 06:36) - *Upvotes: 1*
Beanstalk can not use CPU utilization to roll back. The roll back is a manual re-deployment of the latest working version.
https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html

---

**csG13** (Fri 29 Dec 2023 15:53) - *Upvotes: 4*
It's B - the only one that fulfils all the requirements.

---


<br/>

## Question 156

*Date: Dec. 29, 2023, 1:15 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has a single developer writing code for an automated deployment pipeline. The developer is storing source code in an Amazon S3 bucket for each project. The company wants to add more developers to the team but is concerned about code conflicts and lost work. The company also wants to build a test environment to deploy newer versions of code for testing and allow developers to automatically deploy to both environments when code is changed in the repository.

What is the MOST efficient way to meet these requirements?

**Options:**
- A. Create an AWS CodeCommit repository for each project, use the main branch for production code, and create a testing branch for code deployed to testing. Use feature branches to develop new features and pull requests to merge code to testing and main branches.
- B. Create another S3 bucket for each project for testing code, and use an AWS Lambda function to promote code changes between testing and production buckets. Enable versioning on all buckets to prevent code conflicts.
- C. Create an AWS CodeCommit repository for each project, and use the main branch for production and test code with different deployment pipelines for each environment. Use feature branches to develop new features.
- D. Enable versioning and branching on each S3 bucket, use the main branch for production code, and create a testing branch for code deployed to testing. Have developers use each branch for developing in each environment.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Fri 09 Aug 2024 16:26) - *Upvotes: 5*
A is correct: < The developer is storing source code in an Amazon S3 bucket for each project> and < The company wants to add more developers to the team but is concerned about code conflicts and lost work> means we need codecommit
B and D: no mention of code commnit
C: Should not use the main branch for both production and test

---

**c3518fc** (Wed 23 Oct 2024 19:59) - *Upvotes: 3*
By leveraging AWS CodeCommit and following Git branching best practices, the company can efficiently manage code changes, facilitate collaboration among developers, and automate deployments to both production and testing environments. This solution provides a scalable and organized approach to software development and deployment, while minimizing the risk of code conflicts and lost work.

---

**dkp** (Sun 13 Oct 2024 13:29) - *Upvotes: 3*
answer is A

---

**DanShone** (Mon 16 Sep 2024 14:41) - *Upvotes: 3*
A is correct

---

**a54b16f** (Fri 12 Jul 2024 18:32) - *Upvotes: 3*
typical branching strategy

---

**d262e67** (Sun 30 Jun 2024 08:31) - *Upvotes: 4*
Definitely A. C lacks proper strategy for pull requests for code merging. In general, it's a proper patter in software development.

---

**csG13** (Sat 29 Jun 2024 14:58) - *Upvotes: 4*
It's A, since C suggests to merge to main branch non-production code. Although it's a working pattern, A is definitely more industry standard and safer.

---

**PrasannaBalaji** (Sat 29 Jun 2024 12:15) - *Upvotes: 2*
Correct answer is A.
S3 not good choice , so eliminating B & D

---


<br/>

## Question 157

*Date: Dec. 29, 2023, 1:25 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer notices that all Amazon EC2 instances running behind an Application Load Balancer in an Auto Scaling group are failing to respond to user requests. The EC2 instances are also failing target group HTTP health checks.

Upon inspection, the engineer notices the application process was not running in any EC2 instances. There are a significant number of out of memory messages in the system logs. The engineer needs to improve the resilience of the application to cope with a potential application memory leak. Monitoring and notifications should be enabled to alert when there is an issue.

Which combination of actions will meet these requirements? (Choose two.)

**Options:**
- A. Change the Auto Scaling configuration to replace the instances when they fail the load balancer's health checks.
- B. Change the target group health check HealthCheckIntervalSeconds parameter to reduce the interval between health checks.
- C. Change the target group health checks from HTTP to TCP to check if the port where the application is listening is reachable.
- D. Enable the available memory consumption metric within the Amazon CloudWatch dashboard for the entire Auto Scaling group. Create an alarm when the memory utilization is high. Associate an Amazon SNS topic to the alarm to receive notifications when the alarm goes off.
- E. Use the Amazon CloudWatch agent to collect the memory utilization of the EC2 instances in the Auto Scaling group. Create an alarm when the memory utilization is high and associate an Amazon SNS topic to receive a notification.

> **Suggested Answer:** AE
> **Community Vote:** AE (87%), 13%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Fri 09 Aug 2024 17:40) - *Upvotes: 5*
A and E are correct: If there is failed instance, we should replace it with a new one to restart. Use cloudwatch agent to monitor metrics
B: this does not do anything
C: should not change from HTTP to TCP
D: Cloudwatch work with agents, about which this option does not mention

---

**lovekiller** (Tue 04 Feb 2025 13:51) - *Upvotes: 1*
First choice is A, that is sure because one requirement is resiliency.

For second choice, D vs E:
By default, CloudWatch metrics for EC2 do not include memory utilization. We must install the CloudWatch agent to collect memory metrics which is mentioned in E. So E is correct.

---

**zijo** (Fri 03 Jan 2025 23:58) - *Upvotes: 2*
An Auto Scaling Group (ASG) can replace EC2 instances by default when Application Load Balancer (ALB) health checks fail, but only if ALB health checks are explicitly enabled in the ASG configuration. By default, an ASG uses EC2 instance status checks for health monitoring. If you want the ASG to replace instances based on ALB health check results, you need to configure it to use ELB/ALB health checks.

---

**xdkonorek2** (Mon 04 Nov 2024 20:36) - *Upvotes: 4*
from doc:

When Amazon EC2 Auto Scaling determines that an InService instance is unhealthy, it replaces it with a new instance to maintain the desired capacity of the group.

so you can't actually have a ASG that doesn't replace unhealthy instance so A doesn't make sense, but B makes sense since faster replacement will improve the resilience.

---

**dkp** (Sun 13 Oct 2024 13:34) - *Upvotes: 3*
Answer is A&E

---

**DanShone** (Mon 16 Sep 2024 14:40) - *Upvotes: 3*
A - Autoscaling to replace teh EC2s
E - CloudWatch agent to monitor Memory

---

**Jaguaroooo** (Sat 14 Sep 2024 08:11) - *Upvotes: 2*
I choose AE, but I am not comfortable with E, because CW doesn't by default give memory metrics, it requires customization.

---

**a54b16f** (Fri 12 Jul 2024 18:35) - *Upvotes: 3*
It's A & E

---

**davdan99** (Wed 10 Jul 2024 12:18) - *Upvotes: 3*
We don't have memory metrics for autoscaling, here is the list of metrics
https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/appinsights-metrics-ec2.html#appinsights-metrics-ec2-linux

Here is the list of metrics from cloudwatch agent

https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/appinsights-metrics-ec2.html#appinsights-metrics-ec2-linux

---

**d262e67** (Sun 30 Jun 2024 08:34) - *Upvotes: 2*
Memory monitoring requires an agent and auto scaling needs the self-healing feature turned on.

---


<br/>

## Question 158

*Date: Dec. 29, 2023, 1:37 p.m.
Disclaimers:
- ExamTopics website is not rel*

An ecommerce company uses a large number of Amazon Elastic Block Store (Amazon EBS) backed Amazon EC2 instances. To decrease manual work across all the instances, a DevOps engineer is tasked with automating restart actions when EC2 instance retirement events are scheduled.

How can this be accomplished?

**Options:**
- A. Create a scheduled Amazon EventBridge rule to run an AWS Systems Manager Automation runbook that checks if any EC2 instances are scheduled for retirement once a week. If the instance is scheduled for retirement, the runbook will hibernate the instance.
- B. Enable EC2 Auto Recovery on all of the instances. Create an AWS Config rule to limit the recovery to occur during a maintenance window only.
- C. Reboot all EC2 instances during an approved maintenance window that is outside of standard business hours. Set up Amazon CloudWatch alarms to send a notification in case any instance is failing EC2 instance status checks.
- D. Set up an AWS Health Amazon EventBridge rule to run AWS Systems Manager Automation runbooks that stop and start the EC2 instance when a retirement scheduled event occurs.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Fri 09 Aug 2024 17:53) - *Upvotes: 7*
D is correct: <automating restart actions when EC2 instance retirement events are scheduled>: these even occurs when the underlying infra that runs the instances needs to be repaired. To deal with this, just need to stop and start the instance to re-locate the instance to a different physical machine
A: Hibernate would not work for this scenario
B: This would also bring back the instance. However, AWS config rule cannot limit the recovery. It only reports, not actions
C: This option is a manual work, not automated one

---

**DanShone** (Mon 16 Sep 2024 14:38) - *Upvotes: 5*
D - Retirement relates to AWS Health

---

**c3518fc** (Thu 24 Oct 2024 04:31) - *Upvotes: 5*
By leveraging AWS Health events, Amazon EventBridge, and AWS Systems Manager Automation runbooks, you can create an automated and event-driven solution that responds to EC2 instance retirement events in a timely and consistent manner, minimizing manual effort and reducing the risk of service disruptions.

---

**dkp** (Sun 13 Oct 2024 13:53) - *Upvotes: 3*
answer D

---

**sarlos** (Fri 02 Aug 2024 02:11) - *Upvotes: 2*
D is correct

---

**a54b16f** (Fri 12 Jul 2024 18:36) - *Upvotes: 3*
retirement is tied with Health

---

**kabary** (Sun 30 Jun 2024 21:22) - *Upvotes: 3*
Answer is D. Once a week is a joke :D

---

**csG13** (Sat 29 Jun 2024 15:06) - *Upvotes: 3*
It 's D

---

**PrasannaBalaji** (Sat 29 Jun 2024 12:37) - *Upvotes: 3*
D is correct
https://aws.amazon.com/blogs/mt/automate-remediation-actions-for-amazon-ec2-notifications-and-beyond-using-ec2-systems-manager-automation-and-aws-health/

---


<br/>

## Question 159

*Date: Dec. 29, 2023, 4:10 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company manages AWS accounts for application teams in AWS Control Tower. Individual application teams are responsible for securing their respective AWS accounts.

A DevOps engineer needs to enable Amazon GuardDuty for all AWS accounts in which the application teams have not already enabled GuardDuty. The DevOps engineer is using AWS CloudFormation StackSets from the AWS Control Tower management account.

How should the DevOps engineer configure the CloudFormation template to prevent failure during the StackSets deployment?

**Options:**
- A. Create a CloudFormation custom resource that invokes an AWS Lambda function. Configure the Lambda function to conditionally enable GuardDuty if GuardDuty is not already enabled in the accounts.
- B. Use the Conditions section of the CloudFormation template to enable GuardDuty in accounts where GuardDuty is not already enabled.
- C. Use the CloudFormation Fn::GetAtt intrinsic function to check whether GuardDuty is already enabled. If GuardDuty is not already enabled, use the Resources section of the CloudFormation template to enable GuardDuty.
- D. Manually discover the list of AWS account IDs where GuardDuty is not enabled. Use the CloudFormation Fn::ImportValue intrinsic function to import the list of account IDs into the CloudFormation template to skip deployment for the listed AWS accounts.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Fri 09 Aug 2024 18:25) - *Upvotes: 7*
A is correct: <configure the CloudFormation template> is the requirement of the question. By default, cloudformation doesnot support turning on Guarduty. To turn it on, need to use ACF template in combination with lambda.
A: perfectly correct
B: no mention of lambda
C: Fn::GetAtt intrinsic: This is used to check only. No mention of using lambda to enable Guarduty
D: This might work. However, a manual approach is not recommeded

---

**d262e67** (Sun 30 Jun 2024 08:40) - *Upvotes: 5*
A is correct. Conditions are designed to decide whether or not create resources. GetAtt is to retrieve the value of an attribute from a resource in the same template. and manual processes are usually not good.

---

**dkp** (Sun 13 Oct 2024 13:59) - *Upvotes: 2*
answer A

---

**DanShone** (Mon 16 Sep 2024 14:36) - *Upvotes: 2*
A is correct

---

**a54b16f** (Fri 12 Jul 2024 18:38) - *Upvotes: 2*
standard pattern: use lambda to conditional DO something

---

**PrasannaBalaji** (Sun 30 Jun 2024 13:18) - *Upvotes: 2*
A is correct

---

**csG13** (Sat 29 Jun 2024 15:10) - *Upvotes: 5*
It's a standard pattern, so A

Here is a reference:
https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/enable-amazon-guardduty-conditionally-by-using-aws-cloudformation-templates.html

---


<br/>

## Question 160

*Date: Dec. 30, 2023, 6:02 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an AWS Control Tower landing zone. The company's DevOps team creates a workload OU. A development OU and a production OU are nested under the workload OU. The company grants users full access to the company's AWS accounts to deploy applications.

The DevOps team needs to allow only a specific management IAM role to manage the IAM roles and policies of any AWS accounts in only the production OU.

Which combination of steps will meet these requirements? (Choose two.)

**Options:**
- A. Create an SCP that denies full access with a condition to exclude the management IAM role for the organization root.
- B. Ensure that the FullAWSAccess SCP is applied at the organization root.
- C. Create an SCP that allows IAM related actions. Attach the SCP to the development OU.
- D. Create an SCP that denies IAM related actions with a condition to exclude the management IAM role. Attach the SCP to the workload OU.
- E. Create an SCP that denies IAM related actions with a condition to exclude the management IAM role. Attach the SCP to the production OU.

> **Suggested Answer:** BE
> **Community Vote:** BE (93%), 7%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**d262e67** (Sun 31 Dec 2023 09:54) - *Upvotes: 11*
You need to understand how SCP inheritance works in AWS. The way it works for Deny policies is different that allow policies.

Allow polices are passing down to children ONLY if they don't have an allow policy.

Deny policies always pass down to children.

That's why there is always an SCP set to the Root to allow everything by default. If you limit this policy, the whole organization will be limited, not matter what other policies are saying for the other OUs. So it's not A. It's not D because it restricts the wrong OU.

---

**MalonJay** (Wed 08 May 2024 15:25) - *Upvotes: 1*
CE
FullAWSAccess is applied be default, no need to check it since the question did not say it has been removed.
For an Action to be permitted it has to be allowed from the Root OUs all the way to the accounts.

---

**tinyshare** (Mon 25 Nov 2024 07:30) - *Upvotes: 1*
The organization root is NOT the top level OU. The question specifies the workload OU has full access, but not the organization root. So you still need B.

---

**dkp** (Sat 13 Apr 2024 14:03) - *Upvotes: 2*
ANS: B&E

---

**DanShone** (Sat 16 Mar 2024 15:35) - *Upvotes: 2*
B & E are correct

---

**Ramdi1** (Tue 13 Feb 2024 15:03) - *Upvotes: 3*
B and E are correct because he requirement for dev ou user should still be able to do what they need to

---

**thanhnv142** (Fri 09 Feb 2024 19:33) - *Upvotes: 3*
B and E are correct:
A: this does not make sense. It would mess with permissions for all OUs
C: The question requires <only the production OU>: we need to target the production OU, not development OU
D: <Attach the SCP to the workload OU>: we need to target only the production OU. This option affects both dev and prod OUS

---

**denccc** (Sat 20 Jan 2024 12:19) - *Upvotes: 1*
B & E it is

---

**a54b16f** (Fri 12 Jan 2024 19:43) - *Upvotes: 2*
A is wrong, we only want to limit production OU, development OU users should be able to do anything

---

**kabary** (Sun 31 Dec 2023 22:50) - *Upvotes: 2*
Answer is B & E.

A is not correct because it would prevent the developers team to access the Developer OU. That wouldn't make sense.

---


<br/>

## Question 161

*Date: Dec. 29, 2023, 4:32 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company hired a penetration tester to simulate an internal security breach. The tester performed port scans on the company's Amazon EC2 instances. The company's security measures did not detect the port scans.

The company needs a solution that automatically provides notification when port scans are performed on EC2 instances. The company creates and subscribes to an Amazon Simple Notification Service (Amazon SNS) topic.

What should the company do next to meet the requirement?

**Options:**
- A. Ensure that Amazon GuardDuty is enabled. Create an Amazon CloudWatch alarm for detected EC2 and port scan findings. Connect the alarm to the SNS topic.
- B. Ensure that Amazon Inspector is enabled. Create an Amazon EventBridge event for detected network reachability findings that indicate port scans. Connect the event to the SNS topic.
- C. Ensure that Amazon Inspector is enabled. Create an Amazon EventBridge event for detected CVEs that cause open port vulnerabilities. Connect the event to the SNS topic.
- D. Ensure that AWS CloudTrail is enabled. Create an AWS Lambda function to analyze the CloudTrail logs for unusual amounts of traffic from an IP address range. Connect the Lambda function to the SNS topic.

> **Suggested Answer:** A
> **Community Vote:** A (96%), 4%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**YucelFuat** (Sat 07 Sep 2024 21:50) - *Upvotes: 2*
- GuardDuty is focused on real-time threat detection and alerting, while Inspector is focused on vulnerability scanning and remediation.
- GuardDuty operates continuously in the background, whereas Inspector is typically run on-demand or scheduled for specific workloads.

---

**flaacko** (Wed 21 Aug 2024 15:34) - *Upvotes: 1*
The Answer is A. C is wrong because while you can use Inspector to detect open ports and software vulnerabilities, you can't use it to detect port scanning.

---

**Gomer** (Fri 28 Jun 2024 02:32) - *Upvotes: 2*
Per ChatGPT "AWS offers several services and features that can help detect port scans:"
"GuardDuty" (using VPC Flow Logs), "WAF", and "Network Firewall"
Was able to also provide references
https://aws.amazon.com/blogs/aws/amazon-guardduty-continuous-security-monitoring-threat-detection/

---

**that1guy** (Tue 07 May 2024 15:02) - *Upvotes: 1*
Bad question.

Although you can do it via GuardDuty, the answer doesn't mention the required VPC flow logs.
There is no mention online of how to create a CloudWatch ALARM for GuardDuty only CloudWatch events.

---

**dkp** (Sat 13 Apr 2024 14:13) - *Upvotes: 2*
answer a is correct

---

**thanhnv142** (Sun 11 Feb 2024 09:48) - *Upvotes: 4*
A is correct: To detect port scans in real time, we need Guarduty, not inspector
B, C and D: no mention of Guarduty

---

**a54b16f** (Fri 12 Jan 2024 19:45) - *Upvotes: 4*
only GuardDuty would detect port scanning activities

---

**davdan99** (Wed 10 Jan 2024 14:01) - *Upvotes: 2*
https://medium.com/aws-architech/use-case-aws-inspector-vs-guardduty-3662bf80767a

---

**kabary** (Sun 31 Dec 2023 23:37) - *Upvotes: 2*
GuardDuty should be the answer as it best detects whether a port scan has happened on an EC2 instances; we don't care about whether the port is open or not, we care if it was scanned.

---

**d262e67** (Sun 31 Dec 2023 10:48) - *Upvotes: 4*
Inspector is designed to find vulnerabilities across EC2 servers and detect open ports. It doesn't detect port scans against EC2 servers. The reachability analyzer mentioned below is the port scanner itself. I doesn't detect other port scanners.

https://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/

GuardDuty on the other hand draws upon traffic logs to find specious activities such as port scans in a form of a finding.

---


<br/>

## Question 162

*Date: Dec. 29, 2023, 1:53 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company runs applications in an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster uses an Application Load Balancer to route traffic to the applications that run in the cluster.

A new application that was migrated to the EKS cluster is performing poorly. All the other applications in the EKS cluster maintain appropriate operation. The new application scales out horizontally to the preconfigured maximum number of pods immediately upon deployment, before any user traffic routes to the web application.

Which solution will resolve the scaling behavior of the web application in the EKS cluster?

**Options:**
- A. Implement the Horizontal Pod Autoscaler in the EKS cluster.
- B. Implement the Vertical Pod Autoscaler in the EKS cluster.
- C. Implement the Cluster Autoscaler.
- D. Implement the AWS Load Balancer Controller in the EKS cluster.

> **Suggested Answer:** B
> **Community Vote:** B (62%), A (35%), 3%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**vmahilevskyi** (Thu 21 Mar 2024 08:45) - *Upvotes: 11*
In my opinion A is incorrect because "The new application scales out horizontally to the preconfigured maximum number of pods immediately upon deployment" means that the Horizontal Pod Autoscaler is already implemented but doesn't resolve the issue with poor performance. This may indicate inappropriate resource allocation.
But Vertical Pod Autoscaler will help us "right size" our application.
So, for me it's B.

---

**moonj** (Fri 09 May 2025 03:08) - *Upvotes: 2*
But the question is asking "Which solution will resolve the scaling behavior" but not for the performance, then should be A?

---

**a54b16f** (Mon 15 Jan 2024 21:13) - *Upvotes: 8*
scaled out to maximum when there is no user traffic: this means that the configured pod instance is wrong-sized, for example, need more memory or CPU.

---

**ryuhei** (Sun 24 Aug 2025 06:13) - *Upvotes: 1*
Answer is A

---

**nickp84** (Mon 12 May 2025 18:56) - *Upvotes: 1*
B. Vertical Pod Autoscaler adjusts the CPU/memory resources per pod, not the number of pods. It won’t solve premature scaling out.

---

**GripZA** (Sun 20 Apr 2025 20:17) - *Upvotes: 1*
HPA dynamically adjusts the number of pods based on actual metrics, like CPU utilization, memory, or custom metrics. in this case, the app is likely using a static replica count, or has a misconfigured initial replica setting or resource requests that trigger autoscaling early. implementing HPA would ensure pods scale only when needed, based on real usage, e.g CPU > 70%

---

**Srikantha** (Sun 30 Mar 2025 12:24) - *Upvotes: 1*
If scaled "Before" traffic is flew, then option A to control autoscaling

---

**DKM** (Mon 17 Mar 2025 19:26) - *Upvotes: 1*
Enable Cluster Autoscaler:

Ensure that the Cluster Autoscaler is enabled and configured to scale the number of nodes based on the actual demand.
Verify that the Cluster Autoscaler is not causing unnecessary scaling by reviewing its configuration and logs.

---

**spring21** (Sun 08 Dec 2024 21:12) - *Upvotes: 2*
For stateless applications with variable traffic, start with HPA.
For stateful or resource-intensive applications, start with VPA.

---

**VerRi** (Wed 06 Nov 2024 08:28) - *Upvotes: 3*
The keywords here are "immediately upon deployment"
If the pods scale to the max immediately, it indicates that the pods may not have enough resources to handle the workload

---


<br/>

## Question 163

*Date: Dec. 31, 2023, 11:25 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an AWS Control Tower landing zone that manages its organization in AWS Organizations. The company created an OU structure that is based on the company's requirements. The company's DevOps team has established the core accounts for the solution and an account for all centralized AWS CloudFormation and AWS Service Catalog solutions.

The company wants to offer a series of customizations that an account can request through AWS Control Tower.

Which combination of steps will meet these requirements? (Choose three.)

**Options:**
- A. Enable trusted access for CloudFormation with Organizations by using service-managed permissions.
- B. Create an IAM role that is named AWSControlTowerBlueprintAccess. Configure the role with a trust policy that allows the AWSControlTowerAdmin role in the management account to assume the role. Attach the AWSServiceCatalogAdminFullAccess IAM policy to the AWSControlTowerBlueprintAccess role.
- C. Create a Service Catalog product for each CloudFormation template.
- D. Create a CloudFormation stack set for each CloudFormation template. Enable automatic deployment for each stack set. Create a CloudFormation stack instance that targets specific OUs.
- E. Deploy the Customizations for AWS Control Tower (CfCT) CloudFormation stack.
- F. Create a CloudFormation template that contains the resources for each customization.

> **Suggested Answer:** BCF
> **Community Vote:** BCF (76%), 8%, Other, Other, A (35%), C (25%), B (20%), Other

### Discussions

**0ac7838** (Sat 08 Nov 2025 08:40) - *Upvotes: 1*
Correct trio: E, F, C
E – Deploy the CfCT stack once; that’s the engine Control Tower uses to accept custom templates.
F – Build the actual CloudFormation template(s) that hold your customization resources.
C – Wrap each template in a Service Catalog product; CfCT pulls products from the central portfolio and lets account owners request them through Control Tower.
Why the others don’t fit:
A – Not needed; CfCT already turns on trusted access for you.
B – BlueprintAccess role is created automatically by CfCT; hand-building it is redundant.
D – StackSets are handled under the hood by CfCT; you don’t create them yourself.

---

**nickp84** (Mon 12 May 2025 19:06) - *Upvotes: 1*
B. Create an IAM role named AWSControlTowerBlueprintAccess...
Required to enable CfCT to deploy customizations to accounts.
The trust relationship ensures that the management account can assume the role in target accounts for customization deployments.
E. Deploy the CfCT CloudFormation stack
This is the core mechanism for enabling customizations across your Control Tower-managed environment.
CfCT provisions and manages CloudFormation templates and optionally Service Catalog products across OUs and accounts.
F. Create a CloudFormation template for each customization
These templates define the infrastructure or policies you want to roll out to Control Tower accounts.

---

**Srikantha** (Sun 30 Mar 2025 12:31) - *Upvotes: 1*
This approach ensures automation, governance, and self-service capability with the least operational overhead

---

**Slays** (Fri 13 Dec 2024 13:00) - *Upvotes: 1*
CfCT provides a framework that automates the deployment of custom resources and policies across your AWS Control Tower environment.

Creating and deploying individual CloudFormation templates for each customization requires significant manual effort.

---

**VerRi** (Wed 06 Nov 2024 09:25) - *Upvotes: 1*
"customisations that an account can request"
It is an ongoing customisation, CfCt does better than AFC.

---

**awsarchitect5** (Thu 31 Oct 2024 04:57) - *Upvotes: 1*
CfCT provides predefined stacksets that extend Control Tower. Offers a series of customization.

---

**hzaki** (Thu 25 Jul 2024 12:24) - *Upvotes: 1*
I think the correct answer is ACE.
1- Enable trusted access for CloudFormation with Organizations to allow cross-account deployments.
2- Create Service Catalog products for each CloudFormation template to provide a self-service portal.
3- Deploy the Customizations for AWS Control Tower (CfCT) stack to automate and manage customizations across your organization.

---

**Gomer** (Fri 28 Jun 2024 06:09) - *Upvotes: 4*
"Steps to set up Account Factory for the customization process":
1. "Create the required role...."
- "The role must be named AWSControlTowerBlueprintAccess."
- "The AWSControlTowerBlueprintAccess role must be set up to grant trust to" [...]
- "The role named AWSControlTowerAdmin in the AWS Control Tower management account."
- AWS Control Tower requires that the managed policy named AWSServiceCatalogAdminFullAccess must be attached to the AWSControlTowerBlueprintAccess role. (Required permissions policy)
2. "Create the AWS Service Catalog product..."
[...]
https://docs.aws.amazon.com/controltower/latest/userguide/af-customization-page.html
https://docs.aws.amazon.com/controltower/latest/userguide/afc-setup-steps.html

---

**dkp** (Sat 13 Apr 2024 23:06) - *Upvotes: 1*
ans: BCF

---

**WhyIronMan** (Sat 30 Mar 2024 23:19) - *Upvotes: 1*
B,C,F is the only answer

---


<br/>

## Question 164

*Date: Dec. 29, 2023, 4:58 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company runs a workload on Amazon EC2 instances. The company needs a control that requires the use of Instance Metadata Service Version 2 (IMDSv2) on all EC2 instances in the AWS account. If an EC2 instance does not prevent the use of Instance Metadata Service Version 1 (IMDSv1), the EC2 instance must be terminated.

Which solution will meet these requirements?

**Options:**
- A. Set up AWS Config in the account. Use a managed rule to check EC2 instances. Configure the rule to remediate the findings by using AWS Systems Manager Automation to terminate the instance.
- B. Create a permissions boundary that prevents the ec2:RunInstance action if the ec2:MetadataHttpTokens condition key is not set to a value of required. Attach the permissions boundary to the IAM role that was used to launch the instance.
- C. Set up Amazon Inspector in the account. Configure Amazon Inspector to activate deep inspection for EC2 instances. Create an Amazon EventBridge rule for an Inspector2 finding. Set an AWS Lambda function as the target to terminate the instance.
- D. Create an Amazon EventBridge rule for the EC2 instance launch successful event. Send the event to an AWS Lambda function to inspect the EC2 metadata and to terminate the instance.

> **Suggested Answer:** A
> **Community Vote:** A (94%), 6%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**csG13** (Fri 29 Dec 2023 16:58) - *Upvotes: 8*
AWS Config can do this using the managed ec2-imdsv2-check rule.

Here is a reference:
https://docs.aws.amazon.com/config/latest/developerguide/ec2-imdsv2-check.html

---

**0ac7838** (Sat 08 Nov 2025 08:53) - *Upvotes: 1*
A works, but it’s detective + reactive (launch → check → terminate).

B is preventive (block the launch if IMDSv1 is allowed), so zero bad instances ever exist—cleaner, faster, and exactly what AWS marks as best-practice for this exam scenario.

Pick B for the “most correct” answer; Config rule is a valid fallback, not the primary control AWS wants you to choose here.

---

**flaacko** (Wed 21 Aug 2024 15:49) - *Upvotes: 1*
Using the ec2-imdsv2-check AWS Config managed rule, you can Checks if your Amazon EC2 instance metadata version is configured with Instance Metadata Service Version 2 (IMDSv2).

---

**c3518fc** (Wed 24 Apr 2024 16:50) - *Upvotes: 2*
AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. It provides managed rules that you can use to evaluate the configuration settings of your resources against desired configurations.
In this case, you can use the AWS Config managed rule "ec2-imdsv2-check" to evaluate whether your EC2 instances are using the Instance Metadata Service Version 2 (IMDSv2) or not. This rule checks if the EC2 instances have the HTTP token request enabled for the Instance Metadata Service (IMDS), which is a requirement for using IMDSv2.
If an EC2 instance is found to be non-compliant with the rule (i.e., not using IMDSv2), AWS Config can be configured to automatically remediate the non-compliant resource. You can set up AWS Systems Manager Automation to terminate the non-compliant EC2 instance as the remediation action.

---

**DanShone** (Sat 16 Mar 2024 14:53) - *Upvotes: 1*
A - AWS Config

---

**DanShone** (Sat 16 Mar 2024 14:52) - *Upvotes: 1*
A - AWS Config

---

**thanhnv142** (Sun 11 Feb 2024 12:53) - *Upvotes: 3*
A is correct: use Config to monitor and SSM Automation to terminate instances
B: permission boundary cannot spot the need-to-terminate instances
C: Inspector is for vul scanning
D: EC2 instance launch successful event wont provide sufficient information

---

**kabary** (Mon 01 Jan 2024 00:03) - *Upvotes: 1*
Answer A

---

**d262e67** (Sun 31 Dec 2023 11:31) - *Upvotes: 1*
Only viable option

---


<br/>

## Question 165

*Date: Dec. 29, 2023, 5:07 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company builds an application that uses an Application Load Balancer in front of Amazon EC2 instances that are in an Auto Scaling group. The application is stateless. The Auto Scaling group uses a custom AMI that is fully prebuilt. The EC2 instances do not have a custom bootstrapping process.

The AMI that the Auto Scaling group uses was recently deleted. The Auto Scaling group's scaling activities show failures because the AMI ID does not exist.

Which combination of steps should a DevOps engineer take to meet these requirements? (Choose three.)

**Options:**
- A. Create a new launch template that uses the new AMI.
- B. Update the Auto Scaling group to use the new launch template.
- C. Reduce the Auto Scaling group's desired capacity to 0.
- D. Increase the Auto Scaling group's desired capacity by 1.
- E. Create a new AMI from a running EC2 instance in the Auto Scaling group.
- F. Create a new AMI by copying the most recent public AMI of the operating system that the EC2 instances use.

> **Suggested Answer:** ABE
> **Community Vote:** ABE (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Tue 30 Jul 2024 04:38) - *Upvotes: 1*
ABE
other options not related to solve problem

---

**dkp** (Sat 13 Apr 2024 23:15) - *Upvotes: 2*
ans ABE

---

**WhyIronMan** (Sat 30 Mar 2024 23:23) - *Upvotes: 1*
A,B,E
The others do not actually help solve the problem

---

**DanShone** (Sat 16 Mar 2024 14:25) - *Upvotes: 2*
ABE
E - Create new AMI from existing EC2 (we can do this as they are stateless)
A - New lanuch template with the new AMI
B - Tell autoscaling to use this new launch template

---

**thanhnv142** (Sun 11 Feb 2024 13:02) - *Upvotes: 4*
ABE are correct: <The AMI that the Auto Scaling group uses was recently deleted> means we need a new AMI image from a runnung EC2 instance
C and D: irrelevant, auto Scaling group's desired capacity has nothing to do here
F: This option doesnt make sense

---

**promo286** (Sat 27 Jan 2024 13:57) - *Upvotes: 2*
To address the issue of the deleted AMI in the Auto Scaling group, you can follow these steps:

Create a new AMI from a running EC2 instance in the Auto Scaling group. (Option E)

This will ensure that you have a new AMI based on the current state of one of the running instances in the group.
Create a new launch template that uses the new AMI. (Option A)

After creating the new AMI, update the launch template to use this new AMI. Launch templates provide a versioned and more structured way to define the launch configuration for your Auto Scaling group.
Update the Auto Scaling group to use the new launch template. (Option B)

Update the Auto Scaling group to use the new launch template that includes the new AMI. This will ensure that new instances launched by the Auto Scaling group will use the updated configuration.

---

**sksegha** (Thu 25 Jan 2024 15:24) - *Upvotes: 1*
Create a new AMI from running instance, Update launch template to use new AMI, update ASG to use new launch template

---

**a54b16f** (Fri 12 Jan 2024 20:02) - *Upvotes: 1*
create a new image and use it

---

**d262e67** (Sun 31 Dec 2023 23:11) - *Upvotes: 1*
Only viable steps

---

**csG13** (Fri 29 Dec 2023 17:07) - *Upvotes: 2*
Create a new AMI from a running instance in ASG. Use it to create a new launch template, finally update ASG to use the new template.

---


<br/>

## Question 166

*Date: Dec. 29, 2023, 5:21 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company deploys a web application on Amazon EC2 instances that are behind an Application Load Balancer (ALB). The company stores the application code in an AWS CodeCommit repository. When code is merged to the main branch, an AWS Lambda function invokes an AWS CodeBuild project. The CodeBuild project packages the code, stores the packaged code in AWS CodeArtifact, and invokes AWS Systems Manager Run Command to deploy the packaged code to the EC2 instances.

Previous deployments have resulted in defects, EC2 instances that are not running the latest version of the packaged code, and inconsistencies between instances.

Which combination of actions should a DevOps engineer take to implement a more reliable deployment solution? (Choose two.)

**Options:**
- A. Create a pipeline in AWS CodePipeline that uses the CodeCommit repository as a source provider. Configure pipeline stages that run the CodeBuild project in parallel to build and test the application. In the pipeline, pass the CodeBuild project output artifact to an AWS CodeDeploy action.
- B. Create a pipeline in AWS CodePipeline that uses the CodeCommit repository as a source provider. Create separate pipeline stages that run a CodeBuild project to build and then test the application. In the pipeline, pass the CodeBuild project output artifact to an AWS CodeDeploy action.
- C. Create an AWS CodeDeploy application and a deployment group to deploy the packaged code to the EC2 instances. Configure the ALB for the deployment group.
- D. Create individual Lambda functions that use AWS CodeDeploy instead of Systems Manager to run build, test, and deploy actions.
- E. Create an Amazon S3 bucket. Modify the CodeBuild project to store the packages in the S3 bucket instead of in CodeArtifact. Use deploy actions in CodeDeploy to deploy the artifact to the EC2 instances.

> **Suggested Answer:** BC
> **Community Vote:** BC (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Tue 30 Jul 2024 04:41) - *Upvotes: 1*
BC
CodePipeline, CodeCommit, CodeDeploy

---

**dkp** (Sat 13 Apr 2024 23:34) - *Upvotes: 2*
B (Sequential Stages) if:
Build and test stages have dependencies (tests rely on build output).
You need manual approvals or gates between stages for control.

---

**WhyIronMan** (Sat 30 Mar 2024 23:28) - *Upvotes: 1*
B,C
A is wrong because doesn't make sense to do it in parallel since it'll cause more problems.
D and E are dumb

---

**DanShone** (Sat 16 Mar 2024 14:23) - *Upvotes: 1*
B and C
CodePipeline and CodeDeploy

---

**thanhnv142** (Sun 11 Feb 2024 13:08) - *Upvotes: 4*
B and C are correct: We need to use codedeploy to build instead of using codebuild
A: <run the CodeBuild project in parallel> - this is in correct. Should run the pipiline respectedly
D: Should not use lambda
E: Codeartifact is good, no need to change to S3

---

**denccc** (Wed 17 Jan 2024 19:08) - *Upvotes: 1*
B & C BeCause

---

**yuliaqwerty** (Fri 12 Jan 2024 12:34) - *Upvotes: 1*
Answer B and C

---

**kabary** (Mon 01 Jan 2024 00:36) - *Upvotes: 1*
B & C for sure.

---

**d262e67** (Sun 31 Dec 2023 23:17) - *Upvotes: 1*
B. because in this case sequential approach is more reliable and ensures consistency.
C. Because in the only possible next step in the process.

---

**PrasannaBalaji** (Sat 30 Dec 2023 06:14) - *Upvotes: 1*
B and C

---


<br/>

## Question 167

*Date: Dec. 29, 2023, 5:46 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses an organization in AWS Organizations to manage its AWS accounts. The company's automation account contains a CI/CD pipeline that creates and configures new AWS accounts.

The company has a group of internal service teams that provide services to accounts in the organization. The service teams operate out of a set of services accounts. The service teams want to receive an AWS CloudTrail event in their services accounts when the CreateAccount API call creates a new account.

How should the company share this CloudTrail event with the service accounts?

**Options:**
- A. Create an Amazon EventBridge rule in the automation account to send account creation events to the default event bus in the services accounts. Update the default event bus in the services accounts to allow events from the automation account.
- B. Create a custom Amazon EventBridge event bus in the services accounts. Update the custom event bus to allow events from the automation account. Create an EventBridge rule in the services account that directly listens to CloudTrail events from the automation account.
- C. Create a custom Amazon EventBridge event bus in the automation account and the services accounts. Create an EventBridge rule and policy that connects the custom event buses that are in the automation account and the services accounts.
- D. Create a custom Amazon EventBridge event bus in the automation account. Create an EventBridge rule and policy that connects the custom event bus to the default event buses in the services accounts.

> **Suggested Answer:** A
> **Community Vote:** A (85%), B (15%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**ozansenturk** (Tue 02 Jul 2024 00:36) - *Upvotes: 5*
A is right. "Create an Amazon EventBridge rule in the automation account to send account creation events to the default event bus in the services accounts": propagation of provision events to the service accounts. "Update the default event bus in the services accounts to allow events from the automation account.": correct


B. "Create a custom Amazon EventBridge event bus in the services accounts. Update the custom event bus to allow events from the automation account.": correct however "Create an EventBridge rule in the services account that directly listens to CloudTrail events from the automation account.": Why do you create a rule in the services account listening the events from automation account, in opposite, the rule should be created in the automation account to push the events to the bus in the services account.

---

**thanhnv142** (Sun 11 Aug 2024 13:55) - *Upvotes: 5*
A is correct: We need account creation events and this option provides us with exactly that
B: < Create an EventBridge rule in the services account that directly listens to CloudTrail events>: This does not make sense. We should apply rule to eventbus to send event
C and D: Both options send all events, not just account creation events

---

**xdkonorek2** (Tue 05 Nov 2024 20:15) - *Upvotes: 1*
I'm voting B

A - there could be more than 5 accounts: https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-rules-best-practices.html - "While you can specify up to five targets for a given rule"

It's perfectly viable to create rule in one account to receive events from second account.

---

**vn_thanhtung** (Thu 21 Nov 2024 11:01) - *Upvotes: 2*
https://aws.amazon.com/vi/blogs/aws/new-cross-account-delivery-of-cloudwatch-events/
Option B Wrong. Answer is A

---

**c3518fc** (Thu 24 Oct 2024 17:30) - *Upvotes: 1*
The steps to configure EventBridge to send events to or receive events from an event bus in a different account include the following:
On the receiver account, edit the permissions on an event bus to allow specified AWS accounts, an organization, or all AWS accounts to send events to the receiver account.
On the sender account, set up one or more rules that have the receiver account's event bus as the target.
If the sender account inherits permissions to send events from an AWS Organization, the sender account also must have an IAM role with policies that enable it to send events to the receiver account. If you use the AWS Management Console to create the rule that targets the event bus in the receiver account, the role is created automatically.
On the receiver account, set up one or more rules that match events that come from the sender account.

---

**dkp** (Sun 13 Oct 2024 23:49) - *Upvotes: 3*
answer A

---

**stoy123** (Sat 28 Sep 2024 08:40) - *Upvotes: 3*
of course its A!
(CloudTrail events) ---EventBridge rule---> [automation account default EventBridge event bus] ---allow---> [service accounts custom EventBridge event bus]

---

**stoy123** (Sat 28 Sep 2024 08:41) - *Upvotes: 1*
I mean B

---

**6f258dd** (Tue 16 Jul 2024 13:51) - *Upvotes: 2*
its A, rest don't include account creation.

---

**a54b16f** (Fri 12 Jul 2024 19:21) - *Upvotes: 2*
B is wrong, the event is in automation account. It lacks the step to send the event from automation to service account.

---


<br/>

## Question 168

*Date: Dec. 30, 2023, 6:20 a.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer is building a solution that uses Amazon Simple Queue Service (Amazon SQS) standard queues. The solution also includes an AWS Lambda function and an Amazon DynamoDB table. The Lambda function pulls content from an SQS queue event source and writes the content to the DynamoDB table.

The solution must maximize the scalability of Lambda and must prevent successfully processed SQS messages from being processed multiple times.

Which solution will meet these requirements?

**Options:**
- A. Decrease the batch window to 1 second when configuring the Lambda function's event source mapping.
- B. Decrease the batch size to 1 when configuring the Lambda function's event source mapping.
- C. Include the ReportBatchItemFailures value in the FunctionResponseTypes list in the Lambda function's event source mapping.
- D. Set the queue visibility timeout on the Lambda function's event source mapping to account for invocation throttling of the Lambda function.

> **Suggested Answer:** C
> **Community Vote:** C (86%), 7%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**csG13** (Sat 30 Dec 2023 07:43) - *Upvotes: 6*
It's C, here is a reference:

https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html#services-sqs-batchfailurereporting

---

**noisonnoiton** (Wed 17 Jul 2024 03:33) - *Upvotes: 2*
link updated:

https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-errorhandling.html

---

**youonebe** (Mon 23 Dec 2024 03:09) - *Upvotes: 1*
C does not prevent multiple message processing

---

**youonebe** (Mon 23 Dec 2024 03:07) - *Upvotes: 1*
The visibility timeout determines how long SQS will "hide" a message after Lambda begins processing it. The key here is ensuring that the message is not available for another Lambda function invocation until the current one completes successfully. The visibility timeout should be at least as long as the Lambda function's maximum processing time, ensuring that the same message is not processed again while it is being handled.

---

**flaacko** (Wed 21 Aug 2024 16:24) - *Upvotes: 2*
To prevent Lambda from processing a message multiple times, you can either configure your event source mapping to include batch item failures in your function response, or you can use the DeleteMessage API to remove messages from the queue as your Lambda function successfully processes them.

To avoid reprocessing successfully processed messages in a failed batch, you can configure your event source mapping to make only the failed messages visible again. This is called a partial batch response. To turn on partial batch responses, specify ReportBatchItemFailures for the FunctionResponseTypes action when configuring your event source mapping. This lets your function return a partial success, which can help reduce the number of unnecessary retries on records.

Source: https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-errorhandling.html#services-sqs-batchfailurereporting

---

**Gomer** (Fri 28 Jun 2024 23:35) - *Upvotes: 1*
"Implementing partial batch responses
When "Lambda function encounters an error while processing a batch, all messages"... "become visible in the queue"... "including messages that Lambda processed successfully."
"...your function can end up processing the same message several times.
"To avoid reprocessing successfully processed messages in a failed batch" "configure your event source mapping to make only the failed messages visible again."
"To turn on partial batch responses, specify ReportBatchItemFailures for the FunctionResponseTypes action when configuring your event source mapping."
"This lets your function return a partial success, which can help reduce the number of unnecessary retries on records."
https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-errorhandling.html

---

**Gomer** (Fri 28 Jun 2024 23:36) - *Upvotes: 1*
The URL with "services-sqs-batchfailurereporting" pointer seems to be invalid now. I think the preceeding URL replaced it.

---

**misako** (Wed 15 May 2024 07:00) - *Upvotes: 1*
C doesn't address the "maximize the scalability of Lambda" while B addresses both,
while batch size is 1, you either fail or success

---

**c3518fc** (Wed 24 Apr 2024 17:43) - *Upvotes: 4*
To avoid reprocessing successfully processed messages in a failed batch, you can configure your event source mapping to make only the failed messages visible again. This is called a partial batch response. To turn on partial batch responses, specify ReportBatchItemFailures for the FunctionResponseTypes action when configuring your event source mapping. This lets your function return a partial success, which can help reduce the number of unnecessary retries on records. https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html#services-sqs-batchfailurereporting

---

**dkp** (Sat 13 Apr 2024 23:58) - *Upvotes: 2*
answer C

---


<br/>

## Question 169

*Date: Dec. 29, 2023, 2:17 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has a new AWS account that teams will use to deploy various applications. The teams will create many Amazon S3 buckets for application-specific purposes and to store AWS CloudTrail logs. The company has enabled Amazon Macie for the account.

A DevOps engineer needs to optimize the Macie costs for the account without compromising the account's functionality.

Which solutions will meet these requirements? (Choose two.)

**Options:**
- A. Exclude S3 buckets that contain CloudTrail logs from automated discovery.
- B. Exclude S3 buckets that have public read access from automated discovery.
- C. Configure scheduled daily discovery jobs for all S3 buckets in the account.
- D. Configure discovery jobs to include S3 objects based on the last modified criterion.
- E. Configure discovery jobs to include S3 objects that are tagged as production only.

> **Suggested Answer:** AD
> **Community Vote:** AD (84%), Other, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**c3518fc** (Thu 24 Oct 2024 17:54) - *Upvotes: 3*
Make your sensitive data discovery jobs as targeted and specific as possible in their scope by using the Object criteria

---

**dkp** (Mon 14 Oct 2024 00:12) - *Upvotes: 3*
A&D
Options to make discovery jobs more targeted include:
Include objects by using the “last modified” criterion
Don’t scan CloudTrail logs
Consider using random object sampling
Include objects with specific extensions, tags, or storage size with specific tag key/value pairs such as Environment: Production.
Consider scheduling jobs based on how long objects live in your S3 buckets

---

**devakram** (Sun 13 Oct 2024 11:20) - *Upvotes: 2*
https://aws.amazon.com/blogs/security/how-to-use-amazon-macie-to-reduce-the-cost-of-discovering-sensitive-data/

Options to make discovery jobs more targeted include:

Include objects by using the “last modified” criterion —
Consider using random object sampling —
Include objects with specific extensions, tags, or storage size —

---

**WhyIronMan** (Mon 30 Sep 2024 22:34) - *Upvotes: 2*
A - No need to scan these
D - Reduce costs but not functionallity

---

**DanShone** (Mon 16 Sep 2024 13:11) - *Upvotes: 1*
A - No need to scan these
D - Reduce costs but not functionallity

---

**Diego1414** (Sat 24 Aug 2024 19:53) - *Upvotes: 1*
AD - Correct

https://aws.amazon.com/blogs/security/how-to-use-amazon-macie-to-reduce-the-cost-of-discovering-sensitive-data/

---

**thanhnv142** (Sun 11 Aug 2024 14:18) - *Upvotes: 2*
A and D are correct:
A: We dont need to scan Cloudtrail logs, so this is good
B: Excluding S3 that have public read is just wrong
C: We have excluded cloudtrail logs S3, so scanning all S3 is not correct
D: This is good
E: <Amazon S3 buckets for application-specific purposes and to store AWS CloudTrail logs> means that these S3 buckets are used to store logs and for productions only. Therefore, there will be no production tag, because all of them are production S3 bukets

---

**a54b16f** (Fri 12 Jul 2024 19:28) - *Upvotes: 1*
E sounds right, but the question is about how to optimize, so E would make sense it mentioned skipping non-prod log, or scan prod data only

---

**yuliaqwerty** (Fri 12 Jul 2024 14:27) - *Upvotes: 1*
Answer AD

---

**ozansenturk** (Tue 02 Jul 2024 00:54) - *Upvotes: 3*
Don’t scan CloudTrail logs, Include objects by using the “last modified” criterion :https://aws.amazon.com/blogs/security/how-to-use-amazon-macie-to-reduce-the-cost-of-discovering-sensitive-data/

---


<br/>

## Question 170

*Date: Dec. 29, 2023, 6:35 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses an organization in AWS Organizations to manage its AWS accounts. The company recently acquired another company that has standalone AWS accounts. The acquiring company's DevOps team needs to consolidate the administration of the AWS accounts for both companies and retain full administrative control of the accounts. The DevOps team also needs to collect and group findings across all the accounts to implement and maintain a security posture.

Which combination of steps should the DevOps team take to meet these requirements? (Choose two.)

**Options:**
- A. Invite the acquired company's AWS accounts to join the organization. Create an SCP that has full administrative privileges. Attach the SCP to the management account.
- B. Invite the acquired company's AWS accounts to join the organization. Create the OrganizationAccountAccessRole IAM role in the invited accounts. Grant permission to the management account to assume the role.
- C. Use AWS Security Hub to collect and group findings across all accounts. Use Security Hub to automatically detect new accounts as the accounts are added to the organization.
- D. Use AWS Firewall Manager to collect and group findings across all accounts. Enable all features for the organization. Designate an account in the organization as the delegated administrator account for Firewall Manager.
- E. Use Amazon Inspector to collect and group findings across all accounts. Designate an account in the organization as the delegated administrator account for Amazon Inspector.

> **Suggested Answer:** BC
> **Community Vote:** BC (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Sun 11 Aug 2024 14:23) - *Upvotes: 6*
B and C are correct: <needs to consolidate the administration of the AWS accounts for both companies and retain full administrative control of the accounts> means we need to invite the new accounts to the existing AWS organization. <collect and group findings across all the accounts> means security hub
A: <Attach the SCP to the management account.>: this is incorrect
D: Firewall manager is use to centrally manage all FWs, not to collect and group findings
E: Inspector is used for vulnerability scanning only

---

**c3518fc** (Thu 24 Oct 2024 18:07) - *Upvotes: 2*
B) https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html
C) https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-concepts.html

---

**dkp** (Mon 14 Oct 2024 00:18) - *Upvotes: 2*
answer is BC

---

**DanShone** (Mon 16 Sep 2024 13:09) - *Upvotes: 2*
B - Add accounts to the org
C - collect and group findings

---

**yuliaqwerty** (Fri 12 Jul 2024 14:43) - *Upvotes: 4*
Answer B and C https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html

---

**d262e67** (Sun 30 Jun 2024 23:03) - *Upvotes: 1*
They seem correct

---

**csG13** (Sat 29 Jun 2024 17:35) - *Upvotes: 3*
B is required to access from the management account the new account.
C will provide an aggregate view of all the security findings.

---


<br/>

## Question 171

*Date: Dec. 29, 2023, 6:54 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an application and a CI/CD pipeline. The CI/CD pipeline consists of an AWS CodePipeline pipeline and an AWS CodeBuild project. The CodeBuild project runs tests against the application as part of the build process and outputs a test report. The company must keep the test reports for 90 days.

Which solution will meet these requirements?

**Options:**
- A. Add a new stage in the CodePipeline pipeline after the stage that contains the CodeBuild project. Create an Amazon S3 bucket to store the reports. Configure an S3 deploy action type in the new CodePipeline stage with the appropriate path and format for the reports.
- B. Add a report group in the CodeBuild project buildspec file with the appropriate path and format for the reports. Create an Amazon S3 bucket to store the reports. Configure an Amazon EventBridge rule that invokes an AWS Lambda function to copy the reports to the S3 bucket when a build is completed. Create an S3 Lifecycle rule to expire the objects after 90 days.
- C. Add a new stage in the CodePipeline pipeline. Configure a test action type with the appropriate path and format for the reports. Configure the report expiration time to be 90 days in the CodeBuild project buildspec file.
- D. Add a report group in the CodeBuild project buildspec file with the appropriate path and format for the reports. Create an Amazon S3 bucket to store the reports. Configure the report group as an artifact in the CodeBuild project buildspec file. Configure the S3 bucket as the artifact destination. Set the object expiration to 90 days.

> **Suggested Answer:** B
> **Community Vote:** B (65%), D (35%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**kyuhuck** (Sat 17 Feb 2024 16:16) - *Upvotes: 11*
Best Option: Option D appears to be the most straightforward and effective solution that meets the requirements. It simplifies the process by utilizing CodeBuild's feature to directly send reports to an S3 bucket configured as the artifact destination. By setting the object expiration to 90 days in the S3 bucket settings, it fulfills the requirement to keep the test reports for 90 days. This option does not require additional services for moving the reports to S3, assuming the CodeBuild report group configuration allows for direct report storage in S3 with specified retention policies.

---

**Seoyong** (Wed 13 Mar 2024 03:35) - *Upvotes: 8*
question key word :
how to expire objects in S3.

only S3 Lifecycle rule can expire objects.

---

**b0gdan433** (Sun 12 May 2024 15:58) - *Upvotes: 2*
I think this is the most valid point.

---

**GripZA** (Sun 20 Apr 2025 22:09) - *Upvotes: 1*
Torn between B and D TBH, looking at documentation:
"If you want to upload the raw data of your test report results to an Amazon S3 bucket:
Select Export to Amazon S3."

Here it clearly mentions test report results, not export as artifact.

Then a little further down when referring to encryption, doc says "Disable artifact encryption to disable encryption. You might choose this if you want to share your test results, or publish them to a static website. "

Now referring to it as an artifact...

But this changes if you specifically configure them as artifacts in the buildspec file.

---

**spring21** (Mon 09 Dec 2024 02:22) - *Upvotes: 1*
phases:
build:
commands:
- echo "Running tests..."
- run_tests_command
artifacts:
files:
- path/to/test_report/*
discard-paths: no
base-directory: path/to/test_report

---

**xdkonorek2** (Sun 05 May 2024 21:15) - *Upvotes: 2*
I think D even though this sentence sounds ridiculous: "Configure the report group as an artifact in the CodeBuild project buildspec file" they probably meant that report should be sent as an artifact and that's correct.

---

**flaacko** (Wed 21 Aug 2024 16:53) - *Upvotes: 2*
D is incorrect because it talks about storing report groups as artifacts but since report groups are not CodeBuild artifacts, this cannot be done.

---

**seetpt** (Thu 02 May 2024 14:10) - *Upvotes: 2*
I think B

---

**c3518fc** (Wed 24 Apr 2024 18:19) - *Upvotes: 5*
Not sure why we are configuring the report group as an artifact in the CodeBuild project buildspec file. Doesn't make sense and we use S3 lifecycle to expire objects automatically.

---

**dkp** (Sun 14 Apr 2024 00:38) - *Upvotes: 5*
Both B & D seem to work, but object expiration settings still need to set the lifecycle rule manually for option D

---


<br/>

## Question 172

*Date: Dec. 29, 2023, 2:29 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses an Amazon API Gateway regional REST API to host its application API. The REST API has a custom domain. The REST API's default endpoint is deactivated.

The company's internal teams consume the API. The company wants to use mutual TLS between the API and the internal teams as an additional layer of authentication.

Which combination of steps will meet these requirements? (Choose two.)

**Options:**
- A. Use AWS Certificate Manager (ACM) to create a private certificate authority (CA). Provision a client certificate that is signed by the private CA.
- B. Provision a client certificate that is signed by a public certificate authority (CA). Import the certificate into AWS Certificate Manager (ACM).
- C. Upload the provisioned client certificate to an Amazon S3 bucket. Configure the API Gateway mutual TLS to use the client certificate that is stored in the S3 bucket as the trust store.
- D. Upload the provisioned client certificate private key to an Amazon S3 bucket. Configure the API Gateway mutual TLS to use the private key that is stored in the S3 bucket as the trust store.
- E. Upload the root private certificate authority (CA) certificate to an Amazon S3 bucket. Configure the API Gateway mutual TLS to use the private CA certificate that is stored in the S3 bucket as the trust store.

> **Suggested Answer:** AE
> **Community Vote:** AE (79%), AC (15%), 6%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**d262e67** (Sun 30 Jun 2024 23:38) - *Upvotes: 7*
A. Because it's only for internal teams.
E. Because the truststore dictates which CAs to trust. If you have intermediate CAs those also need to be present in the S3 bucket.

---

**d262e67** (Sun 30 Jun 2024 23:38) - *Upvotes: 1*
https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-mutual-tls.html

---

**nickp84** (Mon 12 May 2025 19:34) - *Upvotes: 1*
C. Upload client certificate to S3
– The trust store should contain the CA certificate, not individual client certificates. API Gateway validates the client cert against the CA, not by comparing the cert directly.

---

**Jay_2pt0_1** (Sat 09 Nov 2024 11:48) - *Upvotes: 2*
A. use ACM to generate cert
E. See https://aws.amazon.com/blogs/compute/introducing-mutual-tls-authentication-for-amazon-api-gateway/

---

**didek1986** (Thu 17 Oct 2024 14:36) - *Upvotes: 4*
C is incorrect because the trust store should contain the root CA certificate, not the client certificate.
Root CA certificate is used to validate the client certificates (can be many) presented by the clients. If the client certificate itself is in the trust store, it would mean that only that specific client is trusted, which is not practical in a scenario where there are multiple clients (read it as company's internal teams).

---

**WhyIronMan** (Sun 29 Sep 2024 23:21) - *Upvotes: 1*
A and C. Details are everything in an Investigation...
What API Gateway needs is the Client Certificate generated by option A and not the CA

---

**DanShone** (Mon 16 Sep 2024 13:02) - *Upvotes: 4*
https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-mutual-tls.html
After reading the above documentation I would determine A & E

---

**Nano803** (Sat 14 Sep 2024 16:24) - *Upvotes: 1*
Check this article, https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-mutual-tls.html. You need to upload the truststore to an Amazon S3 bucket in a single file

---

**Ramdi1** (Tue 13 Aug 2024 14:23) - *Upvotes: 3*
After reading this I would suggest A & E

---

**thanhnv142** (Mon 12 Aug 2024 09:54) - *Upvotes: 1*
A and C is correct:
A: we prefer AWS service more than a public one, which is B
B: The reason is explained in option a
C: Upload the provisioned to S3 bucket.
D: should not upload private key to anywhere.
E: This option has no connection to option A.

---


<br/>

## Question 173

*Date: Dec. 30, 2023, 6:32 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses AWS Directory Service for Microsoft Active Directory as its identity provider (IdP). The company requires all infrastructure to be defined and deployed by AWS CloudFormation.

A DevOps engineer needs to create a fleet of Windows-based Amazon EC2 instances to host an application. The DevOps engineer has created a CloudFormation template that contains an EC2 launch template, IAM role, EC2 security group, and EC2 Auto Scaling group. The DevOps engineer must implement a solution that joins all EC2 instances to the domain of the AWS Managed Microsoft AD directory.

Which solution will meet these requirements with the MOST operational efficiency?

**Options:**
- A. In the CloudFormation template, create an AWS::SSM::Document resource that joins the EC2 instance to the AWS Managed Microsoft AD domain by using the parameters for the existing directory. Update the launch template to include the SSMAssociation property to use the new SSM document. Attach the AmazonSSMManagedInstanceCore and AmazonSSMDirectoryServiceAccess AWS managed policies to the IAM role that the EC2 instances use.
- B. In the CloudFormation template, update the launch template to include specific tags that propagate on launch. Create an AWS::SSM::Association resource to associate the AWS-JoinDirectoryServiceDomain Automation runbook with the EC2 instances that have the specified tags. Define the required parameters to join the AWS Managed Microsoft AD directory. Attach the AmazonSSMManagedInstanceCore and AmazonSSMDirectoryServiceAccess AWS managed policies to the IAM role that the EC2 instances use.
- C. Store the existing AWS Managed Microsoft AD domain connection details in AWS Secrets Manager. In the CloudFormation template, create an AWS::SSM::Association resource to associate the AWS-CreateManagedWindowsInstanceWithApproval Automation runbook with the EC2 Auto Scaling group. Pass the ARNs for the parameters from Secrets Manager to join the domain. Attach the AmazonSSMDirectoryServiceAccess and SecretsManagerReadWrite AWS managed policies to the IAM role that the EC2 instances use.
- D. Store the existing AWS Managed Microsoft AD domain administrator credentials in AWS Secrets Manager. In the CloudFormation template, update the EC2 launch template to include user data. Configure the user data to pull the administrator credentials from Secrets Manager and to join the AWS Managed Microsoft AD domain. Attach the AmazonSSMManagedInstanceCore and SecretsManagerReadWrite AWS managed policies to the IAM role that the EC2 instances use.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**aws_god** (Tue 10 Sep 2024 08:21) - *Upvotes: 1*
B is correct - https://docs.aws.amazon.com/directoryservice/latest/admin-guide/step4_test_ec2_access.html

---

**dkp** (Sun 14 Apr 2024 01:19) - *Upvotes: 2*
ans is B

---

**thanhnv142** (Mon 12 Feb 2024 11:06) - *Upvotes: 3*
B is correct: we need to use AWS:SSM::Document with the AWS-JoinDirectoryServiceDomain automation runbook for this task
A: no mention of the name of runbook to join domain
C: AWS-CreateManagedWindowsInstanceWithApproval Automation runbook is used for creating a windows instance, not to join domain
D: no mention of AWS::SSM::Document

---

**twogyt** (Thu 18 Jan 2024 18:44) - *Upvotes: 2*
B is correct

---

**a54b16f** (Fri 12 Jan 2024 20:48) - *Upvotes: 2*
keyword: JoinDirectoryServiceDomain

---

**d262e67** (Mon 01 Jan 2024 00:52) - *Upvotes: 2*
Must be B:
https://docs.aws.amazon.com/systems-manager/latest/userguide/walkthrough-powershell.html#walkthrough-powershell-domain-join

---

**csG13** (Sat 30 Dec 2023 09:05) - *Upvotes: 2*
It’s B

---

**PrasannaBalaji** (Sat 30 Dec 2023 06:32) - *Upvotes: 2*
B is correct

---


<br/>

## Question 174

*Date: Dec. 29, 2023, 7:07 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses AWS Organizations to manage its AWS accounts. The company has a root OU that has a child OU. The root OU has an SCP that allows all actions on all resources. The child OU has an SCP that allows all actions for Amazon DynamoDB and AWS Lambda, and denies all other actions.

The company has an AWS account that is named vendor-data in the child OU. A DevOps engineer has an IAM user that is attached to the Administrator Access IAM policy in the vendor-data account. The DevOps engineer attempts to launch an Amazon EC2 instance in the vendor-data account but receives an access denied error.

Which change should the DevOps engineer make to launch the EC2 instance in the vendor-data account?

**Options:**
- A. Attach the AmazonEC2FullAccess IAM policy to the IAM user.
- B. Create a new SCP that allows all actions for Amazon EC2. Attach the SCP to the vendor-data account.
- C. Update the SCP in the child OU to allow all actions for Amazon EC2.
- D. Create a new SCP that allows all actions for Amazon EC2. Attach the SCP to the root OU.

> **Suggested Answer:** C
> **Community Vote:** C (74%), B (26%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**ericphl** (Thu 25 Jul 2024 13:07) - *Upvotes: 7*
I vote B. can't understand why B is not correct answer. SCP can be attached to account.

For the C, it is possible. but the potential risk is it's not only allow all EC2 action on "vendor-data" account, but also allow all EC2 actions in other account under the child OU. which is not a best practice.

---

**Srikantha** (Sun 30 Mar 2025 12:55) - *Upvotes: 1*
Deny takes precedence

---

**csG13** (Fri 29 Dec 2023 19:07) - *Upvotes: 6*
It's C - Allow must be explicit from root all the way down to the account level. Since it's not specified in the OU the only way to make it available to vendor-account is to change the OU policy.

---

**ryuhei** (Sat 16 Aug 2025 13:40) - *Upvotes: 1*
Option B doesn't remove the restriction in the child OU's SCP, which doesn't allow EC2 instances to be launched. Option C resolves the issue by updating the child OU's SCP, which allows DevOps engineers to launch EC2 instances in the vendor-data account.

---

**Srikantha** (Sun 30 Mar 2025 12:56) - *Upvotes: 1*
This ensures that IAM policies in the vendor-data account can grant EC2 permissions, resolving the issue.

---

**Abilash2605** (Fri 16 Aug 2024 07:41) - *Upvotes: 2*
Answer is C. You can attach SCP to vendor-data account. however there is deny rule at OU level and that will apply and without updating that your SCP at vendor data account is not useful. As the account will inherit SCP applied at OU.

---

**auxwww** (Mon 12 Aug 2024 20:36) - *Upvotes: 1*
B - Incorrect IMO - The question doesn't ask about taking away anything currently allowed in the existing SCP

---

**c3518fc** (Wed 24 Apr 2024 18:37) - *Upvotes: 3*
By updating the SCP in the child OU to allow all actions for Amazon EC2, the DevOps engineer can grant the necessary permissions to launch EC2 instances in the vendor-data account while maintaining the desired restrictions for other services and accounts within the child OU.

---

**dkp** (Sun 14 Apr 2024 01:22) - *Upvotes: 3*
answer is C

---

**WhyIronMan** (Sun 31 Mar 2024 00:28) - *Upvotes: 2*
C, details are everything during an investigation

---


<br/>

## Question 175

*Date: Dec. 29, 2023, 7:17 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company's security policies require the use of security hardened AMIs in production environments. A DevOps engineer has used EC2 Image Builder to create a pipeline that builds the AMIs on a recurring schedule.

The DevOps engineer needs to update the launch templates of the company's Auto Scaling groups. The Auto Scaling groups must use the newest AMIs during the launch of Amazon EC2 instances.

Which solution will meet these requirements with the MOST operational efficiency?

**Options:**
- A. Configure an Amazon EventBridge rule to receive new AMI events from Image Builder. Target an AWS Systems Manager Run Command document that updates the launch templates of the Auto Scaling groups with the newest AMI ID.
- B. Configure an Amazon EventBridge rule to receive new AMI events from Image Builder. Target an AWS Lambda function that updates the launch templates of the Auto Scaling groups with the newest AMI ID.
- C. Configure the launch template to use a value from AWS Systems Manager Parameter Store for the AMI ID. Configure the Image Builder pipeline to update the Parameter Store value with the newest AMI ID.
- D. Configure the Image Builder distribution settings to update the launch templates with the newest AMI IConfigure the Auto Scaling groups to use the newest version of the launch template.

> **Suggested Answer:** D
> **Community Vote:** D (60%), C (28%), 12%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Mon 12 Feb 2024 11:17) - *Upvotes: 7*
D is correct: Image builder has a built-in that allow updating EC2 launch template
A: AWS Systems Manager Run Command document is used for running scripts on EC2, not to update
B: Lambda is used for other tasks, not this one
C: This seems to be a feasible option, but we can update the launch template directly without using parameter store

---

**d262e67** (Mon 01 Jan 2024 01:00) - *Upvotes: 6*
Definitely D according to this:
https://docs.aws.amazon.com/imagebuilder/latest/userguide/dist-using-launch-template.html

---

**ryuhei** (Sun 24 Aug 2025 07:52) - *Upvotes: 1*
Answer is C.
option B is functional, it is less efficient than option C due to the added complexity of Lambda development,ongoing maintenance, and the need for repeated launch template updates.
Option C provides a simpler, more automated, and maintainable solution using SSM Parameter Store.

---

**nickp84** (Fri 16 May 2025 09:27) - *Upvotes: 1*
Launch templates can reference an SSM Parameter Store value for the AMI ID.
When Image Builder updates that parameter, new EC2 instances launched by the Auto Scaling group will automatically use the new AMI—no need to update the launch template or its version.
This is fully supported, automated, and operationally efficient.

---

**Srikantha** (Sun 30 Mar 2025 12:57) - *Upvotes: 2*
Using Parameter Store as a centralized AMI reference ensures that new instances always use the latest AMI with minimal operational overhead.

---

**f4bi4n** (Mon 26 Aug 2024 13:16) - *Upvotes: 2*
I would go with C even when D seems to be the primary choice.
In D, you would need to maintain all of the launchTemplateConfigurations in a list, which means there is a lot of overhead.
With C this is not the case

---

**Gomer** (Mon 01 Jul 2024 23:39) - *Upvotes: 1*
Answers B, C, and D can work. I'm leaning towards "D", but I'm witholding a formal vote for now. It appears the "correct" answer may depend on how you interpret requirements.
NOT B: EventBridge/Lamba can work, but not as simple as D or C. It DOES "update the launch templates of the company's Auto Scaling groups."
NOT C: Answer C can work and is fairly simple, but it DOES NOT "update the launch templates of the company's Auto Scaling groups", because it does not need to, which could be argued is "operationally efficient".
YES D: Seems like simple solution. ASG does need to be updated, but I don't know if that means defining someting like an $LATEST AMI alias (pointer) in ASG, or if ASG actually needs to be updated for each new version of Launch template. This solution could be more complex than C:.

---

**TEC1** (Mon 13 May 2024 19:31) - *Upvotes: 2*
C: This involves configuring the launch template to reference the AMI ID stored in the AWS Systems Manager Parameter Store. The EC2 Image Builder pipeline is then set up to update this Parameter Store value each time a new AMI is built. By doing so, the launch template always points to the latest AMI without requiring manual updates each time a new AMI is built. This approach automates the update process and ensures that Auto Scaling groups always use the most recent and secure AMIs, with minimal manual intervention and operational overhead.

---

**dkp** (Sun 14 Apr 2024 01:25) - *Upvotes: 2*
ans is D

---

**WhyIronMan** (Sun 31 Mar 2024 00:32) - *Upvotes: 4*
D is the correct and best practice suggested by aws
https://docs.aws.amazon.com/imagebuilder/latest/userguide/dist-using-launch-template.html

---


<br/>

## Question 176

*Date: Dec. 29, 2023, 2:40 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has configured an Amazon S3 event source on an AWS Lambda function. The company needs the Lambda function to run when a new object is created or an existing object is modified in a particular S3 bucket. The Lambda function will use the S3 bucket name and the S3 object key of the incoming event to read the contents of the created or modified S3 object. The Lambda function will parse the contents and save the parsed contents to an Amazon DynamoDB table.

The Lambda function's execution role has permissions to read from the S3 bucket and to write to the DynamoDB table. During testing, a DevOps engineer discovers that the Lambda function does not run when objects are added to the S3 bucket or when existing objects are modified.

Which solution will resolve this problem?

**Options:**
- A. Increase the memory of the Lambda function to give the function the ability to process large files from the S3 bucket.
- B. Create a resource policy on the Lambda function to grant Amazon S3 the permission to invoke the Lambda function for the S3 bucket.
- C. Configure an Amazon Simple Queue Service (Amazon SQS) queue as an OnFailure destination for the Lambda function.
- D. Provision space in the /tmp folder of the Lambda function to give the function the ability to process large files from the S3 bucket.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Mon 12 Feb 2024 11:24) - *Upvotes: 6*
B: is correct: Need to add a permission in lambda resource-based policy, which allow S3 to invode lambda
A: If there is insufficient memory, lambda still runs. This case is about lambda not running at all
C: We dont need SQS for dead-letter queue here
D: Lambda does not run in a test, which proves that the problem does not lie in disk space because in tests, testers usually wont use large objects

---

**jamesf** (Tue 30 Jul 2024 08:02) - *Upvotes: 1*
B due to permission issue

---

**dkp** (Sun 14 Apr 2024 01:48) - *Upvotes: 2*
b is correct

---

**DanShone** (Sat 16 Mar 2024 12:22) - *Upvotes: 2*
B - Is correct
A C and D have no relevance to the problem

---

**twogyt** (Thu 18 Jan 2024 18:51) - *Upvotes: 3*
b is correct

---

**d262e67** (Mon 01 Jan 2024 01:03) - *Upvotes: 3*
Lambda must allow S3 to invoke it

---

**PrasannaBalaji** (Fri 29 Dec 2023 14:40) - *Upvotes: 4*
B is correct

---


<br/>

## Question 177

*Date: Dec. 30, 2023, 6:36 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has deployed a critical application in two AWS Regions. The application uses an Application Load Balancer (ALB) in both Regions. The company has Amazon Route 53 alias DNS records for both ALBs.

The company uses Amazon Route 53 Application Recovery Controller to ensure that the application can fail over between the two Regions. The Route 53 ARC configuration includes a routing control for both Regions. The company uses Route 53 ARC to perform quarterly disaster recovery (DR) tests.

During the most recent DR test, a DevOps engineer accidentally turned off both routing controls. The company needs to ensure that at least one routing control is turned on at all times.

Which solution will meet these requirements?

**Options:**
- A. In Route 53 ARC, create a new assertion safety rule. Apply the assertion safety rule to the two routing controls. Configure the rule with the ATLEAST type with a threshold of 1.
- B. In Route 53 ARC, create a new gating safety rule. Apply the assertion safety rule to the two routing controls. Configure the rule with the OR type with a threshold of 1.
- C. In Route 53 ARC, create a new resource set. Configure the resource set with an AWS::Route53::HealthCheck resource type. Specify the ARNs of the two routing controls as the target resource. Create a new readiness check for the resource set.
- D. In Route 53 ARC, create a new resource set. Configure the resource set with an AWS::Route53RecoveryReadiness::DNSTargetResource resource type. Add the domain names of the two Route 53 alias DNS records as the target resource. Create a new readiness check for the resource set.

> **Suggested Answer:** A
> **Community Vote:** A (94%), 6%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**c3518fc** (Fri 25 Oct 2024 08:03) - *Upvotes: 3*
https://docs.aws.amazon.com/r53recovery/latest/dg/routing-control.safety-rules.html

---

**dkp** (Mon 14 Oct 2024 02:03) - *Upvotes: 1*
answer is A

---

**WhyIronMan** (Sun 29 Sep 2024 23:08) - *Upvotes: 1*
B is correct
Option D is incorrect due to a few reasons:

1. No Automation for Report Copying: Unlike Option B, there's no mention of any automated process to copy the reports to the S3 bucket. It relies solely on configuring the S3 bucket as an artifact destination. Without automation, someone would have to manually manage the copying process, which is less efficient and prone to errors.

2. Expiration: While Option D mentions setting object expiration to 90 days, it doesn't specify how this would be achieved. S3 Lifecycle rules are typically used to manage object expiration, but there's no mention of setting up such a rule in this option.

---

**thanhnv142** (Mon 12 Aug 2024 10:38) - *Upvotes: 3*
A is correct: assertion rule to make sure that atleast on gate is always open. This rules are basically things that users cannot do or only allow to do
B: This gating rule is basically an on-off swith for a set of ARCs. If there is a controller that we dont want to turn off, specify this rule. This rule might help us achive the goal of the question. However, this requires we specify the exact name of the controller that should not be turned off. Meanwhile, the question requires that any controller can be turned off but at least one must be up and running. Therefore, this is not the right option

---

**thanhnv142** (Mon 12 Aug 2024 10:38) - *Upvotes: 1*
C and D are irrelevant. They are about creating new resource

---

**twogyt** (Thu 18 Jul 2024 18:00) - *Upvotes: 2*
A is correct : https://docs.aws.amazon.com/r53recovery/latest/dg/routing-control.safety-rules.html

---

**yuliaqwerty** (Fri 12 Jul 2024 15:16) - *Upvotes: 2*
answer A https://docs.aws.amazon.com/r53recovery/latest/dg/routing-control.safety-rules.html

---

**csG13** (Sun 30 Jun 2024 07:18) - *Upvotes: 3*
It's an assertion rule with ATLEAST threshold set to 1. So, A.

Here is a reference:
https://docs.aws.amazon.com/r53recovery/latest/dg/routing-control.safety-rules.html
https://docs.aws.amazon.com/r53recovery/latest/dg/getting-started-cli-routing.html#getting-started-cli-routing.safety

---

**PrasannaBalaji** (Sun 30 Jun 2024 05:36) - *Upvotes: 2*
A is correct

---


<br/>

## Question 178

*Date: Dec. 29, 2023, 2:44 p.m.
Disclaimers:
- ExamTopics website is not rel*

A healthcare services company is concerned about the growing costs of software licensing for an application for monitoring patient wellness. The company wants to create an audit process to ensure that the application is running exclusively on Amazon EC2 Dedicated Hosts. A DevOps engineer must create a workflow to audit the application to ensure compliance.

What steps should the engineer take to meet this requirement with the LEAST administrative overhead?

**Options:**
- A. Use AWS Systems Manager Configuration Compliance. Use calls to the put-compliance-items API action to scan and build a database of noncompliant EC2 instances based on their host placement configuration. Use an Amazon DynamoDB table to store these instance IDs for fast access. Generate a report through Systems Manager by calling the list-compliance-summaries API action.
- B. Use custom Java code running on an EC2 instance. Set up EC2 Auto Scaling for the instance depending on the number of instances to be checked. Send the list of noncompliant EC2 instance IDs to an Amazon SQS queue. Set up another worker instance to process instance IDs from the SQS queue and write them to Amazon DynamoDUse an AWS Lambda function to terminate noncompliant instance IDs obtained from the queue, and send them to an Amazon SNS email topic for distribution.
- C. Use AWS Config. Identify all EC2 instances to be audited by enabling Config Recording on all Amazon EC2 resources for the region. Create a custom AWS Config rule that triggers an AWS Lambda function by using the "config-rule-change -triggered" blueprint. Modify the Lambda evaluateCompliance() function to verify host placement to return a NON_COMPLIANT result if the instance is not running on an EC2 Dedicated Host. Use the AWS Config report to address noncompliant instances.
- D. Use AWS CloudTrail. Identify all EC2 instances to be audited by analyzing all calls to the EC2 RunCommand API action. Invoke an AWS Lambda function that analyzes the host placement of the instance. Store the EC2 instance ID of noncompliant resources in an Amazon RDS for MySQL DB instance. Generate a report by querying the RDS instance and exporting the query results to a CSV text file.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**seetpt** (Sat 02 Nov 2024 15:13) - *Upvotes: 1*
C for me

---

**thanhnv142** (Mon 12 Aug 2024 10:45) - *Upvotes: 4*
C is correct: Using Config is the right way
A: <Use an Amazon DynamoDB table to store these instance IDs for fast access> DynamoDB is used primarily for storing web section data, not to store these IDs
B: Should not use custom Java code on Ec2. Additionally, This option mention terminating non-compliance ones, which is incorrect. We only need an audit workflow
D: Cloud trail is for auditing user activities, not to check non-compliance EC2 instacnes

---

**csG13** (Sun 30 Jun 2024 07:20) - *Upvotes: 2*
It's C

---

**PrasannaBalaji** (Sat 29 Jun 2024 13:44) - *Upvotes: 2*
C is correct

---


<br/>

## Question 179

*Date: Dec. 29, 2023, 2:46 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer is planning to deploy a Ruby-based application to production. The application needs to interact with an Amazon RDS for MySQL database and should have automatic scaling and high availability. The stored data in the database is critical and should persist regardless of the state of the application stack.

The DevOps engineer needs to set up an automated deployment strategy for the application with automatic rollbacks. The solution also must alert the application team when a deployment fails.

Which combination of steps will meet these requirements? (Choose three.)

**Options:**
- A. Deploy the application on AWS Elastic Beanstalk. Deploy an Amazon RDS for MySQL DB instance as part of the Elastic Beanstalk configuration.
- B. Deploy the application on AWS Elastic Beanstalk. Deploy a separate Amazon RDS for MySQL DB instance outside of Elastic Beanstalk.
- C. Configure a notification email address that alerts the application team in the AWS Elastic Beanstalk configuration.
- D. Configure an Amazon EventBridge rule to monitor AWS Health events. Use an Amazon Simple Notification Service (Amazon SNS) topic as a target to alert the application team.
- E. Use the immutable deployment method to deploy new application versions.
- F. Use the rolling deployment method to deploy new application versions.

> **Suggested Answer:** BCE
> **Community Vote:** BCE (83%), Other, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**csG13** (Sat 29 Jun 2024 18:50) - *Upvotes: 8*
Move RDS out of Beanstalk as it is critical. AWS Health can't check deployment health but services only.

---

**c3518fc** (Fri 25 Oct 2024 08:30) - *Upvotes: 2*
https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.sns.html

---

**dkp** (Mon 14 Oct 2024 02:33) - *Upvotes: 4*
BCE
Immutable deployments perform an immutable update to launch a full set of new instances running the new version of the application in a separate Auto Scaling group, alongside the instances running the old version. Immutable deployments can prevent issues caused by partially completed rolling deployments. If the new instances don't pass health checks, Elastic Beanstalk terminates them, leaving the original instances untouched.

---

**WhyIronMan** (Sun 29 Sep 2024 23:51) - *Upvotes: 2*
B, C, E is the correct answer since
A. its is not a good practice deploy database layer with EB
D. is not directly related to the deployment strategy for the application and does not address the requirement for automatic rollbacks.
F. In a rolling deployment, the application version is gradually deployed across the existing instances in the environment. Each instance is updated one at a time, and the application remains available throughout the deployment process. This approach updates the application in a phased manner, minimizing downtime but potentially complicating rollback if issues arise.

---

**stoy123** (Thu 26 Sep 2024 19:03) - *Upvotes: 1*
BDE for sure

---

**stoy123** (Sat 28 Sep 2024 09:20) - *Upvotes: 1*
BCE is correct

---

**ogerber** (Mon 16 Sep 2024 16:27) - *Upvotes: 2*
BCE is correct

---

**master9** (Thu 12 Sep 2024 02:35) - *Upvotes: 1*
WS Elastic Beanstalk itself does not directly provide email notification capabilities. However, you can integrate AWS Elastic Beanstalk with other AWS services to achieve email notifications.

---

**Diego1414** (Sat 24 Aug 2024 21:26) - *Upvotes: 4*
BCE -
B - Makes sense as you want the RDS to persist.
C- You can set an email notification during the Elastic Beanstalk configuration
E - Immutable for roll back as previous versions persists

---

**rijub2022** (Thu 22 Aug 2024 14:34) - *Upvotes: 1*
BDE seems correct

---


<br/>

## Question 180

*Date: Dec. 29, 2023, 2:48 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is using AWS CodePipeline to deploy an application. According to a new guideline, a member of the company's security team must sign off on any application changes before the changes are deployed into production. The approval must be recorded and retained.

Which combination of actions will meet these requirements? (Choose two.)

**Options:**
- A. Configure CodePipeline to write actions to Amazon CloudWatch Logs.
- B. Configure CodePipeline to write actions to an Amazon S3 bucket at the end of each pipeline stage.
- C. Create an AWS CloudTrail trail to deliver logs to Amazon S3.
- D. Create a CodePipeline custom action to invoke an AWS Lambda function for approval. Create a policy that gives the security team access to manage CodePipeline custom actions.
- E. Create a CodePipeline manual approval action before the deployment step. Create a policy that grants the security team access to approve manual approval stages.

> **Suggested Answer:** CE
> **Community Vote:** CE (84%), Other, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Mon 12 Aug 2024 11:01) - *Upvotes: 7*
C and E are correct:
A: Cloudwatch Logs is to store logs from AWS resources like EC2, not codepipeline
B: We dont need to store codepipeline actions in S3.
C: We need to monitor users'actions, so using cloudtrail to store logs to S3 is the recommended one
D: We should not invoke AWS lambda for approval
E: This is the recommended one

---

**youonebe** (Mon 23 Dec 2024 19:05) - *Upvotes: 2*
CloudTrail tracks API activity in your AWS environment, but it does not specifically capture manual approval actions within CodePipeline. CloudTrail can help you audit changes to resources but is not suited for tracking the specific approval process within CodePipeline itself.

---

**c3518fc** (Fri 25 Oct 2024 08:37) - *Upvotes: 3*
https://docs.aws.amazon.com/codepipeline/latest/userguide/monitoring-cloudtrail-logs.html

---

**dkp** (Mon 14 Oct 2024 02:40) - *Upvotes: 1*
ans ce

---

**WhyIronMan** (Sun 29 Sep 2024 23:58) - *Upvotes: 3*
C- Logging, since Cloudwatch Logs and writelogs to S3 can not capture the Approval that only CloudTrail can
E - Manual Approval Step is natively supported by codepipeline, no need to make it more complex with anything

---

**DanShone** (Mon 16 Sep 2024 11:12) - *Upvotes: 2*
C- Logging
E - Manual Approval Step

---

**davdan99** (Wed 10 Jul 2024 18:37) - *Upvotes: 2*
https://stelligent.com/2019/06/11/aws-codepipeline-approval-gate-tracking/

---

**zolthar_z** (Tue 02 Jul 2024 18:35) - *Upvotes: 4*
C and E: The approval process is an AWS API Event and this is managed by CloudTrail

https://docs.aws.amazon.com/codepipeline/latest/userguide/incident-response.html

---

**ozansenturk** (Tue 02 Jul 2024 10:37) - *Upvotes: 2*
CE:
AWS CodePipeline is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in CodePipeline;. CloudTrail captures all API calls for CodePipeline as events. The calls captured include calls from the CodePipeline console and code calls to the CodePipeline API operations. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for CodePipeline. If you don't configure a trail, you can still view the most recent events in the CloudTrail console in Event history. Using the information collected by CloudTrail, you can determine the request that was made to CodePipeline, the IP address from which the request was made, who made the request, when it was made, and additional details.

https://docs.aws.amazon.com/codepipeline/latest/userguide/monitoring-cloudtrail-logs.html

---

**d262e67** (Mon 01 Jul 2024 01:09) - *Upvotes: 3*
C. because actions performed by the security team are api calls. And api calls go into CloudTrail, if you want to retain them we have to send them into an S3 bucket.

https://docs.aws.amazon.com/codepipeline/latest/userguide/monitoring-cloudtrail-logs.html

---


<br/>

## Question 181

*Date: Dec. 29, 2023, 7:57 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company requires its internal business teams to launch resources through pre-approved AWS CloudFormation templates only. The security team requires automated monitoring when resources drift from their expected state.

Which strategy should be used to meet these requirements?

**Options:**
- A. Allow users to deploy CloudFormation stacks using a CloudFormation service role only. Use CloudFormation drift detection to detect when resources have drifted from their expected state.
- B. Allow users to deploy CloudFormation stacks using a CloudFormation service role only. Use AWS Config rules to detect when resources have drifted from their expected state.
- C. Allow users to deploy CloudFormation stacks using AWS Service Catalog only. Enforce the use of a launch constraint. Use AWS Config rules to detect when resources have drifted from their expected state.
- D. Allow users to deploy CloudFormation stacks using AWS Service Catalog only. Enforce the use of a template constraint. Use Amazon EventBridge notifications to detect when resources have drifted from their expected state.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Jordarlu** (Tue 08 Oct 2024 20:55) - *Upvotes: 2*
In the Option A, Drift detection must be run manually or scheduled, which doesn't fully meet the requirement for "automated monitoring."

---

**jamesf** (Tue 30 Jul 2024 08:31) - *Upvotes: 1*
Keypoint: AWS Config for drift detection

---

**c3518fc** (Thu 25 Apr 2024 11:35) - *Upvotes: 3*
Checks if the actual configuration of a AWS CloudFormation (AWS CloudFormation) stack differs, or has drifted, from the expected configuration. A stack is considered to have drifted if one or more of its resources differ from their expected configuration. https://docs.aws.amazon.com/config/latest/developerguide/cloudformation-stack-drift-detection-check.html

---

**dkp** (Sun 14 Apr 2024 02:49) - *Upvotes: 2*
anwer c

---

**DanShone** (Sat 16 Mar 2024 12:08) - *Upvotes: 3*
C - Service Catalog + AWS Config

---

**thanhnv142** (Mon 12 Feb 2024 12:09) - *Upvotes: 3*
C is correct: <pre-approved AWS CloudFormation templates only> means we need service catalog
A and B: < Allow users to deploy CloudFormation stacks using a CloudFormation service role only>: With service role, users can modify anything in the template
D: Eventbridge cannot detect drift

---

**twogyt** (Thu 18 Jan 2024 21:26) - *Upvotes: 2*
Use config for drift detection

---

**a54b16f** (Fri 12 Jan 2024 21:14) - *Upvotes: 3*
Config for drift detection

---

**d262e67** (Mon 01 Jan 2024 02:17) - *Upvotes: 4*
You can use AWS Managed Config cloudformation-stack-drift-detection-check rule to evaluate drift in CloudFormation stacks.

https://docs.aws.amazon.com/config/latest/developerguide/cloudformation-stack-drift-detection-check.html

---

**PrasannaBalaji** (Sat 30 Dec 2023 06:44) - *Upvotes: 1*
C is correct

---


<br/>

## Question 182

*Date: Dec. 29, 2023, 3:22 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has multiple development groups working in a single shared AWS account. The senior manager of the groups wants to be alerted via a third-party API call when the creation of resources approaches the service limits for the account.

Which solution will accomplish this with the LEAST amount of development effort?

**Options:**
- A. Create an Amazon EventBridge rule that runs periodically and targets an AWS Lambda function. Within the Lambda function, evaluate the current state of the AWS environment and compare deployed resource values to resource limits on the account. Notify the senior manager if the account is approaching a service limit.
- B. Deploy an AWS Lambda function that refreshes AWS Trusted Advisor checks, and configure an Amazon EventBridge rule to run the Lambda function periodically. Create another EventBridge rule with an event pattern matching Trusted Advisor events and a target Lambda function. In the target Lambda function, notify the senior manager.
- C. Deploy an AWS Lambda function that refreshes AWS Health Dashboard checks, and configure an Amazon EventBridge rule to run the Lambda function periodically. Create another EventBridge rule with an event pattern matching Health Dashboard events and a target Lambda function. In the target Lambda function, notify the senior manager.
- D. Add an AWS Config custom rule that runs periodically, checks the AWS service limit status, and streams notifications to an Amazon Simple Notification Service (Amazon SNS) topic. Deploy an AWS Lambda function that notifies the senior manager, and subscribe the Lambda function to the SNS topic.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**c3518fc** (Fri 25 Oct 2024 12:02) - *Upvotes: 4*
Understanding your service limits (and how close you are to them) is an important part of managing your AWS deployments – continuous monitoring allows you to request limit increases or shut down resources before the limit is reached.

One of the easiest ways to do this is via AWS Trusted Advisor’s Service Limit Dashboard, which currently covers 39 limits across 10 services. https://aws.amazon.com/blogs/mt/monitoring-service-limits-with-trusted-advisor-and-amazon-cloudwatch/

---

**dkp** (Mon 14 Oct 2024 02:58) - *Upvotes: 2*
answer b

---

**DanShone** (Mon 16 Sep 2024 11:07) - *Upvotes: 2*
B - Trusted Advisor

---

**twogyt** (Thu 18 Jul 2024 20:32) - *Upvotes: 3*
Trusted advisor

---

**a54b16f** (Fri 12 Jul 2024 20:16) - *Upvotes: 3*
service quote limit === trusted advisor

---

**yuliaqwerty** (Fri 12 Jul 2024 15:33) - *Upvotes: 2*
Answer B https://aws.amazon.com/blogs/mt/monitoring-service-limits-with-trusted-advisor-and-amazon-cloudwatch/

---

**d262e67** (Mon 01 Jul 2024 01:20) - *Upvotes: 1*
Trusted advisor checks the service limits

---

**csG13** (Sat 29 Jun 2024 18:59) - *Upvotes: 1*
It's B

---

**PrasannaBalaji** (Sat 29 Jun 2024 14:22) - *Upvotes: 1*
B is correct

---


<br/>

## Question 183

*Date: Dec. 29, 2023, 3:26 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer is setting up a container-based architecture. The engineer has decided to use AWS CloudFormation to automatically provision an Amazon ECS cluster and an Amazon EC2 Auto Scaling group to launch the EC2 container instances. After successfully creating the CloudFormation stack, the engineer noticed that, even though the ECS cluster and the EC2 instances were created successfully and the stack finished the creation, the EC2 instances were associating with a different cluster.

How should the DevOps engineer update the CloudFormation template to resolve this issue?

**Options:**
- A. Reference the EC2 instances in the AWS::ECS::Cluster resource and reference the ECS cluster in the AWS::ECS::Service resource.
- B. Reference the ECS cluster in the AWS::AutoScaling::LaunchConfiguration resource of the UserData property.
- C. Reference the ECS cluster in the AWS::EC2::Instance resource of the UserData property.
- D. Reference the ECS cluster in the AWS::CloudFormation::CustomResource resource to trigger an AWS Lambda function that registers the EC2 instances with the appropriate ECS cluster.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Jonalb** (Thu 10 Jul 2025 20:00) - *Upvotes: 1*
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ecs-cluster.html

---

**Aquaman** (Fri 22 Nov 2024 10:50) - *Upvotes: 1*
B is correct according to this block
"UserData": {
"Fn::Base64": {
"Fn::Sub": "#!/bin/bash -xe\n echo ECS_CLUSTER=${ECSCluster} >> /etc/ecs/ecs.config\n yum install -y aws-cfn-bootstrap\n /opt/aws/bin/cfn-init -v --stack ${AWS::StackId} --resource ContainerInstances --configsets full_install --region ${AWS::Region} &\n"
}
},
in https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ecs-cluster.html

---

**jamesf** (Tue 30 Jul 2024 08:37) - *Upvotes: 1*
Link the Auto Scaling Group with ECS Cluster

---

**seetpt** (Thu 02 May 2024 14:15) - *Upvotes: 1*
B for me

---

**dkp** (Sun 14 Apr 2024 03:08) - *Upvotes: 2*
answer B

---

**thanhnv142** (Mon 12 Feb 2024 13:19) - *Upvotes: 3*
B is correct: <even though the ECS cluster and the EC2 instances were created successfully and the stack finished the creation, the EC2 instances were associating with a different cluster> means we need to link the auto scaling group to the ECS cluster
A: incorrect. AWS::ECS::Cluster creates an ECS cluster. AWS::ECS::Service creates its services. However, it does not link the EC2 auto scaling group to the ECS cluster
C: incorrect. AWS::EC2::Instance creates an EC2 instance
D: incorrect. AWS::CloudFormation::CustomResource creates CustomResource

---

**a54b16f** (Fri 12 Jan 2024 21:28) - *Upvotes: 3*
B, here is a sample code https://github.com/thinegan/cloudformation-project2/blob/master/infrastructure/ecs-autoscaling-appserver.yaml

---

**ozansenturk** (Tue 02 Jan 2024 12:09) - *Upvotes: 3*
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-autoscaling-launchconfiguration.html#aws-resource-autoscaling-launchconfiguration--examples

---

**PrasannaBalaji** (Fri 29 Dec 2023 15:26) - *Upvotes: 2*
B is correct

---


<br/>

## Question 184

*Date: Dec. 29, 2023, 3:29 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer is implementing governance controls for a company that requires its infrastructure to be housed within the United States. The engineer must restrict which AWS Regions can be used, and ensure an alert is sent as soon as possible if any activity outside the governance policy takes place. The controls should be automatically enabled on any new Region outside the United States (US).

Which combination of actions will meet these requirements? (Choose two.)

**Options:**
- A. Create an AWS Organizations SCP that denies access to all non-global services in non-US Regions. Attach the policy to the root of the organization.
- B. Configure AWS CloudTrail to send logs to Amazon CloudWatch Logs and enable it for all Regions. Use a CloudWatch Logs metric filter to send an alert on any service activity in non-US Regions.
- C. Use an AWS Lambda function that checks for AWS service activity and deploy it to all Regions. Write an Amazon EventBridge rule that runs the Lambda function every hour, sending an alert if activity is found in a non-US Region.
- D. Use an AWS Lambda function to query Amazon Inspector to look for service activity in non-US Regions and send alerts if any activity is found.
- E. Write an SCP using the aws:RequestedRegion condition key limiting access to US Regions. Apply the policy to all users, groups, and roles.

> **Suggested Answer:** AB
> **Community Vote:** AB (91%), 9%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**kiwtirApp** (Fri 22 Nov 2024 21:35) - *Upvotes: 2*
Option A suggests creating an AWS Organizations SCP that denies access to all non-global services in non-US Regions. This is a valid approach.
Option B recommends configuring AWS CloudTrail to send logs to Amazon CloudWatch Logs and enabling it for all Regions.

---

**seetpt** (Sat 02 Nov 2024 15:15) - *Upvotes: 1*
AB for me

---

**dkp** (Mon 14 Oct 2024 03:10) - *Upvotes: 2*
A& B are correct answer

---

**DanShone** (Mon 16 Sep 2024 11:04) - *Upvotes: 4*
A - SCP to restrict
B - CloudTrail to monitor

---

**thanhnv142** (Mon 12 Aug 2024 12:24) - *Upvotes: 4*
A and B are correct: SCP to restrict and AWS cloutrail to monitor
C: Lambda cannot check AWS service activity
D: AWS inspector has nothing to do here
E: <Apply the policy to all users, groups, and roles>: cannot assign a SCP to all users, groups and roles.

---

**kiwtirApp** (Fri 22 Nov 2024 21:31) - *Upvotes: 1*
Your logic for SCP not applying to users, groups and roles is incorrect. SCP can be applied to users and roles. Groups will therefore be indirectly affected.

---

**twogyt** (Thu 18 Jul 2024 20:36) - *Upvotes: 2*
It's A and B

---

**d262e67** (Mon 01 Jul 2024 01:30) - *Upvotes: 2*
A & B are correct.

---

**Alagong** (Sun 30 Jun 2024 02:32) - *Upvotes: 2*
A & B Correct
https://www.examtopics.com/discussions/amazon/view/47872-exam-aws-devops-engineer-professional-topic-1-question-260/

---

**csG13** (Sat 29 Jun 2024 19:01) - *Upvotes: 2*
It's A & B

---


<br/>

## Question 185

*Date: Dec. 29, 2023, 3:36 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company sells products through an ecommerce web application. The company wants a dashboard that shows a pie chart of product transaction details. The company wants to integrate the dashboard with the company's existing Amazon CloudWatch dashboards.

Which solution will meet these requirements with the MOST operational efficiency?

**Options:**
- A. Update the ecommerce application to emit a JSON object to a CloudWatch log group for each processed transaction. Use CloudWatch Logs Insights to query the log group and to visualize the results in a pie chart format. Attach the results to the desired CloudWatch dashboard.
- B. Update the ecommerce application to emit a JSON object to an Amazon S3 bucket for each processed transaction. Use Amazon Athena to query the S3 bucket and to visualize the results in a pie chart format. Export the results from Athena. Attach the results to the desired CloudWatch dashboard.
- C. Update the ecommerce application to use AWS X-Ray for instrumentation. Create a new X-Ray subsegment. Add an annotation for each processed transaction. Use X-Ray traces to query the data and to visualize the results in a pie chart format. Attach the results to the desired CloudWatch dashboard.
- D. Update the ecommerce application to emit a JSON object to a CloudWatch log group for each processed transaction. Create an AWS Lambda function to aggregate and write the results to Amazon DynamoDB. Create a Lambda subscription filter for the log file. Attach the results to the desired CloudWatch dashboard.

> **Suggested Answer:** A
> **Community Vote:** A (92%), 4%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Tue 06 Aug 2024 07:23) - *Upvotes: 1*
Pie Chart support by Cloudwatch
https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph_a_metric.html

---

**dkp** (Sun 14 Apr 2024 03:15) - *Upvotes: 2*
A: the pie chart can be achieved using cloud watch log insights

---

**WhyIronMan** (Sun 31 Mar 2024 01:07) - *Upvotes: 2*
A is obvious

---

**DanShone** (Sat 16 Mar 2024 12:03) - *Upvotes: 3*
A is the correct answer

---

**master9** (Fri 15 Mar 2024 05:58) - *Upvotes: 1*
update in January 2022, Amazon CloudWatch does not provide native support for generating pie charts directly within the CloudWatch console. However, you can achieve this by exporting CloudWatch Logs to Amazon S3 and then analyzing the logs using other AWS services or third-party tools that support pie chart visualization.

---

**thanhnv142** (Mon 12 Feb 2024 13:32) - *Upvotes: 4*
A is correct: <a pie chart of product transaction details> means we need to collect application logs and analyze the log. The recommended way is to use cloudwatch logs and cloudwatch logs insights
B: Should not use S3. We should use cloudwatch logs so that we can integrate easily to the existing cloudwatch dashboards
C: X-ray is for troubleshooting, which is short-term, not for long-term app monitoring
D: Should not use lambda and should not write the results to dynamodb, which is primarily used to store session data
https://www.examtopics.com/exams/amazon/aws-certified-devops-engineer-professional-dop-c02/view/1/#

---

**a54b16f** (Fri 12 Jan 2024 21:34) - *Upvotes: 2*
cloudwatch insight can generate graph

---

**davdan99** (Wed 10 Jan 2024 20:19) - *Upvotes: 2*
Go For A
https://www.delenamalan.co.za/til/2021-06-07-cloudwatch-insights-graph.html

---

**d262e67** (Mon 01 Jan 2024 02:33) - *Upvotes: 2*
Must be A

---

**csG13** (Fri 29 Dec 2023 20:05) - *Upvotes: 2*
it's A

---


<br/>

## Question 186

*Date: Dec. 29, 2023, 3:44 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is launching an application. The application must use only approved AWS services. The account that runs the application was created less than 1 year ago and is assigned to an AWS Organizations OU.

The company needs to create a new Organizations account structure. The account structure must have an appropriate SCP that supports the use of only services that are currently active in the AWS account. The company will use AWS Identity and Access Management (IAM) Access Analyzer in the solution.

Which solution will meet these requirements?

**Options:**
- A. Create an SCP that allows the services that IAM Access Analyzer identifies. Create an OU for the account. Move the account into the new OU. Attach the new SCP to the new OU. Detach the default FullAWSAccess SCP from the new OU.
- B. Create an SCP that denies the services that IAM Access Analyzer identifies. Create an OU for the account. Move the account into the new OU. Attach the new SCP to the new OU.
- C. Create an SCP that allows the services that IAM Access Analyzer identifies. Attach the new SCP to the organization's root.
- D. Create an SCP that allows the services that IAM Access Analyzer identifies. Create an OU for the account. Move the account into the new OU. Attach the new SCP to the management account. Detach the default FullAWSAccess SCP from the new OU.

> **Suggested Answer:** A
> **Community Vote:** A (88%), 13%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Tue 30 Jul 2024 08:56) - *Upvotes: 1*
This question and answers are confusing but I think "Management Account" in option D and "the account" are different accounts.
Hence, A is correct.

---

**dkp** (Sun 14 Apr 2024 03:20) - *Upvotes: 2*
Answer A

---

**DanShone** (Sat 16 Mar 2024 12:02) - *Upvotes: 3*
A is correct

---

**thanhnv142** (Mon 12 Feb 2024 13:44) - *Upvotes: 3*
A is correct: <AWS Identity and Access Management (IAM) Access Analyzer> is a solution for least privilege, which is allow some, deny all. So we need to defy allowed permissions and then remove the <default FullAWSAccess>
B: least privilege is allow some, deny all, not allow all, deny some.
C: The step mentioned would have no effect. The root already had default FullAWSAccess SCP. Allowing some more services does not change anything
D: <Attach the new SCP to the management account>: Cannot attach a SCP to an account

---

**thanhnv142** (Mon 12 Feb 2024 14:25) - *Upvotes: 3*
Correct: D - We can attach SCP to an account. But it only affects an account. We need to impose the scp rule on the entire accounts in the new OU

---

**denccc** (Thu 18 Jan 2024 09:03) - *Upvotes: 1*
A is correct

---

**kabary** (Mon 01 Jan 2024 23:39) - *Upvotes: 1*
I agree with @d262e67.

---

**d262e67** (Mon 01 Jan 2024 02:45) - *Upvotes: 3*
It's A. To those who selected D, why would you assign the SCP to the management account??? The application account goes into an OU, and the SCP must be associated with that OU, period!

---

**csG13** (Sat 30 Dec 2023 05:54) - *Upvotes: 1*
D is the right answer

---

**GokSK** (Sun 31 Dec 2023 14:06) - *Upvotes: 1*
Could you please explain more?

---


<br/>

## Question 187

*Date: Dec. 29, 2023, 3:47 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has multiple development teams in different business units that work in a shared single AWS account. All Amazon EC2 resources that are created in the account must include tags that specify who created the resources. The tagging must occur within the first hour of resource creation.

A DevOps engineer needs to add tags to the created resources that include the user ID that created the resource and the cost center ID. The DevOps engineer configures an AWS Lambda function with the cost center mappings to tag the resources. The DevOps engineer also sets up AWS CloudTrail in the AWS account. An Amazon S3 bucket stores the CloudTrail event logs.

Which solution will meet the tagging requirements?

**Options:**
- A. Create an S3 event notification on the S3 bucket to invoke the Lambda function for s3:ObjectTagging:Put events. Enable bucket versioning on the S3 bucket.
- B. Enable server access logging on the S3 bucket. Create an S3 event notification on the S3 bucket for s3:ObjectTagging:* events.
- C. Create a recurring hourly Amazon EventBridge scheduled rule that invokes the Lambda function. Modify the Lambda function to read the logs from the S3 bucket.
- D. Create an Amazon EventBridge rule that uses Amazon EC2 as the event source. Configure the rule to match events delivered by CloudTrail. Configure the rule to target the Lambda function.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**dkp** (Mon 14 Oct 2024 03:27) - *Upvotes: 2*
D looks more relevant

---

**DanShone** (Mon 16 Sep 2024 10:59) - *Upvotes: 3*
Answer is D.

---

**thanhnv142** (Tue 13 Aug 2024 14:50) - *Upvotes: 3*
D is corect.
A and B: irrelevant
C: using lambda to read log is a bad idea because it takes a lot of time.

---

**a54b16f** (Fri 12 Jul 2024 20:42) - *Upvotes: 4*
the trigger event is the EC2 creation, so D

---

**kabary** (Mon 01 Jul 2024 22:50) - *Upvotes: 2*
Answer is D.

The answer must have CloudTrail for EC2 tagging.

---

**csG13** (Sat 29 Jun 2024 19:57) - *Upvotes: 3*
It's D. It says within an hour so it can't be C, looping over the S3 logs may take a lot (apparently there is also consideration about the 15mins limit of lambda)

---

**PrasannaBalaji** (Sat 29 Jun 2024 14:47) - *Upvotes: 1*
C looks correct

---


<br/>

## Question 188

*Date: Dec. 29, 2023, 3:50 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company runs an application for multiple environments in a single AWS account. An AWS CodePipeline pipeline uses a development Amazon Elastic Container Service (Amazon ECS) cluster to test an image for the application from an Amazon Elastic Container Registry (Amazon ECR) repository. The pipeline promotes the image to a production ECS cluster.

The company needs to move the production cluster into a separate AWS account in the same AWS Region. The production cluster must be able to download the images over a private connection.

Which solution will meet these requirements?

**Options:**
- A. Use Amazon ECR VPC endpoints and an Amazon S3 gateway endpoint. In the separate AWS account, create an ECR repository. Set the repository policy to allow the production ECS tasks to pull images from the main AWS account. Configure the production ECS task execution role to have permission to download the image from the ECR repository.
- B. Set a repository policy on the production ECR repository in the main AWS account. Configure the repository policy to allow the production ECS tasks in the separate AWS account to pull images from the main account. Configure the production ECS task execution role to have permission to download the image from the ECR repository.
- C. Configure ECR private image replication in the main AWS account. Activate cross-account replication. Define the destination account ID of the separate AWS account.
- D. Use Amazon ECR VPC endpoints and an Amazon S3 gateway endpoint. Set a repository policy on the production ECR repository in the main AWS account. Configure the repository policy to allow the production ECS tasks in the separate AWS account to pull images from the main account. Configure the production ECS task execution role to have permission to download the image from the ECR repository.

> **Suggested Answer:** D
> **Community Vote:** D (78%), C (17%), 4%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Sun 30 Mar 2025 13:19) - *Upvotes: 1*
It allows the production ECS tasks in a different AWS account to pull images from the ECR repository in the main AWS account with the proper access control set via repository policies and ECS task execution role permissions

---

**GripZA** (Thu 15 Aug 2024 17:49) - *Upvotes: 1*
"You can configure your Amazon ECR private registry to support the replication of your repositories. Amazon ECR supports both cross-Region and cross-account replication"

https://docs.aws.amazon.com/AmazonECR/latest/userguide/replication.html

If cross-account replication is enabled, then for Cross-account replication, choose the cross-account replication setting for the registry. For Destination account, enter the account ID for the destination account and one or more Destination regions to replicate to. Choose Destination account + to configure additional accounts as replication destinations.

https://docs.aws.amazon.com/AmazonECR/latest/userguide/registry-settings-configure.html

---

**GripZA** (Mon 21 Apr 2025 10:07) - *Upvotes: 1*
changing to D to meet private conn requirement

---

**hzaki** (Fri 26 Jul 2024 14:41) - *Upvotes: 2*
The correct answer is A: The company needs to move the production cluster into a separate AWS account in the same AWS Region. The repository is in a separate account, and permissions are set there, giving better isolation between environments.

---

**hzaki** (Mon 02 Sep 2024 19:54) - *Upvotes: 1*
I will go with option C because cross-account replication is straightforward and secure.

---

**Gomer** (Thu 04 Jul 2024 00:29) - *Upvotes: 1*
Based the references provided, it would appear that both "C" and "D" could work to distribute an image, EXCEPT for the ""private connection" requirement. It's also seems like a cleaner solution to just rely on one ECR repository, rather than replicate repo's to other accounts in same region.

---

**c3518fc** (Thu 25 Apr 2024 13:00) - *Upvotes: 2*
https://docs.aws.amazon.com/AmazonECR/latest/userguide/vpc-endpoints.html

---

**dkp** (Sun 14 Apr 2024 03:34) - *Upvotes: 2*
Ans D:
Amazon ECS tasks to pull private images from Amazon ECR, you must create a gateway endpoint for Amazon S3. The gateway endpoint is required because Amazon ECR uses Amazon S3 to store your image layers.

---

**DanShone** (Sat 16 Mar 2024 11:58) - *Upvotes: 2*
ECR VPC endpoints is needed to meet "download the images over a private connection."

---

**dzn** (Mon 04 Mar 2024 10:52) - *Upvotes: 3*
Use ECR VPC endpoints is necessary to meet the below requirements.
`download the images over a private connection.`

---


<br/>

## Question 189

*Date: Dec. 29, 2023, 9:48 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company needs to ensure that flow logs remain configured for all existing and new VPCs in its AWS account. The company uses an AWS CloudFormation stack to manage its VPCs. The company needs a solution that will work for any VPCs that any IAM user creates.

Which solution will meet these requirements?

**Options:**
- A. Add the AWS::EC2::FlowLog resource to the CloudFormation stack that creates the VPCs.
- B. Create an organization in AWS Organizations. Add the company's AWS account to the organization. Create an SCP to prevent users from modifying VPC flow logs.
- C. Turn on AWS Config. Create an AWS Config rule to check whether VPC flow logs are turned on. Configure automatic remediation to turn on VPC flow logs.
- D. Create an IAM policy to deny the use of API calls for VPC flow logs. Attach the IAM policy to all IAM users.

> **Suggested Answer:** C
> **Community Vote:** C (94%), 6%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Mon 12 Aug 2024 13:02) - *Upvotes: 6*
C is correct: This will monitor and remediate all existing and new VPCs
A: AWS::EC2::FlowLog: This is used to configured flow log, not monitor it
B: SCP wont address existing VPCs
D: IAM policy has nothing to do here

---

**dkp** (Mon 14 Oct 2024 03:41) - *Upvotes: 2*
option c is correct

---

**d262e67** (Tue 02 Jul 2024 20:41) - *Upvotes: 4*
SCPs only prevent people from changing the VPC flow log configuration. It doesn't ensure it's on.

---

**ozansenturk** (Tue 02 Jul 2024 12:16) - *Upvotes: 1*
both AWS config and SCP work, however, SCP is more preventive compared to proactive AWS Config. therefore, I opted B.

---

**kabary** (Mon 01 Jul 2024 22:56) - *Upvotes: 2*
Answer is C.

---

**csG13** (Sat 29 Jun 2024 20:48) - *Upvotes: 3*
It's C, here is a reference how to do it:

https://aws.amazon.com/blogs/mt/how-to-enable-vpc-flow-logs-automatically-using-aws-config-rules/

---


<br/>

## Question 190

*Date: Dec. 29, 2023, 10:04 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company's application teams use AWS CodeCommit repositories for their applications. The application teams have repositories in multiple AWS accounts. All accounts are in an organization in AWS Organizations.

Each application team uses AWS IAM Identity Center (AWS Single Sign-On) configured with an external IdP to assume a developer IAM role. The developer role allows the application teams to use Git to work with the code in the repositories.

A security audit reveals that the application teams can modify the main branch in any repository. A DevOps engineer must implement a solution that allows the application teams to modify the main branch of only the repositories that they manage.

Which combination of steps will meet these requirements? (Choose three.)

**Options:**
- A. Update the SAML assertion to pass the user's team name. Update the IAM role's trust policy to add an access-team session tag that has the team name.
- B. Create an approval rule template for each team in the Organizations management account. Associate the template with all the repositories. Add the developer role ARN as an approver.
- C. Create an approval rule template for each account. Associate the template with all repositories. Add the "aws:ResourceTag/access-team": "$ ;{aws:PrincipalTag/access-team}" condition to the approval rule template.
- D. For each CodeCommit repository, add an access-team tag that has the value set to the name of the associated team.
- E. Attach an SCP to the accounts. Include the following statement:
- F. Create an IAM permissions boundary in each account. Include the following statement:

> **Suggested Answer:** ADE
> **Community Vote:** ADE (72%), ADF (17%), 11%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**Diego1414** (Sun 25 Feb 2024 20:44) - *Upvotes: 7*
A- SAML Assertion
D - Tag the resource
E - Will work with D above ad condition is based on resource tag

---

**jamesf** (Tue 30 Jul 2024 09:18) - *Upvotes: 3*
I go for ADE

Option E, this SCP ensures that only users from the team that manages a repository can modify the main branch by using the access-team tag. It denies actions if the team tags do not match.

---

**kiwtirApp** (Wed 22 May 2024 21:43) - *Upvotes: 1*
Why are people choosing option E?
As per the requirement:
"A company's application teams use AWS CodeCommit repositories for their applications. The application teams have repositories in multiple AWS
accounts. All accounts are in an organization in AWS Organizations.
Each application team uses AWS IAM Identity Center (AWS Single Sign-On) configured with an external IdP to assume a developer IAM role. The
developer role allows the application teams to use Git to work with the code in the repositories"
Shouldn't we then ALLOW GitPush, PutFile and Merge*?
I think it should be ADF.

---

**kiwtirApp** (Wed 22 May 2024 21:44) - *Upvotes: 1*
Sorry, i wanted to highlight this:
"A DevOps engineer must implement a solution that allows the application teams to modify the main branch of only the repositories that they manage."

This is why it should be ADF.

---

**Onolisk** (Tue 28 May 2024 20:40) - *Upvotes: 4*
they can already access all the repositories, the requirement is to scope down their access hense DENY not ALLOW

---

**vdxiii** (Tue 18 Jun 2024 14:10) - *Upvotes: 2*
F is Wrong. E is correct. This SCP ensures that only users from the team that manages a repository can modify the main branch by using the access-team tag. It denies actions if the team tags do not match.

---

**seetpt** (Thu 02 May 2024 14:18) - *Upvotes: 2*
ADE for me

---

**dkp** (Sun 14 Apr 2024 03:46) - *Upvotes: 3*
ADE seems more appropriate

---

**ogerber** (Sun 17 Mar 2024 19:25) - *Upvotes: 2*
ADF, 100%

---

**ogerber** (Mon 25 Mar 2024 18:05) - *Upvotes: 4*
Correction, its ADE:
Permissions boundaries (Option F) are more granular and would be set on each IAM role individually. While they could achieve a similar effect, they are not as broad in scope as SCPs and would require setting up on every IAM role, which could be less efficient than a blanket policy across the organization with an SCP

---


<br/>

## Question 191

*Date: Dec. 29, 2023, 10:29 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses AWS WAF to protect its cloud infrastructure. A DevOps engineer needs to give an operations team the ability to analyze log messages from AWS WAF. The operations team needs to be able to create alarms for specific patterns in the log output.

Which solution will meet these requirements with the LEAST operational overhead?

**Options:**
- A. Create an Amazon CloudWatch Logs log group. Configure the appropriate AWS WAF web ACL to send log messages to the log group. Instruct the operations team to create CloudWatch metric filters.
- B. Create an Amazon OpenSearch Service cluster and appropriate indexes. Configure an Amazon Kinesis Data Firehose delivery stream to stream log data to the indexes. Use OpenSearch Dashboards to create filters and widgets.
- C. Create an Amazon S3 bucket for the log output. Configure AWS WAF to send log outputs to the S3 bucket. Instruct the operations team to create AWS Lambda functions that detect each desired log message pattern. Configure the Lambda functions to publish to an Amazon Simple Notification Service (Amazon SNS) topic.
- D. Create an Amazon S3 bucket for the log output. Configure AWS WAF to send log outputs to the S3 bucket. Use Amazon Athena to create an external table definition that fits the log message pattern. Instruct the operations team to write SQL queries and to create Amazon CloudWatch metric filters for the Athena queries.

> **Suggested Answer:** A
> **Community Vote:** A (84%), D (16%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**GripZA** (Thu 15 Aug 2024 18:23) - *Upvotes: 1*
To send logs to Amazon CloudWatch Logs, you create a CloudWatch Logs log group. When you enable logging in AWS WAF, you provide the log group ARN. After you enable logging for your web ACL, AWS WAF delivers logs to the CloudWatch Logs log group in log streams.

https://docs.aws.amazon.com/waf/latest/developerguide/logging-cw-logs.html

---

**c3518fc** (Thu 25 Apr 2024 15:57) - *Upvotes: 4*
https://docs.aws.amazon.com/waf/latest/developerguide/logging-management.html

---

**dkp** (Sun 14 Apr 2024 03:51) - *Upvotes: 2*
A & D can work, least operation overheard is A

---

**DanShone** (Sat 16 Mar 2024 11:51) - *Upvotes: 2*
LEAST operational overhead = A

---

**thanhnv142** (Mon 12 Feb 2024 14:32) - *Upvotes: 3*
D is correct: We have two tasks: collect log and analyze data. S3 bucket can store log and athena is for log analysis.
A: This options does not mention of log analysis. Additionally, AWS WAF web ACL cannot send log to AWS logs group
B: OpenSearch Service and Amazon Kinesis Data Firehose are used for other purposes. They are high-end features and cost a lots.
C: Should not use lambda to analys log

---

**kiwtirApp** (Wed 22 May 2024 21:49) - *Upvotes: 1*
You fail to notice that the question is asking about LEAST operational overhead. Therefore, it should be A.

---

**a54b16f** (Mon 15 Jan 2024 22:53) - *Upvotes: 1*
cloudwatch

---

**a54b16f** (Fri 12 Jan 2024 21:53) - *Upvotes: 1*
cloudwatch for WAF logging

---

**ozansenturk** (Tue 02 Jan 2024 13:32) - *Upvotes: 1*
https://docs.aws.amazon.com/waf/latest/developerguide/logging.html

---

**kabary** (Tue 02 Jan 2024 00:08) - *Upvotes: 1*
Answer is A based on the following AWS documentation:

https://docs.aws.amazon.com/waf/latest/developerguide/web-acl-creating.html

---


<br/>

## Question 192

*Date: Dec. 29, 2023, 4:01 p.m.
Disclaimers:
- ExamTopics website is not rel*

A software team is using AWS CodePipeline to automate its Java application release pipeline. The pipeline consists of a source stage, then a build stage, and then a deploy stage. Each stage contains a single action that has a runOrder value of 1.

The team wants to integrate unit tests into the existing release pipeline. The team needs a solution that deploys only the code changes that pass all unit tests.

Which solution will meet these requirements?

**Options:**
- A. Modify the build stage. Add a test action that has a runOrder value of 1. Use AWS CodeDeploy as the action provider to run unit tests.
- B. Modify the build stage. Add a test action that has a runOrder value of 2. Use AWS CodeBuild as the action provider to run unit tests.
- C. Modify the deploy stage. Add a test action that has a runOrder value of 1. Use AWS CodeDeploy as the action provider to run unit tests.
- D. Modify the deploy stage. Add a test action that has a runOrder value of 2. Use AWS CodeBuild as the action provider to run unit tests.

> **Suggested Answer:** B
> **Community Vote:** B (95%), 5%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Mon 12 Feb 2024 14:36) - *Upvotes: 5*
B is correct: Runorder value of 2 ensure that we do unit tests after we build artifacts.
A: The unit tests would run in parellel with the build step, which is incorrect. We can only test after we have done building
C and D: The unit tests would not run before the deploy step.

---

**GripZA** (Mon 21 Apr 2025 10:52) - *Upvotes: 1*
"runOrder is a positive integer that indicates the run order of the action within the stage. Parallel actions in the stage are shown as having the same integer. For example, two actions with a run order of two will run in parallel after the first action in the stage runs."

https://docs.aws.amazon.com/codepipeline/latest/userguide/action-requirements.html

---

**Gomer** (Fri 05 Jul 2024 01:00) - *Upvotes: 1*
C: (YES) aws:SecureTransport = data in transit (TLS/HTTPS)
D: (NO) ...server-side-encryption-aws... = data at rest (in S3)

---

**c3518fc** (Thu 25 Apr 2024 16:00) - *Upvotes: 4*
By modifying the build stage, adding a test action with a runOrder value of 2, and using AWS CodeBuild as the action provider to run unit tests, the solution ensures that unit tests are executed as part of the build process and that only the code changes that pass all unit tests are deployed, meeting the requirements of the software team.

---

**dkp** (Sun 14 Apr 2024 03:53) - *Upvotes: 2*
B is correct

---

**DanShone** (Sat 16 Mar 2024 11:51) - *Upvotes: 2*
B is correct

---

**Jonalb** (Fri 01 Mar 2024 10:45) - *Upvotes: 3*
B is correct

---

**a54b16f** (Fri 12 Jan 2024 21:54) - *Upvotes: 2*
order of 2 would create sequence order

---

**csG13** (Fri 29 Dec 2023 22:09) - *Upvotes: 2*
it's definitely B

---

**PrasannaBalaji** (Fri 29 Dec 2023 16:01) - *Upvotes: 3*
Option B - The runOrder value of 2 ensures that the test action runs after the build action, allowing the unit tests to be executed only if the build is successful.

---


<br/>

## Question 193

*Date: Dec. 29, 2023, 4:03 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses an organization in AWS Organizations to manage several AWS accounts that the company's developers use. The company requires all data to be encrypted in transit.

Multiple Amazon S3 buckets that were created in developer accounts allow unencrypted connections. A DevOps engineer must enforce encryption of data in transit for all existing S3 buckets that are created in accounts in the organization.

Which solution will meet these requirements?

**Options:**
- A. Use AWS CloudFormation StackSets to deploy an AWS Network Firewall firewall to each account. Route all outbound requests from the AWS environment through the firewall. Deploy a policy to block access to all outbound requests on port 80.
- B. Use AWS CloudFormation StackSets to deploy an AWS Network Firewall firewall to each account. Route all inbound requests to the AWS environment through the firewall. Deploy a policy to block access to all inbound requests on port 80.
- C. Turn on AWS Config for the organization. Deploy a conformance pack that uses the s3-bucket-ssl-requests-only managed rule and an AWS Systems Manager Automation runbook. Use a runbook that adds a bucket policy statement to deny access to an S3 bucket when the value of the aws:SecureTransport condition key is false.
- D. Turn on AWS Config for the organization. Deploy a conformance pack that uses the s3-bucket-ssl-requests-only managed rule and an AWS Systems Manager Automation runbook. Use a runbook that adds a bucket policy statement to deny access to an S3 bucket when the value of the s3:x-amz-server-side-encryption-aws-kms-key-id condition key is null.

> **Suggested Answer:** C
> **Community Vote:** C (96%), 4%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**kabary** (Tue 02 Jan 2024 00:18) - *Upvotes: 7*
Answer C 100%.

aws:SecureTransport condition this will be allowing only encrypted connections over HTTPS (TLS) --> THIS IS WHAT WE NEED

s3:x-amz-server-side-encryption-aws-kms-key-id --> To require that a particular AWS KMS key be used to encrypt the objects in a bucket. WE DON'T NEED THIS HERE!

---

**thanhnv142** (Mon 12 Feb 2024 14:39) - *Upvotes: 6*
C is correct:
A and B: we should not use AWS Network Firewall firewall here. It is just incorrect
D: s3:x-amz-server-side-encryption-aws-kms-key-id: This is for data encryption at rest, not in transit

---

**dkp** (Sun 14 Apr 2024 03:56) - *Upvotes: 2*
C is correct

---

**Jonalb** (Fri 01 Mar 2024 10:37) - *Upvotes: 3*
C is correct:

---

**ozansenturk** (Tue 02 Jan 2024 13:40) - *Upvotes: 2*
https://repost.aws/knowledge-center/s3-bucket-policy-for-config-rule

---

**hisdlodskfe** (Sat 30 Dec 2023 17:18) - *Upvotes: 2*
C is correct. D is encryption for rest not transit.

---

**csG13** (Fri 29 Dec 2023 22:12) - *Upvotes: 2*
It's C - they want to enforce SSL (i.e., encryption of data in transit).

---

**PrasannaBalaji** (Fri 29 Dec 2023 16:03) - *Upvotes: 1*
option D is correct

---

**flaacko** (Fri 23 Aug 2024 15:34) - *Upvotes: 1*
Option D is wrong as it is for server side encryption and what the question requires is the encryption of data in transit not when the data is at rest within an S3 bucket.

---


<br/>

## Question 194

*Date: Dec. 29, 2023, 4:06 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is reviewing its IAM policies. One policy written by the DevOps engineer has been flagged as too permissive. The policy is used by an AWS Lambda function that issues a stop command to Amazon EC2 instances tagged with Environment: NonProduction over the weekend. The current policy is:



What changes should the engineer make to achieve a policy of least permission? (Choose three.)

**Options:**
- A. Add the following conditional expression:
- B. Change "Resource": "*"to "Resource": "arn:aws:ec2:*:*:instance/*"
- C. Add the following conditional expression:
- D. Add the following conditional expression:
- E. Change "Action": "ec2:*"to "Action": "ec2:StopInstances"
- F. Add the following conditional expression:

> **Suggested Answer:** B
> **Community Vote:** B (62%), A (19%), D (19%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Mon 12 Feb 2024 14:50) - *Upvotes: 8*
B, D and E are correct:
A: This allow all any lambda func to do the task, wont make any change
B: This allow action only on EC2, so it is correct
C: We need to allow action on Ec2 instances tagged with NonProdction only. Using this would grant permissions to other tags as well
D: perfectly correct
E: Only permit stop action, so it is correct
F: irrelevant

---

**vortegon** (Sat 10 Feb 2024 20:12) - *Upvotes: 8*
A,D,E
principalType could be a condition key
https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_condition.html
https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html

---

**rinip86277** (Wed 19 Feb 2025 08:30) - *Upvotes: 2*
BDE seems correct to me.

---

**youonebe** (Tue 24 Dec 2024 14:28) - *Upvotes: 1*
DEF - clearly

---

**3f78595** (Sat 07 Sep 2024 03:07) - *Upvotes: 1*
A, B, D
The engineer should make the following changes to achieve a policy of least permission:

A:Add a condition to ensure that the principal making the request is an AWS Lambda function. This ensures that only Lambda functions can execute this policy.

B:Narrow down the resources by specifying the ARN of EC2 instances instead of allowing all resources. This ensures that the policy only affects EC2 instances.

D:Add a condition to ensure that this policy only applies to EC2 instances tagged with ''Environment: NonProduction''. This ensures that production environments are not affected by this policy.

---

**GripZA** (Thu 15 Aug 2024 18:47) - *Upvotes: 1*
I'll go with BDE

B - restrict resource from wildcard to only "arn:aws:ec2:*:*:instance/*"
D - this condition limits to non Prod only
E - limit actions to "ec2:StopInstances" and not all ec2 actions

as for F, although YOU CAN allow access based on date/time. The typical format is:

"Condition": {
"DateGreaterThan": {"aws:CurrentTime": "2020-04-01T00:00:00Z"},
"DateLessThan": {"aws:CurrentTime": "2020-06-30T23:59:59Z"}

https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws-dates.html

---

**kiko_zhang** (Tue 30 Jul 2024 20:39) - *Upvotes: 4*
DEF.
when using E, there is no need for B.
F: https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws-dates.html

---

**jamesf** (Tue 30 Jul 2024 09:33) - *Upvotes: 2*
BDE are correct.

Option F is irrelevant and can use Amazon EventBridge Rule to execute the Lambda

---

**jamesf** (Tue 30 Jul 2024 09:35) - *Upvotes: 2*
And in question, there is keywords: least permission
So B is needed

---

**ericphl** (Fri 26 Jul 2024 11:29) - *Upvotes: 4*
DEF
E is more prefect answer rather than B.
D. restricted the ec2 using env tag as "nonproduction"
F. support scope of running time which mention in the question.

---


<br/>

## Question 195

*Date: Feb. 7, 2024, 4:58 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is developing an application that will generate log events. The log events consist of five distinct metrics every one tenth of a second and produce a large amount of data.

The company needs to configure the application to write the logs to Amazon Timestream. The company will configure a daily query against the Timestream table.

Which combination of steps will meet these requirements with the FASTEST query performance? (Choose three.)

**Options:**
- A. Use batch writes to write multiple log events in a single write operation.
- B. Write each log event as a single write operation.
- C. Treat each log as a single-measure record.
- D. Treat each log as a multi-measure record.
- E. Configure the memory store retention period to be longer than the magnetic store retention period.
- F. Configure the memory store retention period to be shorter than the magnetic store retention period.

> **Suggested Answer:** ADF
> **Community Vote:** ADF (75%), 14%, 11%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**vortegon** (Sat 10 Feb 2024 20:17) - *Upvotes: 8*
While E suggests configuring the memory store retention period to be longer than the magnetic store retention period, this is typically not aimed at optimizing query performance but rather at keeping data in the faster-access memory store for longer periods, which could be beneficial for workloads requiring frequent access to recent data. However, for the scenario described, focusing on efficient data ingestion methods (A and D) and understanding the role of retention periods (F) provides a balanced approach to achieving the fastest query performance for daily queries.

---

**auxwww** (Fri 26 Jul 2024 14:18) - *Upvotes: 2*
E fast performance
A writer more throughput
D multi measure means less records to store each data point. Faster query

---

**Gomer** (Fri 05 Jul 2024 06:30) - *Upvotes: 2*
My only hesitation is in regards to how batch writes might improve query performance, other than if the stored data is in a contiguous chunk, that could hep a query later. As far as for multi-measure and more memory, I defer to references:
A: (YES) "When writing data to InfluxDB, write data in batches to minimize the network overhead related to every write request."
D: (YES) "Multi-measure records results in lower query latency for most query types when compared to single-measure records."
E: (YES) "The memory store is optimized for high throughput data writes and fast point-in-time queries."
F: (NO) "The magnetic store is optimized for lower throughput late-arriving data writes, long term data storage, and fast analytical queries."

---

**Gomer** (Fri 05 Jul 2024 06:46) - *Upvotes: 1*
Memory based storage is always going to provide "FASTEST query performance" compared to magnetic storage. You want faster query, provide higher ratio of memory storage compared to magnetic.

---

**didek1986** (Thu 18 Apr 2024 09:30) - *Upvotes: 3*
ADF
A - improve write performance and efficiency
D - query for a specific measure in a multi-measure record, Timestream only scans the relevant measure, not the entire record. This means that even though the record contains multiple measures, the query performance for a specific measure is not negatively impacted. Multi-measure record reduces the number of records that need to be written and subsequently queried, which improve query performance.
F - memory store, which is optimised for write and query performance, is not filled with older data that is not frequently accessed

---

**dkp** (Sun 14 Apr 2024 04:09) - *Upvotes: 3*
ADF seems more relevant

---

**Diego1414** (Sun 25 Feb 2024 21:36) - *Upvotes: 4*
ADF – batch writes, Treat log as multi-measure record, Memory story should be shorter,.
https://aws.amazon.com/blogs/database/improve-query-performance-and-reduce-cost-using-scheduled-queries-in-amazon-timestream/#:~:text=Improve%20query%20performance%20and%20reduce%20cost%20using%20scheduled,6%20Query%20performance%20metrics%20...%207%20Conclusion%20

---

**Ramdi1** (Tue 13 Feb 2024 17:57) - *Upvotes: 3*
A. Batch writes: This significantly reduces overhead associated with individual write operations and improves overall write throughput.
C. Single-measure record: For daily queries summarizing multiple metrics, treating each log as a single record helps Timestream leverage its optimized storage and query processing for single measures.
D. Multi-measure record: While it seems counterintuitive, Timestream performs better with multiple measures within a single record compared to separate records for each metric. This allows for efficient data retrieval and aggregation during queries.

---

**Ramdi1** (Tue 13 Feb 2024 17:57) - *Upvotes: 1*
Options B, E, and F are not recommended for optimal performance:
 
B. Single write operations: This increases overhead and reduces write throughput, negating Timestream's scalability benefits.
E. Longer memory store: While faster for recent data, it increases cost and doesn't impact daily queries focused on older, magnetic store data.
F. Shorter memory store: Reduces cost but sacrifices potential performance gains for frequently accessed recent data, which might not be relevant for daily queries.
 
By combining batch writes, single-measure records, and multi-measure records, the company can achieve the fastest query performance for their daily Timestream use case.

---

**thanhnv142** (Mon 12 Feb 2024 15:14) - *Upvotes: 3*
A,D and F are correct:
A: do job in batch optimize costs and performance
B: should not do single write
C: The app emits multiple records the same time. So it should be multi-measure record, not single one
D: correct
E: Should not do this
F: correct

---


<br/>

## Question 196

*Date: Feb. 5, 2024, 5:45 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer has created an AWS CloudFormation template that deploys an application on Amazon EC2 instances. The EC2 instances run Amazon Linux. The application is deployed to the EC2 instances by using shell scripts that contain user data. The EC2 instances have an IAM instance profile that has an IAM role with the AmazonSSMManagedinstanceCore managed policy attached.

The DevOps engineer has modified the user data in the CloudFormation template to install a new version of the application. The engineer has also applied the stack update. However, the application was not updated on the running EC2 instances. The engineer needs to ensure that the changes to the application are installed on the running EC2 instances.

Which combination of steps will meet these requirements? (Choose two.)

**Options:**
- A. Configure the user data content to use the Multipurpose Internet Mail Extensions (MIME) multipart format. Set the scripts-user parameter to always in the text/cloud-config section.
- B. Refactor the user data commands to use the cfn-init helper script. Update the user data to install and configure the cfn-hup and cfn-init helper scripts to monitor and apply the metadata changes.
- C. Configure an EC2 launch template for the EC2 instances. Create a new EC2 Auto Scaling group. Associate the Auto Scaling group with the EC2 launch template. Use the AutoScalingScheduledAction update policy for the Auto Scaling group.
- D. Refactor the user data commands to use an AWS Systems Manager document (SSM document). Add an AWS CLI command in the user data to use Systems Manager Run Command to apply the SSM document to the EC2 instances.
- E. Refactor the user data command to use an AWS Systems Manager document (SSM document). Use Systems Manager State Manager to create an association between the SSM document and the EC2 instances.

> **Suggested Answer:** BE
> **Community Vote:** BE (71%), BD (25%), 4%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**vortegon** (Sat 10 Aug 2024 19:23) - *Upvotes: 9*
B and E are the most effective in ensuring that updates to the application are installed on the running EC2 instances by leveraging CloudFormation's and AWS Systems Manager's capabilities for managing and applying updates.

---

**Srikantha** (Sun 30 Mar 2025 13:32) - *Upvotes: 1*
The best combination of steps to ensure the application is updated on the running EC2 instances during a CloudFormation stack update is:

B. Refactor the user data commands to use the cfn-init helper script. Update the user data to install and configure the cfn-hup and cfn-init helper scripts to monitor and apply the metadata changes.
D. Refactor the user data commands to use an AWS Systems Manager document (SSM document). Add an AWS CLI command in the user data to use Systems Manager Run Command to apply the SSM document to the EC2 instances.

---

**dkp** (Mon 14 Oct 2024 04:20) - *Upvotes: 3*
B&E
D. Systems Manager Run Command (with user data): Using Run Command within user data to apply an SSM document introduces an unnecessary step. Option E with State Manager automates the process.

---

**WhyIronMan** (Mon 30 Sep 2024 01:17) - *Upvotes: 2*
B, E.
"Association" is the key. Details are everything during an Investigation...

---

**Seoyong** (Wed 25 Sep 2024 06:22) - *Upvotes: 1*
User data is executed when the system starts, not executed in runing EC2.

---

**vmahilevskyi** (Tue 24 Sep 2024 10:45) - *Upvotes: 3*
EC2 instance profile with AmazonSSMManagedinstanceCore policy doesn't have permissions to SSM Run Command, so D is incorrect.
So for me it's BE.

---

**ogerber** (Tue 17 Sep 2024 18:55) - *Upvotes: 2*
"Add an AWS CLI command in the user data to use Systems Manager Run Command to apply the SSM document to the EC2 instances."
**Add an AWS CLI command in the ****user data*****
it will not help to update running instances, tricky question.
Only Systems Manager State Manager will update the running instances in this question

---

**fdoxxx** (Thu 29 Aug 2024 19:41) - *Upvotes: 1*
B and D
A. This option is not applicable for updating applications on EC2 instances.

B. Refactoring the user data commands to use the cfn-init helper script helps in handling metadata changes and applying them to the EC2 instances. This is especially useful in CloudFormation stack updates.

C. Creating a new EC2 Auto Scaling group with an update policy doesn't necessarily address the application update requirement in this scenario.

D. Refactoring the user data commands to use an AWS Systems Manager document and using Run Command to apply the SSM document is a valid approach for updating applications on EC2 instances.

E. While using Systems Manager documents and State Manager is a valid approach, it might be more complex than needed for a straightforward update of an application on EC2 instances.

Therefore, options B and D together provide a good solution for updating the application on the running EC2 instances.

---

**Ramdi1** (Tue 13 Aug 2024 16:59) - *Upvotes: 1*
ption B: cfn-init is a powerful tool for managing configuration on EC2 instances. By using cfn-init, the DevOps engineer can ensure that the new application version is installed regardless of the current state of the instances Option D: SSM documents provide a centralized and reusable way to manage configurations. By using Run Command, the engineer can trigger the application update on all instances directly from the template.

---

**Ramdi1** (Tue 13 Aug 2024 17:00) - *Upvotes: 1*
Options A, C, and E are not suitable for this scenario:
 
Option A: MIME multipart format isn't necessary for this scenario.
Option C: While Auto Scaling offers flexibility, creating a new launch template and Auto Scaling group is unnecessary for a simple application update.
Option E: State Manager is generally used for ongoing configuration management, not one-time deployments like this.
 
By using both cfn-init and SSM documents, the DevOps engineer can achieve a reliable and manageable way to update the application on the running EC2 instances.

---


<br/>

## Question 197

*Date: Feb. 7, 2024, 12:52 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is refactoring applications to use AWS. The company identifies an internal web application that needs to make Amazon S3 API calls in a specific AWS account.

The company wants to use its existing identity provider (IdP) auth.company.com for authentication. The IdP supports only OpenID Connect (OIDC). A DevOps engineer needs to secure the web application's access to the AWS account.

Which combination of steps will meet these requirements? (Choose three.)

**Options:**
- A. Configure AWS IAM Identity Center (AWS Single Sign-On). Configure an IdP. Upload the IdP metadata from the existing IdP.
- B. Create an IAM IdP by using the provider URL, audience, and signature from the existing IP.
- C. Create an IAM role that has a policy that allows the necessary S3 actions. Configure the role's trust policy to allow the OIDC IP to assume the role if the sts.amazon.com:aud context key is appid_from_idp.
- D. Create an IAM role that has a policy that allows the necessary S3 actions. Configure the role's trust policy to allow the OIDC IP to assume the role if the auth.company.com:aud context key is appid_from_idp.
- E. Configure the web application to use the AssumeRoleWithWebIdentity API operation to retrieve temporary credentials. Use the temporary credentials to make the S3 API calls.
- F. Configure the web application to use the GetFederationToken API operation to retrieve temporary credentials. Use the temporary credentials to make the S3 API calls.

> **Suggested Answer:** BDE
> **Community Vote:** BDE (75%), 13%, 6%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**vortegon** (Sat 10 Feb 2024 20:33) - *Upvotes: 5*
https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-idp_oidc.html

---

**sn61613** (Tue 24 Dec 2024 23:16) - *Upvotes: 1*
BCE
https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-idp_oidc.html

---

**Gomer** (Fri 05 Jul 2024 21:12) - *Upvotes: 2*
"Use OpenID Connect (OIDC) federated identity providers instead of creating" IAM users." "With an" IdP "you can manage" "user identities outside of AWS and give these external user identities permissions to access AWS resources in your account."
B: (YES) "IAM OIDC identity Providers" "This is useful when creating a mobile app or web application that requires access to AWS resources, but you don't want to create custom sign-in code or manage your own user identities."
D: (YES) "For OIDC providers, use the fully qualified URL of the OIDC IdP with the aud context key" e.g.: "Condition": {"StringEquals": {"server.example.com:aud": "appid_from_oidc_idp"}}"
E: (YES) "AssumeRoleWithWebIdentity" "Federation through a web-based" IDP "returns a set of temporary security credentials for federated users" "authenticated" "with a public identity provider." "This operation is useful for" "client-based web applications that require access to AWS."

---

**seetpt** (Thu 02 May 2024 14:21) - *Upvotes: 1*
BDE for me

---

**dkp** (Sun 14 Apr 2024 04:40) - *Upvotes: 1*
DE is correct not sure between A & B
A. Configure AWS IAM Identity Center (AWS Single Sign-On). Configure an IdP. Upload the IdP metadata from the existing IdP.
Pros: Integrates with AWS SSO and allows for IdP metadata upload.
Cons: AWS SSO is generally used for managing multiple AWS accounts and SSO for multiple AWS services, might be overkill for a single account and application.
B. Create an IAM IdP by using the provider URL, audience, and signature from the existing IP.
Pros: Creates a custom IAM IdP using the existing IdP's details.
Cons: Manual configuration of IAM IdP might be error-prone and not the best practice for OIDC integration.

---

**thanhnv142** (Mon 12 Feb 2024 15:48) - *Upvotes: 4*
BDE:
A: we need to create an IDP. We dont need a AWS Single Sign-On
B: correct
C: we need to authen. sts.amazon.com:aud does not for authen
D: auth.company.com:aud is for authen
E: This used for authen AssumeRoleWithWebIdentity
F: This is not used for authen

---

**Ramdi1** (Mon 12 Feb 2024 13:11) - *Upvotes: 1*
C & D: Creating an IAM role with specific S3 permissions and configuring the trust policy based on the appropriate audience (sts.amazon.com:aud or auth.company.com:aud) allows secure role assumption by the OIDC IdP on behalf of authenticated users.
E: Using AssumeRoleWithWebIdentity fetches temporary credentials with restricted privileges, enhancing security compared to long-lived credentials.

---

**Ramdi1** (Mon 12 Feb 2024 13:11) - *Upvotes: 1*
Options A, B, and F are not suitable for this scenario:
A: AWS SSO is currently not available for public AWS accounts and wouldn't address the specific OIDC integration requirement.
B: While creating an IAM IdP is possible, it's generally less secure than leveraging the existing, trusted IdP with OIDC support.
F: GetFederationToken is often used with SAML-based federation and wouldn't work directly with OIDC.

---

**Chelseajcole** (Wed 07 Feb 2024 17:03) - *Upvotes: 3*
BDE is my answer

---

**Arnaud92** (Wed 07 Feb 2024 12:52) - *Upvotes: 1*
https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-idp_oidc.html

---


<br/>

## Question 198

*Date: Feb. 5, 2024, 5:48 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses Amazon RDS for all databases in its AWS accounts. The company uses AWS Control Tower to build a landing zone that has an audit and logging account. All databases must be encrypted at rest for compliance reasons. The company's security engineer needs to receive notification about any noncompliant databases that are in the company’s accounts.

Which solution will meet these requirements with the MOST operational efficiency?

**Options:**
- A. Use AWS Control Tower to activate the optional detective control (guardrail) to determine whether the RDS storage is encrypted. Create an Amazon Simple Notification Service (Amazon SNS) topic in the company's audit account. Create an Amazon EventBridge rule to filter noncompliant events from the AWS Control Tower control (guardrail) to notify the SNS topic. Subscribe the security engineer's email address to the SNS topic.
- B. Use AWS CloudFormation StackSets to deploy AWS Lambda functions to every account. Write the Lambda function code to determine whether the RDS storage is encrypted in the account the function is deployed to. Send the findings as an Amazon CloudWatch metric to the management account. Create an Amazon Simple Notification Service (Amazon SNS) topic. Create a CloudWatch alarm that notifies the SNS topic when metric thresholds are met. Subscribe the security engineer's email address to the SNS topic.
- C. Create a custom AWS Config rule in every account to determine whether the RDS storage is encrypted. Create an Amazon Simple Notification Service (Amazon SNS) topic in the audit account. Create an Amazon EventBidge rule to filter noncompliant events from the AWS Control Tower control (guardrail) to notify the SNS topic. Subscribe the security engineer's email address to the SNS topic.
- D. Launch an Amazon C2 instance. Run an hourly cron job by using the AWS CLI to determine whether the RDS storage is encrypted in each AWS account. Store the results in an RDS database. Notify the security engineer by sending email messages from the EC2 instance when noncompliance is detected

> **Suggested Answer:** A
> **Community Vote:** A (79%), C (21%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Tue 30 Jul 2024 10:22) - *Upvotes: 2*
Keywords: Control Tower
A company uses Amazon RDS for all databases in its AWS accounts. The company uses AWS Control Tower

---

**didek1986** (Sat 20 Apr 2024 18:22) - *Upvotes: 3*
A
https://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#disallow-rds-storage-unencrypted

---

**dkp** (Sun 14 Apr 2024 04:47) - *Upvotes: 3*
most efficient way is A

---

**DanShone** (Sat 16 Mar 2024 10:46) - *Upvotes: 3*
A - least operational overhead

---

**sejar** (Sat 09 Mar 2024 12:46) - *Upvotes: 3*
Guardrail uses AWS Config for compliance detection

---

**Diego1414** (Sun 25 Feb 2024 22:14) - *Upvotes: 2*
Answer: C - https://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#disallow-rds-storage-unencrypted

---

**thanhnv142** (Mon 12 Feb 2024 15:52) - *Upvotes: 4*
A is correct: we need guardraild to detect non-compliances
B and D: no mention of guardrail.
C: Though this option mentions guardrail, it uses AWS Config to detect non-compliances

---

**thanhnv142** (Tue 13 Feb 2024 16:00) - *Upvotes: 1*
correction: C

---

**Ramdi1** (Mon 12 Feb 2024 13:10) - *Upvotes: 4*
Leverages existing infrastructure: It utilizes native AWS Control Tower functionality for compliance checks and integrates seamlessly with SNS for notifications.
Centralized management: Configuration and monitoring are done in the audit account, eliminating the need for individual resources in each account.
Scalability: Handles future account growth without manual intervention.

---

**vortegon** (Sat 10 Feb 2024 20:41) - *Upvotes: 4*
https://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#disallow-rds-storage-unencrypted

---


<br/>

## Question 199

*Date: Feb. 7, 2024, 1:18 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is migrating from its on-premises data center to AWS. The company currently uses a custom on-premises Cl/CD pipeline solution to build and package software.

The company wants its software packages and dependent public repositories to be available in AWS CodeArtifact to facilitate the creation of application-specific pipelines.

Which combination of steps should the company take to update the CI/CD pipeline solution and to configure CodeArtifact with the LEAST operational overhead? (Choose two.)

**Options:**
- A. Update the C1ICD pipeline to create a VM image that contains newly packaged software. Use AWS Import/Export to make the VM image available as an Amazon EC2 AMI. Launch the AMI with an attached IAM instance profile that allows CodeArtifact actions. Use AWS CLI commands to publish the packages to a CodeArtifact repository.
- B. Create an AWS Identity and Access Management Roles Anywhere trust anchor. Create an IAM role that allows CodeArtifact actions and that has a trust relationship on the trust anchor. Update the on-premises CI/CD pipeline to assume the new IAM role and to publish the packages to CodeArtifact.
- C. Create a new Amazon S3 bucket. Generate a presigned URL that allows the PutObject request. Update the on-premises CI/CD pipeline to use the presigned URL to publish the packages from the on-premises location to the S3 bucket. Create an AWS Lambda function that runs when packages are created in the bucket through a put command. Configure the Lambda function to publish the packages to CodeArtifact.
- D. For each public repository, create a CodeArutact repository that is configured with an external connection. Configure the dependent repositories as upstream public repositories.
- E. Create a Codeartitact repository that is configured with a set of external connections to the public repositories. Configure the external connections to be downstream of the repository.

> **Suggested Answer:** BD
> **Community Vote:** BD (95%), 5%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Mon 12 Aug 2024 15:00) - *Upvotes: 6*
B and D are correct: <wants its software packages and dependent public repositories to be available in AWS CodeArtifact >: we need to push onprem artifact to CodeArtifact with IAM Anywhere Role and create an upstream for public repositories
A: irrelevant
B: correct
C: irrelevant
D: correct
E: there is no downstream in CodeArtifact

---

**GripZA** (Mon 21 Apr 2025 12:11) - *Upvotes: 2*
B: IAM Roles Anywhere lets workloads outside AWS (on‑prem servers, VMs, containers) exchange X.509 certificates for short‑lived AWS credentials (just like an EC2 role)

D: a CodeArtifact external connection is a managed link to a public upstream, you only configure one external connection per repository—CodeArtifact handles mirroring and caching. you then create a second, “downstream” CodeArtifact repo (your private repo) and add the external‑connected repos as an upstream

---

**youonebe** (Tue 24 Dec 2024 15:05) - *Upvotes: 1*
AWS IAM Roles Anywhere is a feature that allows workloads running outside of AWS, such as on-premises servers, containers, and applications, to access AWS resources using temporary security credentials obtained by assuming an IAM role.

---

**dkp** (Mon 14 Oct 2024 04:54) - *Upvotes: 2*
ANS B&D

---

**DanShone** (Mon 16 Sep 2024 09:45) - *Upvotes: 4*
B & D
B - https://docs.aws.amazon.com/rolesanywhere/latest/userguide/getting-started.html
D - Best practice for external connections is to have one repository per domain with an external connection to a given public repository.

---

**Ramdi1** (Mon 12 Aug 2024 12:03) - *Upvotes: 3*
B & D

The other options have drawbacks:
A:
Complex setup: Requires VM image creation, import, and AMI launching, adding unnecessary complexity.
Security concerns: Using EC2 instances might introduce security risks compared to IAM roles.
Inefficient publishing: Relies on manual CLI commands for publishing, less automated than other options.

---

**Ramdi1** (Mon 12 Aug 2024 12:03) - *Upvotes: 1*
B:
Minimal infrastructure: Only requires an IAM role and trust anchor setup in AWS, without creating additional resources like VMs or S3 buckets.
Secure access: Leverages IAM for secure communication between the on-premises pipeline and CodeArtifact.
Direct publishing: Enables direct package publishing from the pipeline to CodeArtifact.
 
D:
Centralized management: Manages public repositories through a single CodeArtifact repository with external connections.
Automatic updates: Upstream repository changes are automatically reflected in CodeArtifact.
Reduced bandwidth: Packages stored in public repositories, minimizing data transfer to AWS.

---

**vortegon** (Sat 10 Aug 2024 19:49) - *Upvotes: 2*
https://www.pulumi.com/ai/answers/bddaepm6EeuDs9du1MVtC8/aws-codeartifact-and-iam-roles-setup

---

**Arnaud92** (Wed 07 Aug 2024 12:18) - *Upvotes: 1*
D. In CodeArtifact, the intended way to use external connections is to have one repository per domain with an external connection to a given public repository.

A. Using aws codeartifact with rolesanywhere is the LEAST operational overhead => https://www.pulumi.com/ai/answers/bddaepm6EeuDs9du1MVtC8/aws-codeartifact-and-iam-roles-setup

---

**Arnaud92** (Wed 07 Aug 2024 12:20) - *Upvotes: 1*
I meant B :-) (not A)

---


<br/>

## Question 200

*Date: Feb. 7, 2024, 7:03 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps team uses AWS CodePipeline, AWS CodeBuild, and AWS CodeDeploy to deploy an application. The application is a REST API that uses AWS Lambda functions and Amazon API Gateway. Recent deployments have introduced errors that have affected many customers.

The DevOps team needs a solution that reverts to the most recent stable version of the application when an error is detected. The solution must affect the fewest customers possible.

Which solution will meet these requirements with the MOST operational efficiency?

**Options:**
- A. Set the deployment configuration in CodeDeploy to LambdaAllAtOnce. Configure automatic rollbacks on the deployment group. Create an Amazon CloudWatch alarm that detects HTTP Bad Gateway errors on API Gateway. Configure the deployment group to roll back when the number of alarms meets the alarm threshold.
- B. Set the deployment configuration in CodeDeploy to LambdaCanary10Percent10Minutes. Configure automatic rollbacks on the deployment group. Create an Amazon CloudWatch alarm that detects HTTP Bad Gateway errors on API Gateway. Configure the deployment group to roll back when the number of alarms meets the alarm threshold.
- C. Set the deployment configuration in CodeDeploy to LambdaAllAtOnce. Configure manual rollbacks on the deployment group. Create an Amazon Simple Notification Service (Amazon SNS) topic to send notifications every time a deployment fails. Configure the SNS topic to invoke a new Lambda function that stops the current deployment and starts the most recent successful deployment.
- D. Set the deployment configuration in CodeDeploy to LambdaCanary10Percent10Minutes. Configure manual rollbacks on the deployment group. Create a metric filter on an Amazon CloudWatch log group for API Gateway to monitor HTTP Bad Gateway errors. Configure the metric filter to invoke a new Lambda function that stops the current deployment and starts the most recent successful deployment.

> **Suggested Answer:** B
> **Community Vote:** B (94%), 6%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Mon 12 Aug 2024 15:06) - *Upvotes: 8*
B is correct:
A and C: <CodeDeploy to LambdaAllAtOnce>: Replacing all at once would not allow roll back
D: Metric filter cannot trigger lambda. Additionally, this options is a manual work

---

**c3518fc** (Sat 26 Oct 2024 07:03) - *Upvotes: 3*
Option B provides the most operationally efficient solution by combining canary deployments, automatic rollbacks, and CloudWatch alarms to detect and respond to issues quickly while minimizing customer impact.

---

**dkp** (Mon 14 Oct 2024 04:56) - *Upvotes: 3*
Answer B

---

**ogerber** (Thu 19 Sep 2024 16:44) - *Upvotes: 3*
Its B, 100%

---

**Ramdi1** (Tue 13 Aug 2024 17:32) - *Upvotes: 1*
option C - LambdaAllAtOnce: Rolling back everything at once minimizes the window for potential customer impact compared to canary deployments.
Manual rollbacks: While automatic rollbacks may seem faster, they can be triggered by false positives, leading to unnecessary rollbacks and service disruptions. Manual rollbacks offer more control and allow the team to assess the situation before reverting.
SNS notifications: Alerts about failing deployments are crucial for quick response.
Lambda function for rollback: Automating the rollback process with a Lambda function triggered by SNS notification streamlines the operation and reduces manual intervention.
Starts the most recent successful deployment: This ensures reverting to a known-good state without manual selection, saving time and avoiding errors.

---

**Ramdi1** (Tue 13 Aug 2024 17:32) - *Upvotes: 1*
Here's how the other options fall short:
 
Option A: While LambdaAllAtOnce is good, automatic rollbacks based on alarms might be triggered by transient issues, leading to unnecessary disruptions.
Option B: Canary deployments are beneficial for testing, but rolling back 10% at a time might not be efficient if the error affects all users. Also, relying on automatic rollbacks has the same drawbacks as mentioned in A.
Option D: While manual control is good, relying solely on CloudWatch logs and a separate Lambda function for stopping and starting deployments adds complexity and requires more manual intervention compared to the streamlined SNS-triggered rollback in option C.

---

**c3518fc** (Sat 26 Oct 2024 07:04) - *Upvotes: 1*
don't say rubbish with confidence. B is the answer

---

**Chelseajcole** (Wed 07 Aug 2024 18:03) - *Upvotes: 1*
D. Rolling deployment with the option to stop once it detects any errors.

---


<br/>

## Question 201

*Date: Feb. 7, 2024, 7:10 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company recently deployed its web application on AWS. The company is preparing for a large-scale sales event and must ensure that the web application can scale to meet the demand.

The application's frontend infrastructure includes an Amazon CloudFront distribution that has an Amazon S3 bucket as an origin. The backend infrastructure includes an Amazon API Gateway API, several AWS Lambda functions, and an Amazon Aurora DB cluster.

The company's DevOps engineer conducts a load test and identifies that the Lambda functions can fulfil the peak number of requests. However, the DevOps engineer notices request latency during the initial burst of requests. Most of the requests to the Lambda functions produce queries to the database. A large portion of the invocation time is used to establish database connections.

Which combination of steps will provide the application with the required scalability? (Choose three.)

**Options:**
- A. Configure a higher reserved concurrency for the Lambda functions.
- B. Configure a higher provisioned concurrency for the Lambda functions.
- C. Convert the DB cluster to an Aurora global database. Add additional Aurora Replicas in AWS Regions based on the locations of the company's customers.
- D. Refactor the Lambda functions. Move the code blocks that initialize database connections into the function handlers.
- F. Use Amazon RDS Proxy to create a proxy for the Aurora database. Update the Lambda functions to use the proxy endpoints for database connections.

> **Suggested Answer:** BCF
> **Community Vote:** BCF (52%), BDF (33%), Other, Other, A (35%), C (25%), B (20%), Other

### Discussions

**WhyIronMan** (Sat 30 Mar 2024 22:49) - *Upvotes: 15*
A. this doesn't directly address the database connection issue and there will be moments were you will be not using it, so spending money
B. correct, Configure a higher provisioned concurrency for the Lambda functions: This ensures that Lambda instances are ready to handle bursts of traffic, reducing cold start latency.
C. Is correct, if they want to read only
D. is wrong because it says "... into the function handlers..." while best practices say to do it OUTSIDE the function handlers. Starting NEW CONNECTIONS is bad thing.
F. Is correct, it is a best practice

---

**WhyIronMan** (Sat 30 Mar 2024 22:50) - *Upvotes: 5*
Also, please notice that "The company's DevOps engineer conducts a load test and identifies that the Lambda functions can fulfil the peak number of requests."

---

**Jay_2pt0_1** (Sun 12 May 2024 20:46) - *Upvotes: 2*
Glad I read this. I read D wrong the first time.

---

**DanShone** (Sat 16 Mar 2024 10:36) - *Upvotes: 8*
A - Provisioned concurrency – This is the number of pre-initialized execution environments allocated to your function
https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html

D - Initialize SDK clients and database connections outside of the function handler
https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html

F - RDS Proxy improves scalability by pooling and sharing database connections
https://aws.amazon.com/rds/proxy/faqs/?nc=sn&loc=4

---

**ryuhei** (Sun 24 Aug 2025 12:45) - *Upvotes: 1*
Where is option E?

---

**Waak** (Sun 02 Feb 2025 13:01) - *Upvotes: 1*
There should be another option E) Refactor the Lambda functions. Move the code blocks that initialize database connections outside the function handlers.

---

**teo2157** (Tue 10 Dec 2024 11:02) - *Upvotes: 1*
DanShone provided the right links to the answers

---

**seetpt** (Sat 24 Aug 2024 13:59) - *Upvotes: 1*
I think BCF

---

**radhi2024** (Mon 19 Aug 2024 15:24) - *Upvotes: 3*
B C D i correct

---

**trungtd** (Thu 11 Jul 2024 01:38) - *Upvotes: 4*
The person who chose D doesn't understand Lambda at all

---


<br/>

## Question 202

*Date: Feb. 7, 2024, 7:15 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company runs a web application that extends across multiple Availability Zones. The company uses an Application Load Balancer (ALB) for routing, AWS Fargate for the application, and Amazon Aurora for the application data. The company uses AWS CloudFormation templates to deploy the application. The company stores all Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository in the same AWS account and AWS Region.

A DevOps engineer needs to establish a disaster recovery (DR) process in another Region. The solution must meet an RPO of 8 hours and an RTO of 2 hours. The company sometimes needs more than 2 hours to build the Docker images from the Dockerfile.

Which solution will meet the RTO and RPO requirements MOST cost-effectively?

**Options:**
- A. Copy the CloudFormation templates and the Dockerfile to an Amazon S3 bucket in the DR Region. Use AWS Backup to configure automated Aurora cross-Region hourly snapshots. In case of DR, build the most recent Docker image and upload the Docker image to an ECR repository in the DR Region. Use the CloudFormation template that has the most recent Aurora snapshot and the Docker image from the ECR repository to launch a new CloudFormation stack in the DR Region. Update the application DNS records to point to the new ALB.
- B. Copy the CloudFormation templates to an Amazon S3 bucket in the DR Region. Configure Aurora automated backup Cross-Region Replication. Configure ECR Cross-Region Replication. In case of DR, use the CloudFormation template with the most recent Aurora snapshot and the Docker image from the local ECR repository to launch a new CloudFormation stack in the DR Region. Update the application DNS records to point to the new ALB.
- C. Copy the CloudFormation templates to an Amazon S3 bucket in the DR Region. Use Amazon EventBridge to schedule an AWS Lambda function to take an hourly snapshot of the Aurora database and of the most recent Docker image in the ECR repository. Copy the snapshot and the Docker image to the DR Region. In case of DR, use the CloudFormation template with the most recent Aurora snapshot and the Docker image from the local ECR repository to launch a new CloudFormation stack in the DR Region.
- D. Copy the CloudFormation templates to an Amazon S3 bucket in the DR Region. Deploy a second application CloudFormation stack in the DR Region. Reconfigure Aurora to be a global database. Update both CloudFormation stacks when a new application release in the current Region is needed. In case of DR, update the application DNS records to point to the new ALB.

> **Suggested Answer:** B
> **Community Vote:** B (95%), 5%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Mon 12 Feb 2024 16:16) - *Upvotes: 6*
B is correct:
A: < build the most recent Docker image and upload the Docker image to an ECR repository in the DR Region>: This process might take longer than 2 hours, which does not meet the RTO
B: correct
C: <AWS Lambda function to take an hourly snapshot of the Aurora database>: lambda has a maximum running time of 15 minutes. Cannot do an hourly task
D: No mention about the docker image for the ECS

---

**Srikantha** (Sun 30 Mar 2025 14:50) - *Upvotes: 1*
The best option in this case is Option B because it:

Ensures both Aurora database and Docker images are replicated to the DR Region, meeting the RPO requirement of 8 hours.
Eliminates the need to rebuild Docker images in the DR Region, thus ensuring the RTO of 2 hours is met, which is crucial during the disaster recovery process.
Provides a straightforward, cost-effective solution with minimal operational overhead by leveraging AWS native features like cross-Region replication for both Aurora and ECR.

---

**spring21** (Wed 11 Dec 2024 18:40) - *Upvotes: 2*
Yes, Amazon Aurora supports cross-Region automated backups:

---

**teo2157** (Tue 10 Dec 2024 14:05) - *Upvotes: 1*
B is not correct as Amazon Aurora doesn´t support cross-Region automated backups: https://repost.aws/questions/QUPm9L8amQTTKkz1RXU7EqWA/unable-to-enable-cross-region-automated-backups-in-rds
Have anyone that has responded B tried to enable cross-Region automated backups in an Aurora RDS?

---

**dkp** (Sat 13 Apr 2024 02:45) - *Upvotes: 3*
answer is B

---

**DanShone** (Sat 16 Mar 2024 10:31) - *Upvotes: 4*
B: Least operational overhead, lower cost and meets RPO and RTO

---

**fdoxxx** (Thu 29 Feb 2024 21:23) - *Upvotes: 4*
B provides a cost-effective solution that meets the RTO and RPO

---

**LeoSantos121212121212121** (Fri 09 Feb 2024 17:51) - *Upvotes: 2*
I also go with B

---

**Chelseajcole** (Wed 07 Feb 2024 19:15) - *Upvotes: 2*
B. Most cost effective

---


<br/>

## Question 203

*Date: Feb. 7, 2024, 7:25 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company's application runs on Amazon EC2 instances. The application writes to a log file that records the username, date, time, and source IP address of the login. The log is published to a log group in Amazon CloudWatch Logs.

The company is performing a root cause analysis for an event that occurred on the previous day. The company needs to know the number of logins for a specific user from the past 7 days.

Which solution will provide this information?

**Options:**
- A. Create a CloudWatch Logs metric filter on the log group. Use a filter pattern that matches the username. Publish a CloudWatch metric that sums the number of logins over the past 7 days.
- B. Create a CloudWatch Logs subscription on the log group. Use a filter pattern that matches the username. Publish a CloudWatch metric that sums the number of logins over the past 7 days.
- C. Create a CloudWatch Logs Insights query that uses an aggregation function to count the number of logins for the username over the past 7 days. Run the query against the log group.
- D. Create a CloudWatch dashboard. Add a number widget that has a filter pattern that counts the number of logins for the username over the past 7 days directly from the log group.

> **Suggested Answer:** C
> **Community Vote:** C (96%), 4%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**fdoxxx** (Thu 29 Feb 2024 21:36) - *Upvotes: 5*
C is the most suitable solution for obtaining the required information

---

**kyuhuck** (Sat 17 Feb 2024 18:04) - *Upvotes: 5*
, C. Create a CloudWatch Logs Insights query that uses an aggregation function to count the number of logins for the username over the past 7 days. Run the query against the log group. is the best solution. This method directly addresses the need to analyze log data for a specific pattern (user logins) and aggregate the counts over a specified period, which is exactly what's needed for a root cause analysis of the event.

---

**heff_bezos** (Thu 26 Sep 2024 06:46) - *Upvotes: 2*
There is no cloudwatch metric that logs user logins

---

**itzrahulyadav** (Sat 10 Aug 2024 15:58) - *Upvotes: 1*
D
For A you would have to setup the cloudwatch metric filters beforehand as you won't be able to analyze past logs without the setup

---

**dkp** (Sat 13 Apr 2024 03:04) - *Upvotes: 5*
answer C

---

**mumumu** (Sun 07 Apr 2024 09:01) - *Upvotes: 4*
The solution that will provide the number of logins for a specific user from the past 7 days is Option C: Create a CloudWatch Logs Insights query that uses an aggregation function to count the number of logins for the username over the past 7 days. Run the query against the log group.

---

**Shasha1** (Wed 13 Mar 2024 09:54) - *Upvotes: 2*
C
https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_AnalyzeLogData_AggregationQuery.html

---

**Ramdi1** (Tue 13 Feb 2024 18:50) - *Upvotes: 5*
C - CloudWatch Logs Insights: This service allows you to run ad-hoc queries against your log data without creating additional infrastructure like metrics or subscriptions.
Aggregation function: Functions like count() can specifically calculate the number of occurrences based on specific criteria.
Filtering by username and timeframe: The query can be tailored to include the specific username and filter for entries within the past 7 days.

---

**Ramdi1** (Tue 13 Feb 2024 18:50) - *Upvotes: 1*
A. Metric filter: It can count occurrences, but requires additional metric creation and subscription, introducing complexity.
B. Subscription: Similar to metric filter, requires creating an additional subscription and pushing data elsewhere.
D. Dashboard widget: Limited in its capabilities, might not allow complex filtering and aggregation needed for this specific analysis.
 
Therefore, CloudWatch Logs Insights offers the most direct and flexible solution for analyzing the desired login data within the given timeframe and user criteria.

---

**thanhnv142** (Mon 12 Feb 2024 16:22) - *Upvotes: 1*
A is correct: CloudWatch Logs metric filter can filter out relevant logs and count
B: irrelevant, this is for sharing logs to other sources
C: This can accomplish the task. However, loudWatch Logs metric filter offers us the same function with less cost. Using CloudWatch Logs Insights query incur more costs. This feature is primarily used for data analysis
D: irrelevant

---


<br/>

## Question 204

*Date: Feb. 7, 2024, 7:28 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an AWS CodeDeploy application. The application has a deployment group that uses a single tag group to identify instances for the deployment of Application. The single tag group configuration identifies instances that have Environment=Production and Name=ApplicationA tags for the deployment of ApplicationA.

The company launches an additional Amazon EC2 instance with Department=Marketing, Environment=Production, and Name=ApplicationB tags. On the next CodeDeploy deployment of Application, the additional instance has ApplicationA installed on it. A DevOps engineer needs to configure the existing deployment group to prevent ApplicationA from being installed on the additional instance.

Which solution will meet these requirements?

**Options:**
- A. Change the current single tag group to include only the Environment=Production tag. Add another single tag group that includes only the Name=ApplicationA tag.
- B. Change the current single tag group to include the Department=Marketing, Environment=production, and Name=ApplicationA tags.
- C. Add another single tag group that includes only the Department=Marketing tag. Keep the Environment=Production and Name=ApplicationA tags with the current single tag group.
- D. Change the current single tag group to include only the Environment=Production tag. Add another single tag group that includes only the Department=Marketing tag.

> **Suggested Answer:** A
> **Community Vote:** A (93%), 7%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**thanhnv142** (Mon 12 Feb 2024 16:32) - *Upvotes: 9*
A is correct: All tags in one tag group are OR operator. Tags are in multiple tag groups are AND operator

---

**Gomer** (Mon 08 Jul 2024 21:24) - *Upvotes: 6*
I Found this question largely incomprehensible untill I understood the following concepts and mapped out everything
- Single tag group: Any instance identified by at least one tag in the group is included in the deployment group. (OR Operator within individual tag groups)
- Multiple tag groups: Instances that are identified by at least one tag in each of the tag groups are included. (AND Operator between multiple tag groups)
https://docs.aws.amazon.com/codedeploy/latest/userguide/instances-tagging.html

---

**Gomer** (Mon 08 Jul 2024 21:24) - *Upvotes: 2*
A:(YES) Block ApplicationA deployments on EC2B because it would now have to have both Production AND ApplicationA tags (AND Operator between tag groups)
B: (NO) Allow ApplicationA deployments on EC2B because it has the Production tag (OR Operator within tag group)
C: (NO) Allow ApplicationA deployments on EC2B because it has the Production and Marketing tags (OR Operator within tag group, AND Operator between tag groups)
D: (NO) Allow ApplicationA deployments on EC2B because it has the Production and Marketing tags (OR Operator within tag group, AND Operator between tag groups)

---

**GripZA** (Mon 21 Apr 2025 13:30) - *Upvotes: 1*
when you understand how multiple tag groups deploy, it makes it easier to map out. the golden rule is that an instance must match AT LEAST ONE of the tags in each of the tag groups. so when adding another single tag group with Name=ApplicationA tag, the new instance needs to have Environment=Production AND Name=ApplicationA tag to be included in the existing deployment group

---

**Srikantha** (Sun 30 Mar 2025 14:55) - *Upvotes: 1*
Option B is the most effective solution, as it specifies that the deployment group should only target instances that have all three tags (Department=Marketing, Environment=Production, and Name=ApplicationA). This ensures that only instances specifically meant for ApplicationA are selected for the deployment, preventing unwanted instances (like those with Name=ApplicationB) from receiving the application.

---

**youonebe** (Tue 24 Dec 2024 16:42) - *Upvotes: 2*
This is a very tricky question, even ChatGPT can't get it right, a good one!

Make sure to go through this AWS doc first and fully understand it.
https://docs.aws.amazon.com/codedeploy/latest/userguide/instances-tagging.html

First thing first, why ApplicationA was installed on the new EC2 instance?
Reason is <single tag group, multiple tags> was used. The <Environment> and <Name> tags are both defined within this tag group. So as long as the EC2 instance has one tag matched, ApplicationA would be deployed. In this case, the new EC2 instance has <Environment=Production> tag, so ApplicationA got deployed.

---

**youonebe** (Tue 24 Dec 2024 16:42) - *Upvotes: 1*
Now we look at the choices one by one:
A. [Correct]This updates the current tag group, add a new tag group. Which means BOTH tags need to match before ApplicationA gets deployed. For the new EC2 instance, because Name=ApplicationB, hence preventing the mistake from happening.

B. [Wrong] This will be very similar to the original configuration, as long as one of the 3 tags matches, ApplicationA will be deployed. This does not solve the problem.

C. [Wrong] The company still needs to install ApplicationA to existing EC2 instances, adding"Department=Marketing will break the current workflow.

D. [Wrong] The company still needs to install ApplicationA to existing EC2 instances, adding Department=Marketing will break the current workflow.

---

**DanShone** (Sat 16 Mar 2024 10:26) - *Upvotes: 5*
A: A single tag group can only contain 1 tag, so multiple Single tag groups will be needed.
https://docs.aws.amazon.com/codedeploy/latest/userguide/instances-tagging.html

---

**fdoxxx** (Thu 29 Feb 2024 21:29) - *Upvotes: 1*
Option B is the most appropriate solution for meeting the requirements:

Changing the current single tag group to include the specific tags (Department=Marketing, Environment=Production, and Name=ApplicationA) ensures that only instances with these specific tags are identified for the deployment of ApplicationA.
The other options are not suitable for achieving the desired outcome

---

**Ramdi1** (Tue 13 Feb 2024 18:56) - *Upvotes: 5*
A - Original configuration: The single tag group with Environment=Production and Name=ApplicationA tags targets any instance with both tags, resulting in ApplicationA being deployed on the new Marketing instance despite its different Name tag.
Solution A:
Changing the current tag group to "Environment=Production" ensures only instances in the Production environment are considered.
Adding a separate tag group with "Name=ApplicationA" specifically targets instances meant for that application.
The Marketing instance with Department=Marketing tag doesn't match the new criteria and avoids unintended ApplicationA installation.

---


<br/>

## Question 205

*Date: Feb. 7, 2024, 7:31 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is launching an application that stores raw data in an Amazon S3 bucket. Three applications need to access the data to generate reports. The data must be redacted differently for each application before the applications can access the data.

Which solution will meet these requirements?

**Options:**
- A. Create an S3 bucket for each application. Configure S3 Same-Region Replication (SRR) from the raw data's S3 bucket to each application's S3 bucket. Configure each application to consume data from its own S3 bucket.
- B. Create an Amazon Kinesis data stream. Create an AWS Lambda function that is invoked by object creation events in the raw data’s S3 bucket. Program the Lambda function to redact data for each application. Publish the data on the Kinesis data stream. Configure each application to consume data from the Kinesis data stream.
- C. For each application, create an S3 access point that uses the raw data's S3 bucket as the destination. Create an AWS Lambda function that is invoked by object creation events in the raw data's S3 bucket. Program the Lambda function to redact data for each application. Store the data in each application's S3 access point. Configure each application to consume data from its own S3 access point.
- D. Create an S3 access point that uses the raw data’s S3 bucket as the destination. For each application, create an S3 Object Lambda access point that uses the S3 access point. Configure the AWS Lambda function for each S3 Object Lambda access point to redact data when objects are retrieved. Configure each application to consume data from its own S3 Object Lambda access point

> **Suggested Answer:** D
> **Community Vote:** D (92%), 8%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Sun 04 Aug 2024 14:20) - *Upvotes: 3*
keywords: S3 Object Lambda access point to redact data

---

**c3518fc** (Fri 26 Apr 2024 18:12) - *Upvotes: 3*
https://docs.aws.amazon.com/AmazonS3/latest/userguide/tutorial-s3-object-lambda-uppercase.html

---

**dkp** (Sat 13 Apr 2024 03:47) - *Upvotes: 3*
answer D

---

**anj_k** (Sun 17 Mar 2024 06:04) - *Upvotes: 4*
https://docs.aws.amazon.com/AmazonS3/latest/userguide/tutorial-s3-object-lambda-uppercase.html

---

**dzn** (Thu 07 Mar 2024 13:30) - *Upvotes: 2*
S3 Object Lambda access point is not suitable for generating reports. Generally, creating a report requires an aggregate process, which is expensive. Since reports are expected to be viewed multiple times, it is inefficient to pay for Lambda processing time and CPU costs each time they are viewed.
To adopt D, CloudFront should be added to the front.
https://aws.amazon.com/jp/blogs/aws/new-use-amazon-s3-object-lambda-with-amazon-cloudfront-to-tailor-content-for-end-users/

---

**fdoxxx** (Thu 29 Feb 2024 21:40) - *Upvotes: 4*
D, using S3 Object Lambda access points, is the most appropriate solution for the requirements:
S3 Object Lambda allows you to add your own code to S3 GET requests to modify and process data at the time of retrieval. In this scenario, you can create an S3 access point that uses the raw data's S3 bucket as the destination. For each application, create a separate S3 Object Lambda access point that uses the S3 access point as the source. Configure the AWS Lambda function for each S3 Object Lambda access point to redact data when objects are retrieved.
This solution ensures that each application can access the data with its own redaction rules, and the redaction is applied dynamically at the time of retrieval.

---

**Ramdi1** (Tue 13 Feb 2024 19:03) - *Upvotes: 3*
Single source of truth: This solution maintains a single copy of the raw data in the original S3 bucket, avoiding data duplication and associated costs.
Fine-grained redaction: Each application has its own S3 Object Lambda access point, allowing independent Lambda functions to redact data according to specific needs. This ensures targeted redaction without creating multiple S3 buckets with potentially inefficient data copies.
Efficient access: Applications access the data through their respective S3 Object Lambda access points, incurring the redaction processing only when data is retrieved, improving cost-effectiveness compared to upfront redaction approaches.

---

**Ramdi1** (Tue 13 Feb 2024 19:03) - *Upvotes: 1*
A: Duplicates data for each application, increasing storage costs and maintenance overhead.
B: While Kinesis Data Streams can handle large data volumes, it adds an extra layer of complexity and latency compared to direct S3 access with redaction.
C: Still requires upfront redaction for each application's specific needs, potentially duplicating redacted data across S3 access points.

---

**thanhnv142** (Mon 12 Feb 2024 16:36) - *Upvotes: 3*
D is correct: S3 access point is actually S3 lambda access point, which is option D
A and B: too expensive
C: is not correct

---

**Chelseajcole** (Wed 07 Feb 2024 19:31) - *Upvotes: 2*
D. S3 Bucker endpoint plus Lambda

---


<br/>

## Question 206

*Date: Feb. 7, 2024, 7:32 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses AWS Control Tower and AWS CloudFormation to manage its AWS accounts and to create AWS resources. The company requires all Amazon S3 buckets to be encrypted with AWS Key Management Service (AWS KMS) when the S3 buckets are created in a CloudFormation stack.

Which solution will meet this requirement?

**Options:**
- A. Use AWS Organizations. Attach an SCP that denies the s3:PutObject permission if the request does not include an x-amz-server-side-encryption header that requests server-side encryption with AWS KMS keys (SSE-KMS).
- B. Use AWS Control Tower with a multi-account environment. Configure and enable proactive AWS Control Tower controls on all OUs with CloudFormation hooks.
- C. Use AWS Control Tower with a multi-account environment. Configure and enable detective AWS Control Tower controls on all OUs with CloudFormation hooks.
- D. Use AWS Organizations. Create an AWS Config organizational rule to check whether a KMS encryption key is enabled for all S3 buckets. Deploy the rule. Create and apply an SCP to prevent users from stopping and deleting AWS Config across all AWS accounts,

> **Suggested Answer:** B
> **Community Vote:** B (97%), 3%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Ramdi1** (Tue 13 Feb 2024 19:11) - *Upvotes: 5*
Proactive controls: Proactive controls are preventative measures that block actions violating defined policies before they occur. This ensures encryption gets applied automatically during S3 bucket creation within CloudFormation stacks.
CloudFormation hooks: Hooks enable Control Tower to intercept and enforce policies on CloudFormation stack operations, making it ideal for enforcing encryption during resource creation.
Multi-account environment: Since the requirement applies across all accounts, Control Tower's multi-account capabilities ensure consistent policy enforcement throughout the organization.

---

**Ramdi1** (Tue 13 Feb 2024 19:12) - *Upvotes: 1*
The other options have limitations:


A: While SCPs enforce policies, they react to actions instead of proactively preventing them. Additionally, denying s3:PutObject might be too restrictive as it can impact other legitimate operations.
C: Detective controls monitor and report on existing resources, not preventing non-compliant creations.
D: Config and SCPs combined address encryption checks and user limitations, but lack the direct integration with CloudFormation stacks crucial for enforcing during creation.

---

**thanhnv142** (Mon 12 Feb 2024 16:41) - *Upvotes: 5*
B is correct: <AWS Control Tower> means we need to use the proactive control
A: SCP s3:PutObject permission only deny action related to put object to S3, not when creating it
B: Detective controls used only for monitoring
C: correct
D: This option can achive the goal of the question. However, it is way more complicated than B.

---

**jamesf** (Mon 05 Aug 2024 05:48) - *Upvotes: 3*
keywords: proactive

---

**Gomer** (Mon 08 Jul 2024 23:44) - *Upvotes: 4*
Here's the Control Tower proactive control:
"[CT.S3.PR.10] Require an Amazon S3 bucket to have server-side encryption configured using an AWS KMS key"
https://docs.aws.amazon.com/controltower/latest/controlreference/s3-rules.html#ct-s3-pr-10-description

---

**Venki_dev** (Sat 15 Jun 2024 06:52) - *Upvotes: 2*
Clearly answer is B , here is article that explains the same.
https://aws.amazon.com/blogs/mt/how-aws-control-tower-users-can-proactively-verify-compliance-in-aws-cloudformation-stacks/

Answer D with config rule also fits the bill (if no control tower), but since we have Control tower managing the accounts already its better to make use of the features that Control tower leverages

---

**dkp** (Sat 13 Apr 2024 04:00) - *Upvotes: 3*
Answer B

---

**fdoxxx** (Sun 07 Apr 2024 11:05) - *Upvotes: 3*
B is better than D...

---

**ogerber** (Tue 19 Mar 2024 19:03) - *Upvotes: 3*
B, 100%

---

**fdoxxx** (Sun 03 Mar 2024 10:19) - *Upvotes: 1*
D provides a solution that leverages AWS Organizations and AWS Config to enforce the requirement for AWS KMS encryption on all S3 buckets created through CloudFormation:
AWS Config Organizational Rule: Create an AWS Config organizational rule to check whether a KMS encryption key is enabled for all S3 buckets. This rule helps ensure that the encryption requirement is enforced.
Options A, B, and C do not directly address the requirement for AWS KMS encryption on S3 buckets created through CloudFormation:
Option A mentions using an SCP but focuses on denying s3:PutObject without the required encryption header. However, this approach doesn't ensure that the encryption is enforced through AWS KMS.
Options B and C mention using AWS Control Tower with proactive or detective controls, but they don't specifically address the encryption requirement for S3 buckets.

---


<br/>

## Question 207

*Date: Feb. 6, 2024, 7:27 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer has developed an AWS Lambda function. The Lambda function starts an AWS CloudFormation drift detection operation on all supported resources for a specific CloudFormation stack. The Lambda function then exits its invocation.

The DevOps engineer has created an Amazon EventBridge scheduled rule that invokes the Lambda function every hour. An Amazon Simple Notification Service (Amazon SNS) topic already exists in the AWS account. The DevOps engineer has subscribed to the SNS topic to receive notifications.

The DevOps engineer needs to receive a notification as soon as possible when drift is detected in this specific stack configuration.

Which solution will meet these requirements?

**Options:**
- A. Configure the existing EventBridge rule to also target the SNS topic. Configure an SNS subscription filter policy to match the CloudFormation stack. Attach the subscription filter policy to the SNS topic.
- B. Create a second Lambda function to query the CloudFormation API for the drift detection results for the stack. Configure the second Lambda function to publish a message to the SNS topic if drift is detected. Adjust the existing EventBridge rule to also target the second Lambda function.
- C. Configure Amazon GuardDuty in the account with drift detection for all CloudFormation stacks. Create a second EventBridge rule that reacts to the GuardDuty drift detection event finding for the specific CloudFormation stack. Configure the SNS topic as a target of the second EventBridge rule.
- D. Configure AWS Config in the account. Use the cloudformation-stack-drift-detection-check managed rule. Create a second EventBridge rule that reacts to a compliance change event for the CloudFormation stack. Configure the SNS topic as a target of the second EventBridge rule.

> **Suggested Answer:** D
> **Community Vote:** D (73%), B (24%), 2%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**Nano803** (Fri 23 Feb 2024 06:02) - *Upvotes: 7*
I recommend checking out this blog which utilizes AWS Config and discusses Edenbridge. Here is the link: https://aws.amazon.com/blogs/mt/implementing-an-alarm-to-automatically-detect-drift-in-aws-cloudformation-stacks/"

---

**Gomer** (Tue 09 Jul 2024 02:10) - *Upvotes: 1*
Info gleaned from following the link(++):

cloudformation-stack-drift-detection-check
AWS Config rule that checks if the actual configuration of a AWS CloudFormation (AWS CloudFormation) stack differs, or has drifted, from the expected configuration.

MaximumExecutionFrequency
The maximum frequency with which AWS Config runs evaluations for a rule.

Example stack to detect and notify on drift:
[...]
MaximumExecutionFrequency:
Description: "The maximum frequency with which drift in CloudFormation stacks need to be evaluated (default - One_Hour)"
Type: "String"
Default: "One_Hour"
AllowedValues: ["One_Hour","Three_Hours","Six_Hours","Twelve_Hours","TwentyFour_Hours"]
[...]

---

**DanShone** (Sat 16 Mar 2024 10:19) - *Upvotes: 5*
D woudl be suitable - https://docs.aws.amazon.com/config/latest/developerguide/cloudformation-stack-drift-detection-check.html
B would not work as it would still only be triggered once per hour as is using the same event bridge rule

---

**ryuhei** (Sun 24 Aug 2025 13:07) - *Upvotes: 1*
I think the correct answer is D, but in D, AWS Config will automatically perform drift detection, so won't the Lambda function created in the question (which starts drift detection every hour) become unnecessary?

---

**nickp84** (Fri 16 May 2025 15:50) - *Upvotes: 1*
D: AWS Config’s cloudformation-stack-drift-detection-check is a managed rule, but it runs on a periodic basis and may not provide immediate detection or notification. It also may not be as responsive as a custom Lambda-based solution.

---

**iulian0585** (Mon 05 Aug 2024 14:27) - *Upvotes: 3*
The solution that will meet the requirements of receiving a notification as soon as possible when drift is detected in the specific CloudFormation stack configuration is:

B. Create a second Lambda function to query the CloudFormation API for the drift detection results for the stack. Configure the second Lambda function to publish a message to the SNS topic if drift is detected. Adjust the existing EventBridge rule to also target the second Lambda function.

Option D (Using AWS Config) would introduce additional complexity and potential delays, as AWS Config periodically evaluates resource configurations and may not provide immediate notifications upon drift detection.

By creating a separate Lambda function dedicated to monitoring drift detection results and publishing notifications to the existing SNS topic, you can ensure timely and reliable notifications while maintaining a modular and scalable architecture.

---

**chinchin97** (Fri 30 Aug 2024 07:43) - *Upvotes: 1*
This approach introduces additional complexity by adding another Lambda function to query and check for drift manually. Who is going to trigger this Lambda function? Even if you do it on a interval, it defeats the purpose of getting notified immediately.

AWS Config provides a more straightforward and managed way to detect and notify on drift with a managed rule, cloudformation-stack-drift-detection-check.
https://docs.aws.amazon.com/config/latest/developerguide/cloudformation-stack-drift-detection-check.html

---

**dkp** (Sat 13 Apr 2024 04:52) - *Upvotes: 4*
answer D
AWS Config Integration: AWS Config is specifically designed to monitor and detect configuration changes and drifts in AWS resources, including CloudFormation stacks. Using AWS Config's built-in cloudformation-stack-drift-detection-check managed rule ensures comprehensive and reliable drift detection for CloudFormation stacks.

Event-Driven Architecture: Creating an EventBridge rule that reacts to a compliance change event for the CloudFormation stack allows you to trigger an alert as soon as drift is detected. This event-driven approach ensures timely detection and alerting for CloudFormation stack drift.

SNS Notification: By configuring the SNS topic as a target of the EventBridge rule, you can easily send notifications/alerts to various endpoints, including email, SMS, or other AWS services, ensuring immediate alerting when drift is detected.

---

**WhyIronMan** (Sat 30 Mar 2024 12:45) - *Upvotes: 4*
D, Use the cloudformation-stack-drift-detection-check managed rule
B uses scheduled rule will not notify as soon as possible as it runs hourly

---

**Shasha1** (Thu 14 Mar 2024 10:19) - *Upvotes: 2*
D
refer this: https://docs.aws.amazon.com/config/latest/developerguide/cloudformation-stack-drift-detection-check.html

---


<br/>

## Question 208

*Date: Feb. 6, 2024, 7:31 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has deployed a complex container-based workload on AWS. The workload uses Amazon Managed Service for Prometheus for monitoring. The workload runs in an Amazon
Elastic Kubernetes Service (Amazon EKS) cluster in an AWS account.

The company’s DevOps team wants to receive workload alerts by using the company’s Amazon Simple Notification Service (Amazon SNS) topic. The SNS topic is in the same AWS account as the EKS cluster.

Which combination of steps will meet these requirements? (Choose three.)

**Options:**
- A. Use the Amazon Managed Service for Prometheus remote write URL to send alerts to the SNS topic
- B. Create an alerting rule that checks the availability of each of the workload’s containers.
- C. Create an alert manager configuration for the SNS topic.
- D. Modify the access policy of the SNS topic. Grant the aps.amazonaws.com service principal the sns:Publish permission and the sns:GetTopicAttributes permission for the SNS topic.
- E. Modify the IAM role that Amazon Managed Service for Prometheus uses. Grant the role the sns:Publish permission and the sns:GetTopicAttributes permission for the SNS topic.
- F. Create an OpenID Connect (OIDC) provider for the EKS cluster. Create a cluster service account. Grant the account the sns:Publish permission and the sns:GetTopicAttributes permission by using an IAM role.

> **Suggested Answer:** BCD
> **Community Vote:** BCD (72%), BCE (18%), 4%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**dkp** (Sat 13 Apr 2024 05:28) - *Upvotes: 5*
ill go with bcd

---

**WhyIronMan** (Sat 30 Mar 2024 12:42) - *Upvotes: 5*
B,C,D.
There is no way to exclude D, as it is really necessary as per all AWS documentations.
You can be in doubt of all the others, but not D

---

**teo2157** (Tue 10 Dec 2024 15:13) - *Upvotes: 2*
https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alertmanager-receiver.html

---

**seetpt** (Sat 24 Aug 2024 14:20) - *Upvotes: 1*
I think BCD is true

---

**jamesf** (Tue 30 Jul 2024 11:16) - *Upvotes: 4*
BCD
For D as You must give Amazon Managed Service for Prometheus permission to send messages to your Amazon SNS topic. The following policy statement will give that permission. ...
https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alertmanager-receiver-AMPpermission.html

---

**trungtd** (Thu 11 Jul 2024 04:59) - *Upvotes: 4*
Agree with BCD

---

**Gomer** (Tue 09 Jul 2024 04:51) - *Upvotes: 5*
B:(YES) Steps towards "configuring rules and the alert manager in Amazon Managed Service for Prometheus via the AWS management console."
"define an alerting rule that causes the Alert Manager to send a notification if a certain condition (defined in expr) holds true for a specified time period (for)."
cat << EOF > rules.yaml
groups:
[...]
rules:
- alert: metric:alerting_rule
expr: rate(adot_test_counter0[5m]) > 0.014
for: 5m
EOF

C:(YES) Add "SNS receiver to" "alert manager configuration" using ARN of "SNS topic"(Q208.5)

D:(YES) "Give Amazon Managed Service for Prometheus permission to send messages to" SNS
"Choose Access policy and add the following policy statement to the existing policy."
[...]
"Principal": {
"Service": "aps.amazonaws.com"
},
"Action": [
"sns:Publish",
"sns:GetTopicAttributes"

---

**Gomer** (Tue 09 Jul 2024 04:52) - *Upvotes: 1*
https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alertmanager-receiver-config.html
https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alertmanager-receiver-AMPpermission.html
https://aws.amazon.com/blogs/mt/amazon-managed-service-for-prometheus-is-now-generally-available/

---

**KaranNishad** (Sat 29 Jun 2024 16:52) - *Upvotes: 4*
BCD is answer

---

**xdkonorek2** (Sat 22 Jun 2024 17:09) - *Upvotes: 5*
https://docs.aws.amazon.com/prometheus/latest/userguide/Troubleshooting-alerting-no-policy.html

---


<br/>

## Question 209

*Date: Feb. 7, 2024, 5:39 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company's organization in AWS Organizations has a single OU. The company runs Amazon EC2 instances in the OU accounts. The company needs to limit the use of each EC2 instance’s credentials to the specific EC2 instance that the credential is assigned to. A DevOps engineer must configure security for the EC2 instances.

Which solution will meet these requirements?

**Options:**
- A. Create an SCP that specifies the VPC CIDR block. Configure the SCP to check whether the value of the aws:VpcSourcelp condition key is in the specified block. In the same SCP check, check whether the values of the aws:EC2InstanceSourcePrivatelPv4 and aws:SourceVpc condition keys are the same. Deny access if either condition is false. Apply the SCP to the OU.
- B. Create an SCP that checks whether the values of the aws:EC2InstanceSourceVPC and aws:SourceVpc condition keys are the same. Deny access if the values are not the same. In the same SCP check, check whether the values of the aws:EC2InstanceSourcePrivateIPv4 and aws:VpcSourceIp condition keys are the same. Deny access if the values are not the same. Apply the SCP to the OU.
- C. Create an SCP that includes a list of acceptable VPC values and checks whether the value of the aws:SourceVpc condition key is in the list. In the same SCP check, define a list of acceptable IP address values and check whether the value of the aws:VpcSourceIp condition key is in the list. Deny access if either condition is false. Apply the SCP to each account in the organization.
- D. Create an SCP that checks whether the values of the aws:EC2InstanceSourceVPC and aws:VpcSourceIp condition keys are the same. Deny access if the values are not the same. In the same SCP check, check whether the values of the aws:EC2InstanceSourcePrivateIPv4 and aws:SourceVpc condition keys are the same. Deny access if the values are not the same. Apply the SCP to each account in the organization.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**devakram** (Sat 13 Apr 2024 21:33) - *Upvotes: 5*
B obviously : https://aws.amazon.com/blogs/security/how-to-use-policies-to-restrict-where-ec2-instance-credentials-can-be-used-from/

---

**RajAWSDevOps007** (Mon 16 Dec 2024 11:52) - *Upvotes: 1*
Answer is B here.

However, pls note SCPs can be applied directly to member accounts as well-
https://docs.aws.amazon.com › orgs_manage_policies_scps

---

**6ef9a08** (Mon 01 Jul 2024 16:06) - *Upvotes: 1*
NOT C,D:
"Apply the SCP to each account in the organization" - SCPs apply to OUs, not accounts

---

**fdoxxx** (Sun 03 Mar 2024 10:55) - *Upvotes: 4*
B is the most appropriate solution:
Option A introduces unnecessary complexity with multiple conditions and may not provide the intended restriction.
Option C suggests creating an SCP with lists of acceptable values, but it might be challenging to maintain and is less straightforward.
Option D has the same issues as option A, introducing complexity with multiple conditions.

---

**Diego1414** (Mon 26 Feb 2024 23:57) - *Upvotes: 4*
Answer: B - aws:EC2InstanceSourceVPC = aws:SourceVpc and aws:EC2InstanceSourcePrivateIPv4 = aws:VpcSourceIp
https://aws.amazon.com/blogs/security/how-to-use-policies-to-restrict-where-ec2-instance-credentials-can-be-used-from/

---

**thanhnv142** (Mon 12 Feb 2024 16:57) - *Upvotes: 4*
B is correct: aws:EC2InstanceSourceVPC and aws:SourceVpc must be the same. Additionally, aws:EC2InstanceSourcePrivateIPv4 and aws:VpcSourceIp must be the same
A: irrelevant
C: <define a list of acceptable IP address values> is not correct
D: <aws:EC2InstanceSourceVPC and aws:VpcSourceIp> is incorrect

---

**thanhnv142** (Mon 12 Feb 2024 16:57) - *Upvotes: 2*
Finally, I 've made it to the last one

---

**vortegon** (Sun 11 Feb 2024 05:40) - *Upvotes: 2*
https://aws.amazon.com/fr/blogs/security/how-to-use-policies-to-restrict-where-ec2-instance-credentials-can-be-used-from/

---

**Chelseajcole** (Wed 07 Feb 2024 19:49) - *Upvotes: 1*
B. checks whether the values of the aws:EC2InstanceSourceVPC and aws:SourceVpc condition keys are the same and Apply the SCP to the OU.

---

**Arnaud92** (Wed 07 Feb 2024 17:39) - *Upvotes: 1*
Source: https://aws.amazon.com/fr/blogs/security/how-to-use-policies-to-restrict-where-ec2-instance-credentials-can-be-used-from/

---


<br/>

## Question 210

*Date: March 13, 2024, 6:21 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has a fleet of Amazon EC2 instances that run Linux in a single AWS account. The company is using an AWS Systems Manager Automation task across the EC2 instances.

During the most recent patch cycle, several EC2 instances went into an error state because of insufficient available disk space. A DevOps engineer needs to ensure that the EC2 instances have sufficient available disk space during the patching process in the future.

Which combination of steps will meet these requirements? (Choose two.)

**Options:**
- A. Ensure that the Amazon CloudWatch agent is installed on all EC2 instances.
- B. Create a cron job that is installed on each EC2 instance to periodically delete temporary files.
- C. Create an Amazon CloudWatch log group for the EC2 instances. Configure a cron job that is installed on each EC2 instance to write the available disk space to a CloudWatch log stream for the relevant EC2 instance.
- D. Create an Amazon CloudWatch alarm to monitor available disk space on all EC2 instances. Add the alarm as a safety control to the Systems Manager Automation task.
- E. Create an AWS Lambda function to periodically check for sufficient available disk space on all EC2 instances by evaluating each EC2 instance's respective Amazon CloudWatch log stream.

> **Suggested Answer:** AD
> **Community Vote:** AD (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**trungtd** (Thu 11 Jul 2024 05:20) - *Upvotes: 3*
A. Ensure that the Amazon CloudWatch agent is installed on all EC2 instances.

The Amazon CloudWatch agent can collect system-level metrics, including disk space usage, and send them to Amazon CloudWatch. This will allow you to monitor the available disk space on each EC2 instance.

D. Create an Amazon CloudWatch alarm to monitor available disk space on all EC2 instances. Add the alarm as a safety control to the Systems Manager Automation task.

By setting up a CloudWatch alarm to monitor the available disk space, you can trigger actions or notifications when the disk space falls below a certain threshold. Adding this alarm as a safety control to the Systems Manager Automation task ensures that the patching process will only proceed if there is sufficient available disk space.

---

**dkp** (Sat 13 Apr 2024 06:03) - *Upvotes: 3*
answer A & D
to configure disk usage, we can use custom metrics in the Cloudwatch agent configuration. don't need a cron job to pipe the disk usage.

---

**WhyIronMan** (Sat 30 Mar 2024 12:29) - *Upvotes: 4*
A,D, Simple and accurate

---

**Nano803** (Wed 13 Mar 2024 16:26) - *Upvotes: 4*
This article details the solution:
https://aws.amazon.com/blogs/mt/avoid-patching-failures-due-to-low-disk-space-with-aws-systems-manager-automation-and-cloudwatch-alarms/

---

**Seoyong** (Wed 13 Mar 2024 06:21) - *Upvotes: 4*
A: Install AWS CloudWatch agent which will push disk information to a log group.
B: Alarm depends on disk space.

---


<br/>

## Question 211

*Date: March 13, 2024, 6:22 a.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer is building an application that uses an AWS Lambda function to query an Amazon Aurora MySQL DB cluster. The Lambda function performs only read queries. Amazon EventBridge events invoke the Lambda function.

As more events invoke the Lambda function each second, the database's latency increases and the database's throughput decreases. The DevOps engineer needs to improve the performance of the application.

Which combination of steps will meet these requirements? (Choose three.)

**Options:**
- A. Use Amazon RDS Proxy to create a proxy. Connect the proxy to the Aurora cluster reader endpoint. Set a maximum connections percentage on the proxy.
- B. Implement database connection pooling inside the Lambda code. Set a maximum number of connections on the database connection pool.
- C. Implement the database connection opening outside the Lambda event handler code.
- D. Implement the database connection opening and closing inside the Lambda event handler code.
- E. Connect to the proxy endpoint from the Lambda function.
- F. Connect to the Aurora cluster endpoint from the Lambda function.

> **Suggested Answer:** ACE
> **Community Vote:** ACE (84%), ADE (16%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**WhyIronMan** (Sat 30 Mar 2024 12:24) - *Upvotes: 6*
A, D, E
For those going with A,C,E, the same link already provided here https://aws.amazon.com/blogs/compute/using-amazon-rds-proxy-with-aws-lambda/
in the nodejs/javascript code you can see that database connection opening (connection.connect) and closing (connection.end) are being handled INSIDE the handler function, which is correct because you want to open connections but it is a good practice to close connections

---

**dkp** (Sat 13 Apr 2024 06:15) - *Upvotes: 6*
Opening and closing database connections outside the Lambda handler allows for efficient reuse of connections and implements connection pooling

---

**GripZA** (Fri 16 Aug 2024 09:18) - *Upvotes: 3*
"By opening the database connection outside the event handler, the connection can be reused across multiple invocations, which reduces the overhead of establishing new connections repeatedly."

---

**jamesf** (Tue 30 Jul 2024 13:39) - *Upvotes: 4*
Should be ACE
Opening database connections OUTSIDE the Lambda handler allows for efficient reuse of connections and implements connection pooling

---

**c3518fc** (Fri 26 Apr 2024 19:55) - *Upvotes: 4*
https://aws.amazon.com/blogs/compute/using-amazon-rds-proxy-with-aws-lambda/

---

**L1_** (Sun 17 Mar 2024 19:52) - *Upvotes: 5*
ACE: I also agree - https://aws.amazon.com/blogs/compute/using-amazon-rds-proxy-with-aws-lambda/

---

**DanShone** (Sat 16 Mar 2024 09:51) - *Upvotes: 5*
I agree

---

**Seoyong** (Wed 13 Mar 2024 06:22) - *Upvotes: 5*
perfect

---


<br/>

## Question 212

*Date: March 27, 2024, 5:36 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an AWS CloudFormation stack that is deployed in a single AWS account. The company has configured the stack to send event notifications to an Amazon Simple Notification Service (Amazon SNS) topic.

A DevOps engineer must implement an automated solution that applies a tag to the specific CloudFormation stack instance only after a successful stack update occurs. The DevOps engineer has created an AWS Lambda function that applies and updates this tag for the specific stack instance.

Which solution will meet these requirements?

**Options:**
- A. Run the AWS-UpdateCloudFormationStack AWS Systems ManagerAutomation runbook when Systems Manager detects an UPDATE_COMPLETE event for the instance status of the CloudFormation stack. Configure the runbook to invoke the Lambda function.
- B. Create a custom AWS Config rule that produces a compliance change event if the CloudFormation stack has an UPDATE_COMPLETE instance status. Configure AWS Config to directly invoke the Lambda function to automatically remediate the change event.
- C. Create an Amazon EventBridge rule that matches the UPDATE_COMPLETE event pattern for the instance status of the CloudFormation stack. Configure the rule to invoke the Lambda function.
- D. Adjust the configuration of the CloudFormation stack to send notifications for only an UPDATE_COMPLETE instance status event to the SNS topic. Subscribe the Lambda function to the SNS topic.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**GripZA** (Fri 16 Aug 2024 09:25) - *Upvotes: 3*
EventBridge is designed to detect specific events in AWS services, and it can be configured to match events such as UPDATE_COMPLETE from CloudFormation.
This allows you to automate the process of tagging the CloudFormation stack instancess whenever the UPDATE_COMPLETE event occurs.
The EventBridge rule will trigger the Lambda function, which will then apply the necessary tag to the stack.

---

**dkp** (Sat 13 Apr 2024 07:19) - *Upvotes: 3*
options C and D are suitable for implementing the automated solution. However, using Option C with Amazon EventBridge is more direct and does not require additional SNS configuration

---

**WhyIronMan** (Sat 30 Mar 2024 12:10) - *Upvotes: 3*
C,
EventBridge + Lambda Function
https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html

---

**ogerber** (Wed 27 Mar 2024 17:36) - *Upvotes: 3*
Its C, 100%

---


<br/>

## Question 213

*Date: March 16, 2024, 7:54 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company deploys an application to two AWS Regions. The application creates and stores objects in an Amazon S3 bucket that is in the same Region as the application. Both deployments of the application need to have access to all the objects and their metadata from both Regions. The company has configured two-way replication between the S3 buckets and has enabled S3 Replication metrics on each S3 bucket.

A DevOps engineer needs to implement a solution that retries the replication process if an object fails to replicate.

Which solution will meet these requirements?

**Options:**
- A. Create an Amazon EventBridge rule that listens to S3 event notifications for failed replication events. Create an AWS Lambda function that downloads the failed replication object and then runs a PutObject command for the object to the destination bucket. Configure the EventBridge rule to invoke the Lambda function to handle the object that failed to replicate.
- B. Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure S3 event notifications to send failed replication notifications to the SQS queue. Create an AWS Lambda function that downloads the failed replication object and then runs a PutObject command for the object to the destination bucket. Configure the Lambda function to poll the queue for notifications to process.
- C. Create an Amazon EventBridge rule that listens to S3 event notifications for failed replications. Create an AWS Lambda function that downloads the failed replication object and then runs a PutObject command for the object to the destination bucket.
- D. Create an AWS Lambda function that will use S3 batch operations to retry the replication on the existing object for a failed replication. Configure S3 event notifications to send failed replication notifications to the Lambda function.

> **Suggested Answer:** D
> **Community Vote:** D (85%), A (15%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**r4nd0m1z3r** (Wed 20 Mar 2024 19:02) - *Upvotes: 6*
S3 Batch Replication provides you a way to replicate objects that existed before a replication configuration was in place, objects that have previously been replicated, and objects that have failed replication.
https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-batch-replication-batch.html

---

**L1_** (Sun 17 Mar 2024 20:12) - *Upvotes: 5*
This post suggests D: https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-batch-replication-batch.html

---

**Srikantha** (Sun 30 Mar 2025 15:17) - *Upvotes: 1*
Option A provides the most efficient and streamlined solution by using EventBridge and Lambda to automatically retry replication failures

---

**iulian0585** (Mon 05 Aug 2024 15:03) - *Upvotes: 4*
By using S3 Batch Replication, you can replicate the following types of objects:

Objects that existed before a replication configuration was in place

Objects that have previously been replicated

Objects that have failed replication

---

**xdkonorek2** (Sat 22 Jun 2024 19:35) - *Upvotes: 2*
100% D
S3 Batch Replication is one of actions provided by S3 batch operations https://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-operations.html

batch operations can be used for objects that failed replication
https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-batch-replication-batch.html#:~:text=Objects%20that%20have%20failed%20replication

---

**c3518fc** (Fri 26 Apr 2024 20:12) - *Upvotes: 3*
S3 Batch Replication provides you a way to replicate objects that existed before a replication configuration was in place, objects that have previously been replicated, and objects that have failed replication. This is done through the use of a Batch Operations job. https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-batch-replication-batch.html

---

**WhyIronMan** (Sat 30 Mar 2024 12:02) - *Upvotes: 3*
D,
https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-batch-replication-batch.html

---

**CloudHell** (Sat 16 Mar 2024 19:54) - *Upvotes: 3*
It's A for me.

---


<br/>

## Question 214

*Date: March 16, 2024, 8:20 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company needs to implement failover for its application. The application includes an Amazon CloudFront distribution and a public Application Load Balancer (ALB) in an AWS Region. The company has configured the ALB as the default origin for the distribution.

After some recent application outages, the company wants a zero-second RTO. The company deploys the application to a secondary Region in a warm standby configuration. A DevOps engineer needs to automate the failover of the application to the secondary Region so that HTTP GET requests meet the desired RTO.

Which solution will meet these requirements?

**Options:**
- A. Create a second CloudFront distribution that has the secondary ALB as the default origin. Create Amazon Route 53 alias records that have a failover policy and Evaluate Target Health set to Yes for both CloudFront distributions. Update the application to use the new record set.
- B. Create a new origin on the distribution for the secondary ALCreate a new origin group. Set the original ALB as the primary origin. Configure the origin group to fail over for HTTP 5xx status codes. Update the default behavior to use the origin group.
- C. Create Amazon Route 53 alias records that have a failover policy and Evaluate Target Health set to Yes for both ALBs. Set the TTL of both records to 0. Update the distribution's origin to use the new record set.
- D. Create a CloudFront function that detects HTTP 5xx status codes. Configure the function to return a 307 Temporary Redirect error response to the secondary ALB if the function detects 5xx status codes. Update the distribution's default behavior to send origin responses to the function.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**youonebe** (Tue 24 Dec 2024 20:15) - *Upvotes: 2*
vote for b

---

**dkp** (Sun 13 Oct 2024 08:11) - *Upvotes: 3*
answer B

---

**WhyIronMan** (Mon 30 Sep 2024 11:00) - *Upvotes: 4*
B,
mazon CloudFront offers origin failover, where if a given request to the primary endpoint fails, CloudFront routes the request to the secondary endpoint. Unlike the failover operations described previously, all subsequent requests still go to the primary endpoint, and failover is done per each request.

https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html#concept_origin_groups.creating

---

**CloudHell** (Mon 16 Sep 2024 19:20) - *Upvotes: 4*
It's B for me.

https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html

---


<br/>

## Question 215

*Date: March 16, 2024, 8:38 p.m.
Disclaimers:
- ExamTopics website is not rel*

A cloud team uses AWS Organizations and AWS IAM Identity Center (AWS Single Sign-On) to manage a company's AWS accounts. The company recently established a research team. The research team requires the ability to fully manage the resources in its account. The research team must not be able to create IAM users.

The cloud team creates a Research Administrator permission set in IAM Identity Center for the research team. The permission set has the AdministratorAccess AWS managed policy attached. The cloud team must ensure that no one on the research team can create IAM users.

Which solution will meet these requirements?

**Options:**
- A. Create an IAM policy that denies the iam:CreateUser action. Attach the IAM policy to the Research Administrator permission set.
- B. Create an IAM policy that allows all actions except the iam:CreateUser action. Use the IAM policy to set the permissions boundary for the Research Administrator permission set.
- C. Create an SCP that denies the iam:CreateUser action. Attach the SCP to the research team's AWS account.
- D. Create an AWS Lambda function that deletes IAM users. Create an Amazon EventBridge rule that detects the IAM CreateUser event. Configure the rule to invoke the Lambda function.

> **Suggested Answer:** C
> **Community Vote:** C (52%), A (48%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**CloudHell** (Sat 16 Mar 2024 20:38) - *Upvotes: 8*
It's C for me, here is a link with a similar scenario:

https://asecure.cloud/a/scp_deny_iam_user_creation_w_exception/

---

**dkp** (Sat 13 Apr 2024 08:47) - *Upvotes: 7*
While IAM policies can deny actions, they are typically attached to individual users or roles. In this scenario, you want to restrict user creation across the entire research team's account, making an SCP the more appropriate choice.

---

**teo2157** (Mon 13 Jan 2025 15:21) - *Upvotes: 3*
A as the restriction just needs to be applied to the research team but not the whole account users

---

**MrTizz** (Wed 08 Jan 2025 19:31) - *Upvotes: 3*
The wording is that only the research team should not be allowed to create users. This is A as the Permission Set will apply to just them. If you choose C it's an account wide deny so no other user or admins would be able to create a user which is outside the scope of the question.

---

**spring21** (Tue 31 Dec 2024 23:08) - *Upvotes: 4*
IAM Policy to Deny iam:CreateUser
An IAM policy is applied to individual IAM users, groups, or roles within an AWS account. Here's an example policy that denies the iam:CreateUser action:

IAM Policy JSON
json
Copy code
{
"Version": "2012-10-17",
"Statement": [
{
"Effect": "Deny",
"Action": "iam:CreateUser",
"Resource": "*"
}
]
}
Steps to Attach IAM Policy to Research Administrator Permission Set:
Navigate to AWS IAM Identity Center (SSO).
Select the Permission Sets section.
Choose the Research Administrator permission set.
Attach the custom policy above to the permission set by selecting Add permissions → Custom policy.

---

**spring21** (Tue 31 Dec 2024 23:08) - *Upvotes: 1*
Comparison: IAM Policy vs SCP
Aspect IAM Policy SCP
Scope Affects only the user, group, or role it is attached to. Applies to all users, roles, and policies within the target account or OU.
Use Case Granular control within an account. Broad guardrails across accounts or OUs.
Hierarchy Impact Does not affect parent accounts or organization. Enforces policies across all child accounts.
Effect Denies specific actions only for targeted users or groups. Overrides any permissions granted at any level within the account.

---

**Impromptu** (Thu 21 Nov 2024 19:26) - *Upvotes: 4*
A meets the requirements.
C would deny CreateUser for all the IAM entities in the account, not only the research team

---

**GripZA** (Fri 16 Aug 2024 09:49) - *Upvotes: 2*
For those who selected C, why would you create ab SCP that will deny any IAM user from creating another IAM when the question clearly states only the research team shouldn't be able to create an IAM user? the deny policy will restrict only the Research Administrator permission set, which is what we want.

---

**jamesf** (Tue 30 Jul 2024 14:03) - *Upvotes: 3*
i go for C
just make sure no one can create account
scp also can create with exception as mentioned by @CloudHell

---

**tgv** (Thu 18 Jul 2024 16:11) - *Upvotes: 5*
I'll go for A as the question says:
"The cloud team must ensure that no one on the research team can create IAM users."

C will block everybody (not just the research team)

---


<br/>

## Question 216

*Date: March 27, 2024, 5:58 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company releases a new application in a new AWS account. The application includes an AWS Lambda function that processes messages from an Amazon Simple Queue Service (Amazon SQS) standard queue. The Lambda function stores the results in an Amazon S3 bucket for further downstream processing. The Lambda function needs to process the messages within a specific period of time after the messages are published. The Lambda function has a batch size of 10 messages and takes a few seconds to process a batch of messages.

As load increases on the application's first day of service, messages in the queue accumulate at a greater rate than the Lambda function can process the messages. Some messages miss the required processing timelines. The logs show that many messages in the queue have data that is not valid. The company needs to meet the timeline requirements for messages that have valid data.

Which solution will meet these requirements?

**Options:**
- A. Increase the Lambda function's batch size. Change the SQS standard queue to an SQS FIFO queue. Request a Lambda concurrency increase in the AWS Region.
- B. Reduce the Lambda function's batch size. Increase the SQS message throughput quota. Request a Lambda concurrency increase in the AWS Region.
- C. Increase the Lambda function's batch size. Configure S3 Transfer Acceleration on the S3 bucket. Configure an SQS dead-letter queue.
- D. Keep the Lambda function's batch size the same. Configure the Lambda function to report failed batch items. Configure an SQS dead-letter queue.

> **Suggested Answer:** D
> **Community Vote:** D (93%), 7%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**c3518fc** (Fri 26 Apr 2024 20:38) - *Upvotes: 5*
Configure a dead-letter queue to avoid creating a snowball anti-pattern in your serverless application’s architecture. For more information, see the Avoiding snowball anti-patterns section of this guide.

Configure your Lambda function event source mapping to make only the failed messages visible. To do this, you must include the value ReportBatchItemFailures in the FunctionResponseTypes list when configuring your event source mapping. https://docs.aws.amazon.com/prescriptive-guidance/latest/lambda-event-filtering-partial-batch-responses-for-sqs/best-practices-partial-batch-responses.html

---

**hk0308** (Wed 25 Dec 2024 03:11) - *Upvotes: 1*
We have to increase the batch size to speed up the processing.
D makes no sense since it will not speed up the processing in anyway.
A cannot be right since it used FIFO queue which will reduce lambda concurrency.

---

**VerRi** (Thu 07 Nov 2024 02:50) - *Upvotes: 2*
A. 1 failure within a batch will cause all messages in that batch to fail, blocking other tasks and delaying overall processing

---

**seetpt** (Thu 02 May 2024 15:00) - *Upvotes: 2*
D for me

---

**dkp** (Sat 13 Apr 2024 09:06) - *Upvotes: 2*
answer D seems more approprite

---

**Ola2234** (Sat 06 Apr 2024 10:26) - *Upvotes: 1*
I am torn between option A or D

---

**WhyIronMan** (Sat 30 Mar 2024 11:50) - *Upvotes: 4*
D,
https://docs.aws.amazon.com/prescriptive-guidance/latest/lambda-event-filtering-partial-batch-responses-for-sqs/best-practices-partial-batch-responses.html

---

**ogerber** (Wed 27 Mar 2024 17:58) - *Upvotes: 2*
its D, 100%

---


<br/>

## Question 217

*Date: March 27, 2024, 9:38 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an application that runs on AWS Lambda and sends logs to Amazon CloudWatch Logs. An Amazon Kinesis data stream is subscribed to the log groups in CloudWatch Logs. A single consumer Lambda function processes the logs from the data stream and stores the logs in an Amazon S3 bucket.

The company’s DevOps team has noticed high latency during the processing and ingestion of some logs.

Which combination of steps will reduce the latency? (Choose three.)

**Options:**
- A. Create a data stream consumer with enhanced fan-out. Set the Lambda function that processes the logs as the consumer.
- B. Increase the ParallelizationFactor setting in the Lambda event source mapping.
- C. Configure reserved concurrency for the Lambda function that processes the logs.
- D. Increase the batch size in the Kinesis data stream.
- E. Turn off the ReportBatchItemFailures setting in the Lambda event source mapping.
- F. Increase the number of shards in the Kinesis data stream.

> **Suggested Answer:** ABF
> **Community Vote:** ABF (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**GripZA** (Sat 17 Aug 2024 09:29) - *Upvotes: 2*
A: Kinesis Enhanced fan-out is an Amazon Kinesis Data Streams feature that enables consumers to receive records from a data stream with dedicated throughput of up to 2 MB of data per second per shard. A consumer that uses enhanced fan-out doesn't have to contend with other consumers that are receiving data from the stream.

B: Reserved concurrency – This represents the maximum number of concurrent instances allocated to your function. When a function has reserved concurrency, no other function can use that concurrency. Reserved concurrency is useful for ensuring that your most critical functions always have enough concurrency to handle incoming requests.

F: The data capacity of your stream is a function of the number of shards that you specify for the stream. The total capacity of the stream is the sum of the capacities of its shards.

---

**seetpt** (Thu 02 May 2024 15:01) - *Upvotes: 2*
ABF for me

---

**c3518fc** (Fri 26 Apr 2024 20:48) - *Upvotes: 4*
https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html

---

**Ola2234** (Sat 06 Apr 2024 11:08) - *Upvotes: 1*
ABF or ACF

---

**WhyIronMan** (Sat 30 Mar 2024 11:45) - *Upvotes: 3*
A,B,F,
https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html

---

**Seoyong** (Wed 27 Mar 2024 09:38) - *Upvotes: 4*
https://aws.amazon.com/about-aws/whats-new/2019/11/aws-lambda-supports-parallelization-factor-for-kinesis-and-dynamodb-event-sources/

---


<br/>

## Question 218

*Date: March 27, 2024, 6:09 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company operates sensitive workloads across the AWS accounts that are in the company's organization in AWS Organizations. The company uses an IP address range to delegate IP addresses for Amazon VPC CIDR blocks and all non-cloud hardware.

The company needs a solution that prevents principals that are outside the company’s IP address range from performing AWS actions in the organization's accounts.

Which solution will meet these requirements?

**Options:**
- A. Configure AWS Firewall Manager for the organization. Create an AWS Network Firewall policy that allows only source traffic from the company's IP address range. Set the policy scope to all accounts in the organization.
- B. In Organizations, create an SCP that denies source IP addresses that are outside of the company’s IP address range. Attach the SCP to the organization's root.
- C. Configure Amazon GuardDuty for the organization. Create a GuardDuty trusted IP address list for the company's IP range. Activate the trusted IP list for the organization.
- D. In Organizations, create an SCP that allows source IP addresses that are inside of the company’s IP address range. Attach the SCP to the organization's root.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**WhyIronMan** (Mon 30 Sep 2024 10:39) - *Upvotes: 5*
B
https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws_deny-ip.html

---

**seetpt** (Sat 02 Nov 2024 16:01) - *Upvotes: 3*
B 100%

---

**c3518fc** (Sat 26 Oct 2024 20:54) - *Upvotes: 4*
https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws_deny-ip.html

---

**dkp** (Sun 13 Oct 2024 09:29) - *Upvotes: 4*
answer b
uses an SCP within AWS Organizations to deny source IP addresses that are outside of the company’s IP address range, providing a centralized and organization-wide control over AWS actions based on source IP addresses for all accounts and resources within the organization.

---

**ogerber** (Fri 27 Sep 2024 17:09) - *Upvotes: 2*
its B, 100%

---


<br/>

## Question 219

*Date: March 27, 2024, 6:14 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company deploys an application in two AWS Regions. The application currently uses an Amazon S3 bucket in the primary Region to store data.

A DevOps engineer needs to ensure that the application is highly available in both Regions. The DevOps engineer has created a new S3 bucket in the secondary Region. All existing and new objects must be in both S3 buckets. The application must fail over between the Regions with no data loss.

Which combination of steps will meet these requirements with the MOST operational efficiency? (Choose three.)

**Options:**
- A. Create a new IAM role that allows the Amazon S3 and S3 Batch Operations service principals to assume the role that has the necessary permissions for S3 replication.
- B. Create a new IAM role that allows the AWS Batch service principal to assume the role that has the necessary permissions for S3 replication.
- C. Create an S3 Cross-Region Replication (CRR) rule on the source S3 bucket. Configure the rule to use the IAM role for Amazon S3 to replicate to the target S3 bucket.
- D. Create a two-way replication rule on the source S3 bucket. Configure the rule to use the IAM role for Amazon S3 to replicate to the target S3 bucket.
- E. Create an AWS Batch job that has an AWS Fargate orchestration type. Configure the job to use the IAM role for AWS Batch. Specify a Bash command to use the AWS CLI to synchronize the contents of the source S3 bucket and the target S3 bucket
- F. Create an operation in S3 Batch Operations to replicate the contents of the source S3 bucket to the target S3 bucket. Configure the operation to use the IAM role for Amazon S3.

> **Suggested Answer:** ADF
> **Community Vote:** ADF (66%), ACF (34%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**that1guy** (Fri 17 May 2024 14:34) - *Upvotes: 6*
ADF, "All existing and new objects must be in BOTH S3 buckets." this requires two-way replication.

---

**d9iceguy** (Mon 22 Jul 2024 06:55) - *Upvotes: 5*
Note: Application deployed to both regions, bi-directional replication will be required

---

**ryuhei** (Sun 17 Aug 2025 11:30) - *Upvotes: 1*
Answer is A、C、F

---

**nickp84** (Mon 19 May 2025 08:45) - *Upvotes: 2*
D. Two-way replication rule: S3 does not support native two-way replication. You must configure two one-way CRR rules, one in each direction, if needed.

---

**VerRi** (Thu 07 Nov 2024 03:19) - *Upvotes: 2*
Poor wording. An active-active solution is recommended for HA, but bidirectional replication means CRR * 2. 'a two-way replication rule' is quite misleading

---

**aws_god** (Thu 12 Sep 2024 08:26) - *Upvotes: 2*
Not D because it states creating the two-way replication on the source bucket and you need to configure it on both to work:

When two-way replication is set up, a replication rule from the source bucket (DOC-EXAMPLE-BUCKET-1) to the bucket containing the replicas (DOC-EXAMPLE-BUCKET-2) is created. Then, a second replication rule from the bucket containing the replicas (DOC-EXAMPLE-BUCKET-2) to the source bucket (DOC-EXAMPLE-BUCKET-1) is created.

---

**Trex247** (Fri 16 Aug 2024 23:47) - *Upvotes: 3*
I think it's ADF check out this:
https://docs.aws.amazon.com/AmazonS3/latest/userguide/mrap-create-two-way-replication-rules.html

---

**everydaysmile** (Tue 13 Aug 2024 08:00) - *Upvotes: 2*
Two-way replication is possible using CRR.

"Replication is configured via rules. There is no rule for bi-directional replication. You will however setup a rule to replicate from the S3 bucket in the east AWS region to the west bucket, and you will setup a second rule to replicate going the opposite direction. These two rules will enable bi-directional replication across AWS regions."

- https://catalog.workshops.aws/well-architected-reliability/en-US/4-failure-management/1-backup/20-bidirectional-replication-for-s3/2-configure-replication

---

**hzaki** (Thu 01 Aug 2024 07:12) - *Upvotes: 1*
there is nothing called (Create a two-way replication rule on the source S3 bucket), the two-way replication is configured separately in each region per each bucket, that's why option D is incorrect.

---

**CHRIS12722222** (Sun 22 Dec 2024 16:38) - *Upvotes: 1*
There is !

https://docs.aws.amazon.com/AmazonS3/latest/userguide/mrap-create-two-way-replication-rules.html#:~:text=page%2C%20choose%20the-,Replicate%20objects%20among%20all%20specified%20buckets,-template.%20The%20Replicate

---


<br/>

## Question 220

*Date: March 27, 2024, 6:17 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses an organization in AWS Organizations to manage multiple AWS accounts. The company needs an automated process across all AWS accounts to isolate any compromised Amazon EC2 instances when the instances receive a specific tag.

Which combination of steps will meet these requirements? (Choose two.)

**Options:**
- A. Use AWS CloudFormation StackSets to deploy the CloudFormation stacks in all AWS accounts.
- B. Create an SCP that has a Deny statement for the ec2:* action with a condition of "aws:RequestTag/isolation": false.
- C. Attach the SCP to the root of the organization.
- D. Create an AWS CloudFormation template that creates an EC2 instance role that has no IAM policies attached. Configure the template to have a security group that has an explicit Deny rule on all traffic. Use the CloudFormation template to create an AWS Lambda function that attaches the IAM role to instances. Configure the Lambda function to add a network ACL. Set up an Amazon EventBridge rule to invoke the Lambda function when a specific tag is applied to a compromised EC2 instance.
- E. Create an AWS CloudFormation template that creates an EC2 instance role that has no IAM policies attached. Configure the template to have a security group that has no inbound rules or outbound rules. Use the CloudFormation template to create an AWS Lambda function that attaches the IAM role to instances. Configure the Lambda function to replace any existing security groups with the new security group. Set up an Amazon EventBridge rule to invoke the Lambda function when a specific tag is applied to a compromised EC2 instance.

> **Suggested Answer:** AE
> **Community Vote:** AE (84%), BC (16%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Jay_2pt0_1** (Mon 13 May 2024 10:23) - *Upvotes: 7*
What a weirdly worded question. I tend to agree with A & E. We need to isolate an EC2 that has a certain tag.

---

**Jordarlu** (Wed 09 Oct 2024 18:17) - *Upvotes: 2*
The B + C means no actions allowed on the tagged EC2 for all accounts in Organizations, but the asking was the needs of the isolation(implying the network isolation) on the tagged EC2; hence, A + E is a good option here..

---

**jamesf** (Tue 30 Jul 2024 14:24) - *Upvotes: 3*
I go for AE
isolating the instance should be mean block traffic

---

**trungtd** (Thu 11 Jul 2024 12:47) - *Upvotes: 4*
This CloudFormation template creates the necessary resources:

An EC2 instance role with no IAM policies, ensuring the instance cannot perform any actions.
A security group with no inbound or outbound rules, effectively isolating the instance from all network traffic.
A Lambda function that will be triggered by an EventBridge rule when a specific tag is applied to an EC2 instance. This function will attach the isolated security group to the compromised instance, ensuring it is isolated from any network communication.
Combining these steps will provide an automated and consistent approach to isolate compromised EC2 instances across all AWS accounts in the organization.

---

**xdkonorek2** (Sat 22 Jun 2024 21:14) - *Upvotes: 3*
BD is wrong
isolating the instance doesn't mean "don't touch it" with aws actions but to block traffic from and to it

---

**seetpt** (Thu 02 May 2024 15:06) - *Upvotes: 1*
BC for me

---

**vn_thanhtung** (Sat 25 May 2024 10:55) - *Upvotes: 3*
so funny, how to isolate incoming traffic. B,C means deny action with EC2

---

**vn_thanhtung** (Sat 25 May 2024 10:55) - *Upvotes: 2*
Answer is A, E

---

**seetpt** (Thu 02 May 2024 15:06) - *Upvotes: 1*
BC for me

---

**dkp** (Sat 13 Apr 2024 09:53) - *Upvotes: 4*
ill go with AE

---


<br/>

## Question 221

*Date: March 27, 2024, 10:10 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company manages multiple AWS accounts by using AWS Organizations with OUs for the different business divisions. The company is updating their corporate network to use new IP address ranges. The company has 10 Amazon S3 buckets in different AWS accounts. The S3 buckets store reports for the different divisions. The S3 bucket configurations allow only private corporate network IP addresses to access the S3 buckets.

A DevOps engineer needs to change the range of IP addresses that have permission to access the contents of the S3 buckets. The DevOps engineer also needs to revoke the permissions of two OUs in the company.

Which solution will meet these requirements?

**Options:**
- A. Create a new SCP that has two statements, one that allows access to the new range of IP addresses for all the S3 buckets and one that denies access to the old range of IP addresses for all the S3 buckets. Set a permissions boundary for the OrganizationAccountAccessRole role in the two OUs to deny access to the S3 buckets.
- B. Create a new SCP that has a statement that allows only the new range of IP addresses to access the S3 buckets. Create another SCP that denies access to the S3 buckets. Attach the second SCP to the two OUs.
- C. On all the S3 buckets, configure resource-based policies that allow only the new range of IP addresses to access the S3 buckets. Create a new SCP that denies access to the S3 buckets. Attach the SCP to the two OUs.
- D. On all the S3 buckets, configure resource-based policies that allow only the new range of IP addresses to access the S3 buckets. Set a permissions boundary for the OrganizationAccountAccessRole role in the two OUs to deny access to the S3 buckets.

> **Suggested Answer:** C
> **Community Vote:** C (94%), 6%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Sun 30 Mar 2025 15:29) - *Upvotes: 1*
On all the S3 buckets, configure resource-based policies that allow only the new range of IP addresses to access the S3 buckets. Set a permissions boundary for the OrganizationAccountAccessRole role in the two OUs to deny access to the S3 buckets.

This solution meets the requirements most effectively:

The resource-based S3 bucket policies ensure that only the new IP address ranges are allowed access, effectively controlling access at the network level.
By setting a permissions boundary on the OrganizationAccountAccessRole role, the OUs' permissions to the S3 buckets can be explicitly controlled and revoked, ensuring that only the appropriate accounts have access.

---

**seetpt** (Sat 02 Nov 2024 16:07) - *Upvotes: 3*
C for me

---

**dkp** (Sun 13 Oct 2024 10:42) - *Upvotes: 4*
answer c

---

**Ola2234** (Sat 12 Oct 2024 07:27) - *Upvotes: 1*
C.
Use bucket policy to allow or deny access to a range of IP addresses or VPC endpoints to an S3 resource. Restriction to OUs is best done using SCP.

---

**ogerber** (Fri 27 Sep 2024 17:26) - *Upvotes: 4*
C for me

---

**Seoyong** (Fri 27 Sep 2024 09:10) - *Upvotes: 4*
restrict access to S3 bucket using specific VPC endpoints or IP addresses:
https://repost.aws/knowledge-center/block-s3-traffic-vpc-ip

---


<br/>

## Question 222

*Date: March 27, 2024, 10:04 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has started using AWS across several teams. Each team has multiple accounts and unique security profiles. The company manages the accounts in an organization in AWS Organizations. Each account has its own configuration and security controls.

The company's DevOps team wants to use preventive and detective controls to govern all accounts. The DevOps team needs to ensure the security of accounts now and in the future as the company creates new accounts in the organization.

Which solution will meet these requirements?

**Options:**
- A. Use Organizations to create OUs that have appropriate SCPs attached for each team. Place team accounts in the appropriate OUs to apply security controls. Create any new team accounts in the appropriate OUs.
- B. Create an AWS Control Tower landing zone. Configure OUs and appropriate controls in AWS Control Tower for the existing teams. Configure trusted access for AWS Control Tower. Enroll the existing accounts in the appropriate OUs that match the appropriate security policies for each team. Use AWS Control Tower to provision any new accounts.
- C. Create AWS CloudFormation stack sets in the organization's management account. Configure a stack set that deploys AWS Config with configuration rules and remediation actions for all controls to each account in the organization. Update the stack sets to deploy to new accounts as the accounts are created.
- D. Configure AWS Config to manage the AWS Config rules across all AWS accounts in the organization. Deploy conformance packs that provide AWS Config rules and remediation actions across the organization.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**c3518fc** (Sat 26 Oct 2024 07:27) - *Upvotes: 6*
A control is a high-level rule that provides ongoing governance for your overall AWS environment. It's expressed in plain language. AWS Control Tower implements preventive, detective, and proactive controls that help you govern your resources and monitor compliance across groups of AWS accounts. https://docs.aws.amazon.com/controltower/latest/controlreference/controls.html

---

**Seoyong** (Fri 27 Sep 2024 09:04) - *Upvotes: 5*
About controls in AWS Control Tower:
https://docs.aws.amazon.com/controltower/latest/userguide/controls.html

---

**dkp** (Sun 13 Oct 2024 11:04) - *Upvotes: 3*
answer B

---

**Ola2234** (Sat 12 Oct 2024 07:34) - *Upvotes: 2*
Option B.

Keyword: Preventive and detective controls to govern all accounts. This service is provided by guardrails as part of AWS Control Tower.

---

**WhyIronMan** (Mon 30 Sep 2024 09:49) - *Upvotes: 3*
https://docs.aws.amazon.com/controltower/latest/userguide/controls.html

---

**ogerber** (Fri 27 Sep 2024 17:29) - *Upvotes: 2*
its B for me

---


<br/>

## Question 223

*Date: June 27, 2024, 10:57 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses an AWS CodeCommit repository to store its source code and corresponding unit tests. The company has configured an AWS CodePipeline pipeline that includes an AWS CodeBuild project that runs when code is merged to the main branch of the repository.

The company wants the CodeBuild project to run the unit tests. If the unit tests pass, the CodeBuild project must tag the most recent commit.

How should the company configure the CodeBuild project to meet these requirements?

**Options:**
- A. Configure the CodeBuild project to use native Git to done the CodeCommit repository. Configure the project to run the unit tests. Configure the project to use native Git to create a tag and to push the Git tag to the repository if the code passes the unit tests.
- B. Configure the CodeBuild projed to use native Git to done the CodeCommit repository. Configure the project to run the unit tests. Configure the project to use AWS CLI commands to create a new repository tag in the repository if the code passes the unit tests.
- C. Configure the CodeBuild project to use AWS CLI commands to copy the code from the CodeCommit repository. Configure the project to run the unit tests. Configure the project to use AWS CLI commands to create a new Git tag in the repository if the code passes the unit tests.
- D. Configure the CodeBuild project to use AWS CLI commands to copy the code from the CodeCommit repository. Configure the project to run the unit tests. Configure the project to use AWS CLI commands to create a new repository tag in the repository if the code passes the unit tests.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Wed 31 Jul 2024 03:18) - *Upvotes: 2*
Option A effectively meets the requirements by using standard Git operations for cloning and tagging, ensuring an efficient and clear workflow for managing the repository in AWS CodePipeline.

---

**tgv** (Mon 15 Jul 2024 11:15) - *Upvotes: 2*
---> A

---

**syh_rapha** (Sun 14 Jul 2024 22:02) - *Upvotes: 2*
A
https://docs.aws.amazon.com/codecommit/latest/userguide/getting-started.html#getting-started-create-commit

---

**trungtd** (Thu 11 Jul 2024 15:55) - *Upvotes: 3*
Using Native Git: By configuring CodeBuild to use native Git, you ensure the repository is cloned in a way that supports all Git operations, including tagging and pushing changes.

Running Unit Tests: CodeBuild can be set up to run the unit tests using the build specification file (buildspec.yml), ensuring that the tests are executed before any further actions are taken.

Creating and Pushing Tags:
post_build:
commands:
- echo Unit tests passed, tagging commit...
- git tag -a v1.0.0 -m "Tagging commit after successful unit tests"
- git push origin v1.0.0

---

**KaranNishad** (Thu 27 Jun 2024 19:43) - *Upvotes: 2*
A is the Answer.

---

**ihustle** (Thu 27 Jun 2024 10:57) - *Upvotes: 2*
A is the Answer.

---


<br/>

## Question 224

*Date: June 27, 2024, 7:46 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer manages a company's Amazon Elastic Container Service (Amazon ECS) cluster. The cluster runs on several Amazon EC2 instances that are in an Auto Scaling group. The DevOps engineer must implement a solution that logs and reviews all stopped tasks for errors.

Which solution will meet these requirements?

**Options:**
- A. Create an Amazon EventBridge rule to capture task state changes. Send the event to Amazon CloudWatch Logs. Use CloudWatch Logs Insights to investigate stopped tasks.
- B. Configure tasks to write log data in the embedded metric format. Store the logs in Amazon CloudWatch Logs. Monitor the ContainerInstanceCount metric for changes.
- C. Configure the EC2 instances to store logs in Amazon CloudWatch Logs. Create a CloudWatch Contributor Insights rule that uses the EC2 instance log data. Use the Contributor Insights rule to investigate stopped tasks.
- D. Configure an EC2 Auto Scaling lifecycle hook for the EC2_INSTANCE_TERMINATING scale-in event. Write the SystemEventLog file to Amazon S3. Use Amazon Athena to query the log file for errors.

> **Suggested Answer:** A
> **Community Vote:** A (91%), 9%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**6ef9a08** (Tue 02 Jul 2024 09:30) - *Upvotes: 7*
By using Amazon EventBridge to capture ECS task state changes and sending these events to CloudWatch Logs, combined with the analytical capabilities of CloudWatch Logs Insights, option A provides a comprehensive and straightforward solution for logging and investigating stopped tasks for errors.

---

**teo2157** (Tue 14 Jan 2025 07:38) - *Upvotes: 1*
Going for A as C is referring to configure the EC2 instances to store logs in Amazon CloudWatch Logs while the question is referring to ECS tasks

---

**tgv** (Mon 15 Jul 2024 11:15) - *Upvotes: 1*
---> A

---

**trungtd** (Thu 11 Jul 2024 16:00) - *Upvotes: 2*
Just A

---

**KaranNishad** (Thu 27 Jun 2024 19:46) - *Upvotes: 1*
CloudWatch Contributor Insights

---


<br/>

## Question 225

*Date: July 11, 2024, 4:07 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company wants to deploy a workload on several hundred Amazon EC2 instances. The company will provision the EC2 instances in an Auto Scaling group by using a launch template.

The workload will pull files from an Amazon S3 bucket, process the data, and put the results into a different S3 bucket. The EC2 instances must have least-privilege permissions and must use temporary security credentials.

Which combination of steps will meet these requirements? (Choose two.)

**Options:**
- A. Create an IAM role that has the appropriate permissions for S3 buckets Add the IAM role to an instance profile.
- B. Update the launch template to include the IAM instance profile.
- C. Create an IAM user that has the appropriate permissions for Amazon S3 Generate a secret key and token.
- D. Create a trust anchor and profile Attach the IAM role to the profile.
- E. Update the launch template Modify the user data to use the new secret key and token.

> **Suggested Answer:** AB
> **Community Vote:** AB (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**trungtd** (Thu 11 Jul 2024 16:07) - *Upvotes: 6*
A. This step ensures that the EC2 instances have the necessary permissions to access the S3 buckets. The IAM role should have policies attached that allow it to pull files from one S3 bucket and put results into another S3 bucket. By using an instance profile, the role can be associated with the EC2 instances.
B. This step ensures that the EC2 instances launched by the Auto Scaling group will automatically use the instance profile (and thus the IAM role) with the appropriate permissions.
C. This approach uses long-term credentials
D. The term "trust anchor" is more relevant to AWS IAM Identity Center (formerly AWS Single Sign-On) or AWS Organizations. It is not directly applicable to setting up permissions for EC2 instances via Auto Scaling.
E. Storing and using secret keys and tokens in user data scripts is insecure and not recommended.

---

**tgv** (Mon 15 Jul 2024 11:16) - *Upvotes: 3*
---> AB

---


<br/>

## Question 226

*Date: June 27, 2024, 7:49 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is using AWS CodeDeploy to automate software deployment. The deployment must meet these requirements:

• A number of instances must be available to serve traffic during the deployment. Traffic must be balanced across those instances, and the instances must automatically heal in the event of failure. • A new fleet of instances must be launched for deploying a new revision automatically, with no manual provisioning.
• Traffic must be rerouted to the new environment to half of the new instances at a time. The deployment should succeed if traffic is rerouted to at least half of the instances: otherwise, it should fail.
• Before routing traffic to the new fleet of instances, the temporary files generated during the deployment process must be deleted.
• At the end of a successful deployment, the original instances in the deployment group must be deleted immediately to reduce costs.

How can a DevOps engineer meet these requirements?

**Options:**
- A. Use an Application Load Balancer and an in-place deployment. Associate the Auto Scaling group with the deployment group. Use the Automatically copy Auto Scaling group option, and use CodeDeployDefault.OneAtAtime as the deployment configuration. Instruct AWS CodeDeploy to terminate the original instances in the deployment group, and use the AllowTraffic hook within appspec.yml to delete the temporary files.
- B. Use an Application Load Balancer and a blue/green deployment. Associate the Auto Scaling group and Application Load Balancer target group with the deployment group. Use the Automatically copy Auto Scaling group option, create a custom deployment configuration with minimum healthy hosts defined as 50%, and assign the configuration to the deployment group. Instruct AWS CodeDeploy to terminate the original instances in the deployment group, and use the BeforeBlockTraffic hook within appspec.yml to delete the temporary files.
- C. Use an Application Load Balancer and a blue/green deployment. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. Use the Automatically copy Auto Scaling group option, and use CodeDeployDefault.HalfAtAtime as the deployment configuration. Instruct AWS CodeDeploy to terminate the original instances in the deployment group, and use the BeforeAllowTraffic hook within appspec.yml to delete the temporary files.
- D. Use an Application Load Balancer and an in-place deployment. Associate the Auto Scaling group and Application Load Balancer target group with the deployment group. Use the Automatically copy Auto Scaling group option, and use CodeDeployDefault.AllatOnce as a deployment configuration. Instruct AWS CodeDeploy to terminate the original instances in the deployment group, and use the BlockTraffic hook within appspec.yml to delete the temporary files.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Wed 31 Jul 2024 03:40) - *Upvotes: 3*
Option C is the best choice as it meets all the given requirements by utilizing a blue/green deployment strategy, correct traffic routing, proper temporary file cleanup timing, and automatic instance termination to manage costs.

Option B is wrong due to:
- Allows for custom deployment configuration, ensuring at least 50% of the instances must be healthy. However, "reroute to half" is somewhat ambiguous, as custom deployment with 50% only ensures half remain healthy but doesn't explicitly guarantee half reroute.
- Uses BeforeBlockTraffic hook, which is not the correct timing for cleaning up temporary files because it should occur after instance provisioning but before allowing traffic.

---

**tgv** (Mon 15 Jul 2024 11:17) - *Upvotes: 1*
---> C

---

**trungtd** (Fri 12 Jul 2024 01:56) - *Upvotes: 2*
Automatically Copy Auto Scaling Group:
This option allows launching a new fleet of instances for each deployment automatically.

CodeDeployDefault.HalfAtAtime Deployment Configuration:
This configuration meets the requirement of rerouting traffic to half of the new instances at a time. It ensures that the deployment succeeds if traffic is rerouted to at least half of the instances, otherwise, it fails.

---

**6ef9a08** (Tue 02 Jul 2024 09:44) - *Upvotes: 3*
B: blue/green deployment with CodeDeployDefault.HalfAtAtime

---


<br/>

## Question 227

*Date: June 27, 2024, 7:53 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company needs to adopt a multi-account strategy to deploy its applications and the associated CI/CD infrastructure. The company has created an organization in AWS Organizations that has all features enabled. The company has configured AWS Control Tower and has set up a landing zone.

The company needs to use AWS Control Tower controls (guardrails) in all AWS accounts in the organization. The company must create the accounts for a multi-environment application and must ensure that all accounts are configured to an initial baseline.

Which solution will meet these requirements with the LEAST operational overhead?

**Options:**
- A. Create an AWS Control Tower Account Factory Customization (AFC) blueprint that uses the baseline configuration. Use AWS Control Tower Account Factory to provision a dedicated AWS account for each environment and a CI/CD account by using the blueprint.
- B. Use AWS Control Tower Account Factory to provision a dedicated AWS account for each environment and a CI/CD account. Use AWS CloudFormation StackSets to apply the baseline configuration to the new accounts.
- C. Use Organizations to provision a multi-environment AWS account and a CI/CD account. In the Organizations management account, create an AWS Lambda function that assumes the Organizations access role to apply the baseline configuration to the new accounts.
- D. Use Organizations to provision a dedicated AWS account for each environment, an audit account, and a CI/CD account. Use AWS CloudFormation StackSets to apply the baseline configuration to the new accounts.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**limelight04** (Mon 26 Aug 2024 03:00) - *Upvotes: 1*
Answer is B
Use AWS Control Tower Account Factory to provision dedicated AWS accounts for each environment and a CI/CD account. Then, use AWS CloudFormation StackSets to apply the baseline configuration to the new accounts.
This approach simplifies account provisioning and ensures consistency across environments while minimizing manual effort.

---

**jamesf** (Wed 31 Jul 2024 03:44) - *Upvotes: 4*
keywords: AWS Control Tower Account Factory Customization (AFC)
Option A is the best choice as it meets all the requirements with the least operational overhead by leveraging AWS Control Tower’s Account Factory and Customization features. This option provides an automated, compliant, and consistent approach to account provisioning and baseline configuration.

---

**tgv** (Mon 15 Jul 2024 11:17) - *Upvotes: 1*
---> A

---

**trungtd** (Fri 12 Jul 2024 02:12) - *Upvotes: 1*
All of these options are possible. But A is the LEAST operational overhead

---

**KaranNishad** (Thu 27 Jun 2024 19:53) - *Upvotes: 2*
AWS Control Tower Account Factory Customization (AFC)

---


<br/>

## Question 228

*Date: July 5, 2024, 1:17 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps team has created a Custom Lambda rule in AWS Config. The rule monitors Amazon Elastic Container Repository (Amazon ECR) policy statements for ecr:* actions. When a noncompliant repository is detected, Amazon EventBridge uses Amazon Simple Notification Service (Amazon SNS) to route the notification to a security team.

When the custom AWS Config rule is evaluated, the AWS Lambda function fails to run.

Which solution will resolve the issue?

**Options:**
- A. Modify the Lambda function's resource policy to grant AWS Config permission to invoke the function.
- B. Modify the SNS topic policy to include configuration changes for EventBridge to publish to the SNS topic.
- C. Modify the Lambda function's execution role to include configuration changes for custom AWS Config rules.
- D. Modify all the ECR repository policies to grant AWS Config access to the necessary ECR API actions.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**youonebe** (Thu 26 Dec 2024 17:09) - *Upvotes: 1*
Bad wording "fails to run" which sounds like to "fails to execute", which here it actually means "failed to invoke"

---

**ThiagoCruzRJ** (Thu 29 Aug 2024 11:17) - *Upvotes: 2*
When you create a custom AWS Config rule that uses a Lambda function, AWS Config needs permission to invoke it. This is done by adding a resource-based policy to the Lambda function that explicitly permits AWS Config to invoke it. Without this permission, AWS Config cannot trigger the Lambda function, leading to the function failing to run.

---

**jamesf** (Wed 31 Jul 2024 03:47) - *Upvotes: 1*
Option A is the best choice to resolve the issue. By modifying the Lambda function's resource policy to grant AWS Config permission to invoke the function, we address the root cause of the invocation failure. This ensures that AWS Config can successfully execute the custom rule using the Lambda function.

---

**d9iceguy** (Mon 22 Jul 2024 08:29) - *Upvotes: 2*
Resource policy should allow Config invocation

---

**amehim** (Sun 21 Jul 2024 13:45) - *Upvotes: 2*
A. Modify the Lambda function's resource policy to grant AWS Config permission to invoke the function.
{
"Version": "2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Principal": {
"Service": "config.amazonaws.com"
},
"Action": "lambda:InvokeFunction",
"Resource": "arn:aws:lambda:region:account-id:function:function-name"
}
]
}

---

**tgv** (Mon 15 Jul 2024 11:17) - *Upvotes: 4*
---> A

---


<br/>

## Question 229

*Date: July 13, 2024, 11:50 p.m.
Disclaimers:
- ExamTopics website is not rel*

A developer is creating a proof of concept for a new software as a service (SaaS) application. The application is in a shared development AWS account that is part of an organization in AWS Organizations.

The developer needs to create service-linked IAM roles for the AWS services that are being considered for the proof of concept. The solution needs to give the developer the ability to create and configure the service-linked roles only.

Which solution will meet these requirements?

**Options:**
- A. Create an IAM user for the developer in the organization's management account. Configure a cross-account role in the development account for the developer to use. Limit the scope of the cross-account role to common services.
- B. Add the developer to an IAM group. Attach the PowerUserAccess managed policy to the IAM group. Enforce multi-factor authentication (MFA) on the user account.
- C. Add an SCP to the development account in Organizations. Configure the SCP with a Deny rule for iam:* to limit the developer's access.
- D. Create an IAM role that has the necessary IAM access to allow the developer to create policies and roles. Create and attach a permissions boundary to the role. Grant the developer access to assume the role.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**trungtd** (Sat 13 Jul 2024 23:50) - *Upvotes: 5*
A. This approach involves creating a user in the management account and setting up cross-account roles, which adds unnecessary complexity and potential security risks.
B. PowerUserAccess managed policy provides broad permissions that go beyond just creating and configuring service-linked roles. This approach does not meet the requirement to restrict the developer's capabilities specifically to service-linked role management.
C. SCPs are used to set permission guardrails at the organizational or account level, but they do not grant permissions. They are used to restrict actions, and configuring an SCP with a deny rule for iam:* would likely prevent the developer from performing necessary actions

D effectively meets the requirements

---

**tgv** (Mon 15 Jul 2024 11:18) - *Upvotes: 2*
---> D

---

**TEC1** (Sun 14 Jul 2024 07:41) - *Upvotes: 4*
D - is more granular since it provides the right balance of granting necessary permissions while maintaining security and following the principle of least privilege. It allows the developer to create and configure service-linked roles as needed for the proof of concept, while the permissions boundary ensures that they can't exceed their intended level of access.

---


<br/>

## Question 230

*Date: June 27, 2024, 8 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses AWS Organizations to manage its AWS accounts. The company wants its monitoring system to receive an alert when a root user logs in. The company also needs a dashboard to display any log activity that the root user generates.

Which combination of steps will meet these requirements? (Choose three.)

**Options:**
- A. Enable AWS Config with a multi-account aggregator. Configure log forwarding to Amazon CloudWatch Logs.
- B. Create an Amazon QuickSight dashboard that uses an Amazon CloudWatch Logs query.
- C. Create an Amazon CloudWatch Logs metric filter to match root user login events. Configure a CloudWatch alarm and an Amazon Simple Notification Service (Amazon SNS) topic to send alerts to the company's monitoring system.
- D. Create an Amazon CloudWatch Logs subscription filter to match root user login events. Configure the filter to forward events to an Amazon Simple Notification Service (Amazon SNS) topic. Configure the SNS topic to send alerts to the company's monitoring system.
- E. Create an AWS CloudTrail organization trail. Configure the organization trail to send events to Amazon CloudWatch Logs.
- F. Create an Amazon CloudWatch dashboard that uses a CloudWatch Logs Insights query.

> **Suggested Answer:** CEF
> **Community Vote:** CEF (95%), 5%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**KaranNishad** (Thu 27 Jun 2024 20:00) - *Upvotes: 6*
Correct answer.

---

**asimohat** (Sun 09 Mar 2025 09:42) - *Upvotes: 1*
I think “C” is better than “D”.
The filter pattern can identify the root user's login event and include the content of that event in the SNS.
With “C” notification, we only know that the root user logged in.

---

**jamesf** (Wed 31 Jul 2024 03:55) - *Upvotes: 4*
Option E: Use CloudTrail to capture and forward root user activities.
Option C: Set up metric filters and alarms to alert on root user login events.
Option F: Create a CloudWatch dashboard for visualizing root user activities.

Additional Note:
CloudWatch Logs Subscription Filter:
- Real-time processing of log events, but typically used for streaming log data to other services like AWS Lambda or Elasticsearch.
- Not necessary for the specific task of alerting on root user login events.

AWS Config is not directly relevant to capturing and forwarding root user login events to CloudWatch Logs.

---

**tgv** (Mon 15 Jul 2024 11:20) - *Upvotes: 2*
---> CEF

---

**TEC1** (Sun 14 Jul 2024 07:48) - *Upvotes: 4*
E- AWS CloudTrail will log all activities, including root user logins, across all accounts in the organisation. Sending these logs to CloudWatch Logs enables further processing and analysis.

C- Creating a metric filter to detect root user login events will allow you to trigger a CloudWatch alarm. The alarm can then send notifications via SNS to the company's monitoring system, ensuring real-time alerts for root user logins.

F- Using CloudWatch Logs Insights, you can create queries to extract and visualise log data related to root user activity. This data can be displayed on a CloudWatch dashboard, providing a centralised view of root user actions.

---

**trungtd** (Sat 13 Jul 2024 23:56) - *Upvotes: 4*
E first, then C, and the last is F

E ensures that all events, including root user login events, are captured across all accounts in the organization. By sending these events to CloudWatch Logs, you centralize the logging data, making it accessible for further processing.
C creating a metric filter in CloudWatch Logs to detect specific patterns in the log data, such as root user login events.
F creating a CloudWatch dashboard that utilizes CloudWatch Logs Insights to query and visualize the log data. This dashboard can be used to display detailed information about root user login activity and other relevant log events.

---


<br/>

## Question 231

*Date: June 27, 2024, 8:12 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses AWS Organizations to manage its AWS accounts. A DevOps engineer must ensure that all users who access the AWS Management Console are authenticated through the company’s corporate identity provider (IdP).

Which combination of steps will meet these requirements? (Choose two.)

**Options:**
- A. Use Amazon GuardDuty with a delegated administrator account Use GuardDuty to enforce denial of IAM user logins.
- B. Use AWS IAM Identity Center to configure identity federation with SAML 2.0.
- C. Create a permissions boundary in AWS IAM Identity Center to deny password logins for IAM users.
- D. Create IAM groups in the Organizations management account to apply consistent permissions for all IAM users.
- E. Create an SCP in Organizations to deny password creation for IAM users.

> **Suggested Answer:** BE
> **Community Vote:** BE (85%), BC (15%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**limelight04** (Mon 26 Aug 2024 03:23) - *Upvotes: 2*
Use AWS IAM Identity Center to configure identity federation with SAML 2.0:
Configure SAML-based federation between your corporate IdP and AWS IAM.
This allows users to authenticate via your corporate identity provider when accessing the AWS Management Console.

Create a permissions boundary in AWS IAM Identity Center:
Set up a permissions boundary to deny password logins for IAM users.
This ensures that users must authenticate through the corporate IdP rather than using IAM user credentials.

---

**jamesf** (Wed 31 Jul 2024 04:01) - *Upvotes: 4*
Option B: Configure identity federation with SAML 2.0 using AWS IAM Identity Center.
Option E: Implement an SCP to deny password creation for IAM users, enforcing IdP authentication.

Incorrect for C - Permissions Boundaries
- Permissions boundaries in AWS IAM Identity Center define the maximum permissions an IAM entity can have but are not used to control login methods or deny password logins.
- Permissions boundaries do not restrict authentication methods or enforce federation.
- Permissions boundaries are not applicable for denying IAM user logins.

---

**tgv** (Mon 15 Jul 2024 11:21) - *Upvotes: 3*
---> BE

---

**trungtd** (Sun 14 Jul 2024 00:01) - *Upvotes: 3*
of course B.
E enforce that users cannot log in directly with IAM credentials. Instead, they must use the SSO setup provided by AWS IAM Identity Center, ensuring compliance with the requirement to authenticate through the corporate IdP.

---

**KaranNishad** (Thu 27 Jun 2024 20:12) - *Upvotes: 4*
BE is answer
{
"Version": "2012-10-17",
"Statement": [
{
"Effect": "Deny",
"Action": [
"iam:CreateLoginProfile",
"iam:UpdateLoginProfile"
],
"Resource": "*"
}
]
}

---


<br/>

## Question 232

*Date: July 5, 2024, 4:53 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has deployed a new platform that runs on Amazon Elastic Kubernetes Service (Amazon EKS). The new platform hosts web applications that users frequently update. The application developers build the Docker images for the applications and deploy the Docker images manually to the platform.

The platform usage has increased to more than 500 users every day. Frequent updates, building the updated Docker images for the applications, and deploying the Docker images on the platform manually have all become difficult to manage.

The company needs to receive an Amazon Simple Notification Service (Amazon SNS) notification if Docker image scanning returns any HIGH or CRITICAL findings for operating system or programming language package vulnerabilities.

Which combination of steps will meet these requirements? (Choose two.)

**Options:**
- A. Create an AWS CodeCommit repository to store the Dockerfile and Kubernetes deployment files. Create a pipeline in AWS CodePipeline. Use an Amazon S3 event to invoke the pipeline when a newer version of the Dockerfile is committed. Add a step to the pipeline to initiate the AWS CodeBuild project.
- B. Create an AWS CodeCommit repository to store the Dockerfile and Kubernetes deployment files. Create a pipeline in AWS CodePipeline. Use an Amazon EventBridge event to invoke the pipeline when a newer version of the Dockerfile is committed. Add a step to the pipeline to initiate the AWS CodeBuild project.
- C. Create an AWS CodeBuild project that builds the Docker images and stores the Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository. Turn on basic scanning for the ECR repository. Create an Amazon EventBridge rule that monitors Amazon GuardDuty events. Configure the EventBridge rule to send an event to an SNS topic when the finding-severity-counts parameter is more than 0 at a CRITICAL or HIGH level.
- D. Create an AWS CodeBuild project that builds the Docker images and stores the Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository. Turn on enhanced scanning for the ECR repository. Create an Amazon EventBridge rule that monitors ECR image scan events. Configure the EventBridge rule to send an event to an SNS topic when the finding-severity-counts parameter is more than 0 at a CRITICAL or HIGH level.
- E. Create an AWS CodeBuild project that scans the Dockerfile. Configure the project to build the Docker images and store the Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository if the scan is successful. Configure an SNS topic to provide notification if the scan returns any vulnerabilities.

> **Suggested Answer:** BD
> **Community Vote:** BD (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**trungtd** (Sun 14 Jul 2024 00:15) - *Upvotes: 5*
B sets up a CI/CD pipeline with AWS CodePipeline triggered by changes in the AWS CodeCommit repository. Using Amazon EventBridge ensures that the pipeline is invoked whenever there is a new commit, automating the build and deployment process.

D ensures that Docker images are built and pushed to ECR, where enhanced scanning is enabled. Enhanced scanning provides detailed vulnerability information. An EventBridge rule is configured to monitor scan events and trigger notifications via SNS when HIGH or CRITICAL vulnerabilities are found.

---

**limelight04** (Mon 26 Aug 2024 03:29) - *Upvotes: 2*
The answer is BD

---

**jamesf** (Wed 31 Jul 2024 04:08) - *Upvotes: 4*
Option B:
- AWS CodeCommit repository to store the Dockerfile and Kubernetes deployment files.
- Amazon EventBridge event to invoke the pipeline when a newer version of the Dockerfile is committed.

Option D:
- AWS CodeBuild project that builds the Docker images and stores the Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository.
- enhanced scanning for the ECR repository.

---

**tgv** (Mon 15 Jul 2024 11:23) - *Upvotes: 2*
---> BD

---

**inturist** (Mon 08 Jul 2024 18:06) - *Upvotes: 3*
Agree with B,D

---

**tgv** (Sun 07 Jul 2024 20:10) - *Upvotes: 2*
--> B D

---


<br/>

## Question 233

*Date: July 5, 2024, 4:23 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company groups its AWS accounts in OUs in an organization in AWS Organizations. The company has deployed a set of Amazon API Gateway APIs in one of the Organizations accounts. The APIs are bound to the account's VPC and have no existing authentication mechanism. Only principals in a specific OU can have permissions to invoke the APIs.

The company applies the following policy to the API Gateway interface VPC endpoint:



The company also updates the API Gateway resource policies to deny invocations that do not come through the interface VPC endpoint. After the updates, the following error message appears during attempts to use the interface VPC endpoint URL to invoke an API: "User: anonymous is not authorized."

Which combination of steps will solve this problem? (Choose two.)

**Options:**
- A. Enable IAM authentication on all API methods by setting AWS JAM as the authorization method.
- B. Create a token-based AWS Lambda authorizer that passes the caller's identity in a bearer token.
- C. Create a request parameter-based AWS Lambda authorizer that passes the caller's identity in a combination of headers, query string parameters, stage variables, and $cortext variables.
- D. Use Amazon Cognito user pools as the authorizer to control access to the API.
- E. Verify the identity of the requester by using Signature Version 4 to sign client requests by using AWS credentials.

> **Suggested Answer:** AE
> **Community Vote:** AE (95%), 5%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Wed 31 Jul 2024 04:14) - *Upvotes: 6*
Hope is Typo for the Option A, AWS JAM = AWS IAM

Option A. Enable IAM authentication on all API methods by setting AWS IAM as the authorization method.
- This ensures that all requests to the API must be authenticated using IAM credentials, directly addressing the anonymous access issue.

Option E. Verify the identity of the requester by using Signature Version 4 to sign client requests by using AWS credentials.
- By using AWS Signature Version 4, requests are authenticated, ensuring they are authorized according to IAM policies linked to the specific Organizational Unit.

---

**d9iceguy** (Mon 22 Jul 2024 13:20) - *Upvotes: 5*
JAM= IAM

---

**youonebe** (Thu 26 Dec 2024 17:33) - *Upvotes: 3*
This is for requests from Interface VPC endpoints, which means all principals are internal and have aws identities.
BCD are all for external request control in general.

---

**limelight04** (Mon 26 Aug 2024 03:45) - *Upvotes: 1*
Option A
Enable IAM authentication on all API methods:
Set AWS IAM as the authorization method for all API methods.
This ensures that authentication is required for invoking the APIs1.

Option B
Create a token-based AWS Lambda authorizer:
Implement a custom Lambda authorizer that validates bearer tokens.
Pass the caller’s identity in the token to authorize API requests

---

**GripZA** (Mon 19 Aug 2024 14:58) - *Upvotes: 4*
You can enable IAM authorization for HTTP API routes. When IAM authorization is enabled, clients must use Signature Version 4 (SigV4) to sign their requests with AWS credentials. API Gateway invokes your API route only if the client has execute-api permission for the route.

---

**tgv** (Mon 15 Jul 2024 11:38) - *Upvotes: 3*
---> A E (assuming there's a typo in AWS JAM)
If there's no typo in AWS JAM, I'd go for B & E

---

**komorebi** (Sat 13 Jul 2024 01:14) - *Upvotes: 1*
Anser:B,E

---


<br/>

## Question 234

*Date: July 14, 2024, 12:53 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company wants to decrease the time it takes to develop new features. The company uses AWS CodeBuild and AWS CodeDeploy to build and deploy its applications. The company uses AWS CodePipeline to deploy each microservice with its own CI/CD pipeline.

The company needs more visibility into the average time between the release of new features and the average time to recover after a failed deployment.

Which solution will provide this visibility with the LEAST configuration effort?

**Options:**
- A. Program an AWS Lambda function that creates Amazon CloudWatch custom metrics with information about successful runs and failed runs for each pipeline. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. Use the metrics to build a CloudWatch dashboard.
- B. Program an AWS Lambda function that creates Amazon CloudWatch custom metrics with information about successful runs and failed runs for each pipeline. Create an Amazon EventBridge rule to invoke the Lambda function after every successful run and after every failed run. Use the metrics to build a CloudWatch dashboard.
- C. Program an AWS Lambda function that writes information about successful runs and failed runs to Amazon DynamoDB. Create an Amazon EventBridge rule to invoke the Lambda function after every successful run and after every failed run. Build an Amazon QuickSight dashboard to show the information from DynamoDB.
- D. Program an AWS Lambda function that writes information about successful runs and failed runs to Amazon DynamoDB. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. Build an Amazon QuickSight dashboard to show the information from DynamoDB.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**limelight04** (Mon 26 Aug 2024 03:49) - *Upvotes: 3*
B is the correct answer

---

**jamesf** (Wed 31 Jul 2024 04:41) - *Upvotes: 3*
B is most simple and direct with LEAST configuration effort.

---

**tgv** (Mon 15 Jul 2024 11:41) - *Upvotes: 2*
---> B

---

**trungtd** (Sun 14 Jul 2024 00:53) - *Upvotes: 4*
A. Invoking the Lambda function every 5 minutes is less efficient compared to event-driven invocation
B. provides the needed visibility with minimal configuration effort
C & D. Using DynamoDB and QuickSight involves more configuration

---


<br/>

## Question 235

*Date: July 5, 2024, 4:09 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has developed a static website hosted on an Amazon S3 bucket. The website is deployed using AWS CloudFormation. The CloudFormation template defines an S3 bucket and a custom resource that copies content into the bucket from a source location.

The company has decided that it needs to move the website to a new location, so the existing CloudFormation stack must be deleted and re-created. However, CloudFormation reports that the stack could not be deleted cleanly.

What is the MOST likely cause and how can the DevOps engineer mitigate this problem for this and future versions of the website?

**Options:**
- A. Deletion has failed because the S3 bucket has an active website configuration. Modify the CloudFormation template to remove the WebsiteConfiguration property from the S3 bucket resource.
- B. Deletion has failed because the S3 bucket is not empty. Modify the custom resource's AWS Lambda function code to recursively empty the bucket when RequestType is Delete.
- C. Deletion has failed because the custom resource does not define a deletion policy. Add a DeletionPolicy property to the custom resource definition with a value of RemoveOnDeletion.
- D. Deletion has failed because the S3 bucket is not empty. Modify the S3 bucket resource in the CloudFormation template to add a DeletionPolicy property with a value of Empty.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**youonebe** (Thu 26 Dec 2024 17:45) - *Upvotes: 2*
You can only delete empty buckets. Deletion fails for buckets that have contents.

https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-s3-bucket.html

---

**GripZA** (Mon 19 Aug 2024 15:11) - *Upvotes: 3*
By default cloudformation can't delete an S3 bucket that's not empty. If the bucket still contains objects when the stack deletion is attempted, the stack deletion will fail.

Although the Q doesn't specify that the custom resource uses Lambda. I think it's safe to assume here that since a custom resource is responsible for copying content into the bucket, it can also be used to handle the cleanup process.

---

**tgv** (Mon 15 Jul 2024 11:43) - *Upvotes: 2*
---> B

---

**trungtd** (Sun 14 Jul 2024 00:55) - *Upvotes: 3*
of course B

---

**inturist** (Fri 12 Jul 2024 11:59) - *Upvotes: 3*
Agree B

---

**siheom** (Fri 12 Jul 2024 03:17) - *Upvotes: 3*
Definitely B

---


<br/>

## Question 236

*Date: July 6, 2024, 10:32 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses Amazon EC2 as its primary compute platform. A DevOps team wants to audit the company's EC2 instances to check whether any prohibited applications have been installed on the EC2 instances.

Which solution will meet these requirements with the MOST operational efficiency?

**Options:**
- A. Configure AWS Systems Manager on each instance. Use AWS Systems Manager Inventory. Use Systems Manager resource data sync to synchronize and store findings in an Amazon S3 bucket. Create an AWS Lambda function that runs when new objects are added to the S3 bucket. Configure the Lambda function to identify prohibited applications.
- B. Configure AWS Systems Manager on each instance. Use Systems Manager Inventory Create AWS Config rules that monitor changes from Systems Manager Inventory to identify prohibited applications.
- C. Configure AWS Systems Manager on each instance. Use Systems Manager Inventory. Filter a trail in AWS CloudTrail for Systems Manager Inventory events to identify prohibited applications.
- D. Designate Amazon CloudWatch Logs as the log destination for all application instances. Run an automated script across all instances to create an inventory of installed applications. Configure the script to forward the results to CloudWatch Logs. Create a CloudWatch alarm that uses filter patterns to search log data to identify prohibited applications.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Mon 05 Aug 2024 04:15) - *Upvotes: 5*
keywords: AWS Systems Manage, Systems Manager Inventory, AWS Config rule

---

**GripZA** (Mon 21 Apr 2025 17:59) - *Upvotes: 1*
AWS Systems Manager Inventory integrates with AWS Config to record inventory data for historical views, change tracking, or auditing. When you use AWS Config recording for systems inventory data you can enable scenarios such as tracking newly installed or removed software applications, assessing security risks, troubleshooting, and tracking license usage. Additionally, you can create AWS Config Rules to define compliance rules based on inventory data (such as, detecting a blacklisted application) and take remediation action (such as sending email notifications or running an AWS Lambda function to uninstall the application) automatically.

---

**limelight04** (Mon 26 Aug 2024 03:55) - *Upvotes: 3*
Option B: Configure AWS Systems Manager on each instance. Use Systems Manager Inventory and create AWS Config rules that monitor changes from Systems Manager Inventory to identify prohibited applications.
This approach leverages Systems Manager Inventory and AWS Config to efficiently track and identify prohibited applications while minimizing operational overhead.

---

**tgv** (Mon 15 Jul 2024 11:47) - *Upvotes: 2*
---> B

---

**getadroit** (Sat 06 Jul 2024 22:32) - *Upvotes: 4*
B
https://aws.amazon.com/blogs/mt/preventing-blacklisted-applications-with-aws-systems-manager-and-aws-config/

---


<br/>

## Question 237

*Date: July 5, 2024, 4:06 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an event-driven JavaScript application. The application uses decoupled AWS managed services that publish, consume, and route events. During application testing, events are not delivered to the target that is specified by an Amazon EventBridge rule.

A DevOps team must provide application testers with additional functionality to view, troubleshoot, and prevent the loss of events without redeployment of the application.

Which combination of steps should the DevOps team take to meet these requirements? (Choose three.)

**Options:**
- A. Launch AWS Device Farm with a standard test environment and project to run a specific build of the application.
- B. Create an Amazon S3 bucket. Enable AWS CloudTrail. Create a CloudTrail trail that specifies the S3 bucket as the storage location.
- C. Configure the EventBridge rule to use an Amazon Simple Queue Service (Amazon SQS) standard queue as a dead-letter queue.
- D. Configure the EventBridge rule to use an Amazon Simple Queue Service (Amazon SQS) FIFO queue as a dead-letter queue.
- E. Create a log group in Amazon CloudWatch Logs Specify the log group as an additional target of the EventBridge rule.
- F. Update the application code base to use the AWS X-Ray SDK tracing feature to instrument the code with support for the X-Amzn-Trace-Id header.

> **Suggested Answer:** BCE
> **Community Vote:** BCE (74%), CEF (21%), 5%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**RajAWSDevOps007** (Mon 16 Dec 2024 10:41) - *Upvotes: 3*
F is incorrect because it invites an app redeployment with SDK being used to change code base! So BCE are the correct options

---

**jamesf** (Wed 31 Jul 2024 06:45) - *Upvotes: 4*
Keywords: without redeployment of the application.
Option B: Enabling CloudTrail will allow testers to track event activities and interactions with AWS services, aiding in troubleshooting.
Option C: Using a standard SQS queue as a DLQ ensures that failed events are captured and can be analyzed or retried.
Option E: Adding CloudWatch Logs as a target provides immediate logging of event processing, aiding in real-time monitoring and troubleshooting.

---

**TEC1** (Mon 22 Jul 2024 20:47) - *Upvotes: 2*
I will go with CEF

---

**dalieba** (Thu 18 Jul 2024 23:40) - *Upvotes: 1*
CEF, X-ray enables you to debug distributed applications to troubleshoot the root cause of performance issues and errors

---

**noisonnoiton** (Wed 17 Jul 2024 08:04) - *Upvotes: 4*
without redeployment of the application.

B,C,E

---

**VerRi** (Thu 07 Nov 2024 05:22) - *Upvotes: 1*
GJ bro

---

**tgv** (Mon 15 Jul 2024 12:16) - *Upvotes: 1*
---> CEF
I thought E its not possible, but --> https://repost.aws/knowledge-center/cloudwatch-log-group-eventbridge

---

**tgv** (Thu 18 Jul 2024 12:32) - *Upvotes: 2*
good catch with "without redeployment of the application."
B, C, E

---

**trungtd** (Sun 14 Jul 2024 06:57) - *Upvotes: 1*
C allows you to capture events that could not be delivered to the specified target
E capture detailed logs of the events that are processed
F end-to-end tracing capabilities

B is wrong because while CloudTrail provides logging for AWS API calls, it is not specifically designed for capturing and troubleshooting event flows in EventBridge

---

**siheom** (Fri 12 Jul 2024 03:16) - *Upvotes: 3*
BCE...

---


<br/>

## Question 238

*Date: July 5, 2024, 3:47 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is migrating its container-based workloads to an AWS Organizations multi-account environment. The environment consists of application workload accounts that the company uses to deploy and run the containerized workloads. The company has also provisioned a shared services account for shared workloads in the organization.

The company must follow strict compliance regulations. All container images must receive security scanning before they are deployed to any environment. Images can be consumed by downstream deployment mechanisms after the images pass a scan with no critical vulnerabilities. Pre-scan and post-scan images must be isolated from one another so that a deployment can never use pre-scan images.

A DevOps engineer needs to create a strategy to centralize this process.

Which combination of steps will meet these requirements with the LEAST administrative overhead? (Choose two.)

**Options:**
- A. Create Amazon Elastic Container Registry (Amazon ECR) repositories in the shared services account: one repository for each pre-scan image and one repository for each post-scan image. Configure Amazon ECR image scanning to run on new image pushes to the pre-scan repositories. Use resource-based policies to grant the organization write access to the pre-scan repositories and read access to the post-scan repositories.
- B. Create pre-scan Amazon Elastic Container Registry (Amazon ECR) repositories in each account that publishes container images. Create repositories for post-scan images in the shared services account. Configure Amazon ECR image scanning to run on new image pushes to the pre-scan repositories. Use resource-based policies to grant the organization read access to the post-scan repositories.
- C. Configure image replication for each image from the image's pre-scan repository to the image's post-scan repository.
- D. Create a pipeline in AWS CodePipeline for each pre-scan repository. Create a source stage that runs when new images are pushed to the pre-scan repositories. Create a stage that uses AWS CodeBuild as the action provider. Write a buildspec.yaml definition that determines the image scanning status and pushes images without critical vulnerabilities to the post-scan repositories.
- E. Create an AWS Lambda function. Create an Amazon EventBridge rule that reacts to image scanning completed events and invokes the Lambda function. Write function code that determines the image scanning status and pushes images without critical vulnerabilities to the post-scan repositories.

> **Suggested Answer:** AE
> **Community Vote:** AE (58%), AD (42%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**trungtd** (Sun 14 Jul 2024 09:07) - *Upvotes: 5*
LEAST administrative overhead:
=> Should create ECR repositories in the shared services account => A
And should create only 1 Lambda function => E

D wrong because it involves creating and managing multiple pipelines, which increases administrative overhead significantly

---

**jojewi8143** (Sat 01 Feb 2025 12:53) - *Upvotes: 1*
AE because lambda

---

**aws_god** (Fri 13 Sep 2024 07:26) - *Upvotes: 4*
Lambda is not meant to work with Docker

---

**limelight04** (Mon 26 Aug 2024 04:12) - *Upvotes: 3*
AD gives the least administrative overhead

---

**auxwww** (Wed 07 Aug 2024 13:55) - *Upvotes: 2*
Why E is not optimal - https://stackoverflow.com/questions/51158595/build-and-push-docker-image-to-aws-ecr-using-lambda

---

**auxwww** (Wed 07 Aug 2024 13:52) - *Upvotes: 3*
Why not E - To push images to the post-scan repo, you need a custom lambda container to run docker pull and push commands which is more complicated than Option D

---

**jamesf** (Wed 31 Jul 2024 06:53) - *Upvotes: 4*
keywords: LEAST Administrative overhead

Option A centralizes the repository management in the shared services account, simplifying access control and configuration management. Pre-scan and post-scan repositories are clearly separated, ensuring that only post-scan images are deployed.

Option E uses event-driven automation to handle the scanning results and image promotion, reducing manual intervention and ensuring that only images that pass the security scan are moved to the post-scan repositories. This approach is efficient and minimizes administrative overhead compared to manually setting up pipelines or replication mechanisms.

---

**tgv** (Mon 15 Jul 2024 12:24) - *Upvotes: 2*
---> AE

---

**xdkonorek2** (Fri 05 Jul 2024 15:47) - *Upvotes: 4*
E > D for LEAST administrative overhead

---


<br/>

## Question 239

*Date: July 5, 2024, 3:45 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to deploy its web applications on containers. The web applications contain confidential data that cannot be decrypted without specific credentials.

A DevOps engineer has stored the credentials in AWS Secrets Manager. The secrets are encrypted by an AWS Key Management Service (AWS KMS) customer managed key. A Kubernetes service account for a third-party tool makes the secrets available to the applications. The service account assumes an IAM role that the company created to access the secrets.

The service account receives an Access Denied (403 Forbidden) error while trying to retrieve the secrets from Secrets Manager.

What is the root cause of this issue?

**Options:**
- A. The IAM role that is attached to the EKS cluster does not have access to retrieve the secrets from Secrets Manager.
- B. The key policy for the customer managed key does not allow the Kubernetes service account IAM role to use the key.
- C. The key policy for the customer managed key does not allow the EKS cluster IAM role to use the key.
- D. The IAM role that is assumed by the Kubernetes service account does not have permission to access the EKS cluster.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jojewi8143** (Sat 01 Feb 2025 12:54) - *Upvotes: 2*
B seems correct to me

---

**jamesf** (Wed 31 Jul 2024 06:59) - *Upvotes: 3*
When a service account in Amazon EKS tries to access secrets in AWS Secrets Manager, it does so by assuming an IAM role. The permissions required to access these secrets include:
- Secrets Manager permissions: The IAM role must have the necessary permissions to retrieve the secrets from AWS Secrets Manager.
- KMS key permissions: The IAM role must also have permissions to use the AWS KMS key that encrypts the secrets.

---

**jamesf** (Wed 31 Jul 2024 06:59) - *Upvotes: 2*
If the IAM role has the correct permissions to access Secrets Manager but still receives an "Access Denied" error, the issue is likely related to the KMS key policy. Specifically, the key policy needs to explicitly allow the IAM role to use the key for decrypting the secrets.

So, the error message indicates that the key policy for the customer-managed KMS key does not include the necessary permissions for the IAM role assumed by the Kubernetes service account. Adjusting the key policy to grant the required permissions should resolve the issue.

---

**tgv** (Mon 15 Jul 2024 12:42) - *Upvotes: 3*
---> B

---

**trungtd** (Sun 14 Jul 2024 09:13) - *Upvotes: 4*
The IAM role assumed by the Kubernetes service account, not the EKS cluster IAM role => C is wrong

---


<br/>

## Question 240

*Date: July 5, 2024, 3:27 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is migrating its product development teams from an on-premises data center to a hybrid environment. The new environment will add four AWS Regions and will give the developers the ability to use the Region that is geographically closest to them.

All the development teams use a shared set of Linux applications. The on-premises data center stores the applications on a NetApp ONTAP storage device. The storage volume is mounted read-only on the development on-premises VMs. The company updates the applications on the shared volume once a week.

A DevOps engineer needs to replicate the data to all the new Regions. The DevOps engineer must ensure that the data is always up to date with deduplication. The data also must not be dependent on the availability of the on-premises storage device.

Which solution will meet these requirements?

**Options:**
- A. Create an Amazon S3 File Gateway in the on-premises data center. Create S3 buckets in each Region. Set up a cron job to copy the data from the storage device to the S3 File Gateway. Set up S3 Cross-Region Replication (CRR) to the S3 buckets in each Region.
- B. Create an Amazon FSx File Gateway in one Region. Create file servers in Amazon FSx for Windows File Server in each Region. Set up a cron job to copy the data from the storage device to the FSx File Gateway.
- C. Create Multi-AZ Amazon FSx for NetApp ONTAP instances and volumes in each Region. Configure a scheduled SnapMirror relationship between the on-premises storage device and the FSx for ONTAP instances.
- D. Create an Amazon Elastic File System (Amazon EFS) file system in each Region. Deploy an AWS DataSync agent in the on-premises data center. Configure a schedule for DataSync to copy the data to Amazon EFS daily.

> **Suggested Answer:** C
> **Community Vote:** C (90%), 10%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**limelight04** (Mon 26 Aug 2024 04:21) - *Upvotes: 1*
Option C does involve using Amazon FSx for NetApp ONTAP, it doesn’t address the deduplication requirement or the independence from the availability of the on-premises storage device. Additionally, SnapMirror relationships are typically used for data replication within the same storage system rather than across multiple Regions.

For the specific requirements of deduplication, independence, and multi-Region replication, Option D (using Amazon EFS with AWS DataSync) is a more suitable solution.

---

**Duke315** (Thu 08 Aug 2024 08:52) - *Upvotes: 3*
Amazon FSx for NetApp ONTAP provides a managed NetApp ONTAP experience in the cloud. By creating Multi-AZ FSx for ONTAP instances in each Region, you can replicate data with high availability and redundancy.

Checkout cheaper contributor access here: https://exammatter.net/

SnapMirror is a replication technology provided by NetApp that allows for efficient and reliable data replication. Configuring SnapMirror relationships between your on-premises NetApp storage device and the FSx for ONTAP instances will ensure that your data is consistently replicated across all AWS Regions.

---

**jamesf** (Wed 31 Jul 2024 07:08) - *Upvotes: 3*
Amazon FSx for NetApp ONTAP provides a managed NetApp ONTAP experience in the cloud. By creating Multi-AZ FSx for ONTAP instances in each Region, you can replicate data with high availability and redundancy.

SnapMirror is a replication technology provided by NetApp that allows for efficient and reliable data replication. Configuring SnapMirror relationships between your on-premises NetApp storage device and the FSx for ONTAP instances will ensure that your data is consistently replicated across all AWS Regions.

---

**tgv** (Mon 15 Jul 2024 12:45) - *Upvotes: 2*
---> C

---

**trungtd** (Sun 14 Jul 2024 09:29) - *Upvotes: 3*
C
https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/migrating-fsx-ontap-snapmirror.html

---

**getadroit** (Sat 06 Jul 2024 22:41) - *Upvotes: 2*
C
https://aws.amazon.com/blogs/storage/cross-region-disaster-recovery-with-amazon-fsx-for-netapp-ontap/

---


<br/>

## Question 241

*Date: July 5, 2024, 6:43 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an application that stores data that includes personally identifiable information (PII) in an Amazon S3 bucket. All data is encrypted with AWS Key Management Service (AWS KMS) customer managed keys. All AWS resources are deployed from an AWS CloudFormation template.

A DevOps engineer needs to set up a development environment for the application in a different AWS account. The data in the development environment's S3 bucket needs to be updated once a week from the production environment's S3 bucket.

The company must not move PII from the production environment without anonymizing the PII first. The data in each environment must be encrypted with different KMS customer managed keys.

Which combination of steps should the DevOps engineer take to meet these requirements? (Choose two.)

**Options:**
- A. Activate Amazon Macie on the S3 bucket in the production account. Create an AWS Step Functions state machine to initiate a discovery job and redact all PII before copying files to the S3 bucket in the development account. Give the state machine tasks decrypt permissions on the KMS key in the production account. Give the state machine tasks encrypt permissions on the KMS key in the development account.
- B. Set up S3 replication between the production S3 bucket and the development S3 bucket. Activate Amazon Macie on the development S3 bucket. Create an AWS Step Functions state machine to initiate a discovery job and redact all PII as the files are copied to the development S3 bucket. Give the state machine tasks encrypt and decrypt permissions on the KMS key in the development account.
- C. Set up an S3 Batch Operations job to copy files from the production S3 bucket to the development S3 bucket. In the development account, configure an AWS Lambda function to redact ail PII. Configure S3 Object Lambda to use the Lambda function for S3 GET requests. Give the Lambda function's IAM role encrypt and decrypt permissions on the KMS key in the development account.
- D. Create a development environment from the CloudFormation template in the development account. Schedule an Amazon EventBridge rule to start the AWS Step Functions state machine once a week.
- E. Create a development environment from the CloudFormation template in the development account. Schedule a cron job on an Amazon EC2 instance to run once a week to start the S3 Batch Operations job.

> **Suggested Answer:** AD
> **Community Vote:** AD (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Wed 31 Jul 2024 07:19) - *Upvotes: 4*
Option A addresses the need to anonymize PII before moving data to the development environment. By using Amazon Macie, you can identify PII in the production S3 bucket. AWS Step Functions can orchestrate a workflow to redact this PII before transferring the data. This ensures compliance with data protection requirements. You need to provide the necessary KMS key permissions for decrypting and encrypting data as it moves between accounts.

Option D ensures that the data update process is automated and scheduled. Using Amazon EventBridge to trigger the AWS Step Functions state machine on a weekly basis automates the data transfer and anonymization process.

---

**tgv** (Fri 19 Jul 2024 07:58) - *Upvotes: 3*
---> A D

---

**trungtd** (Sun 14 Jul 2024 09:36) - *Upvotes: 3*
A. Anonymizing PII in the Production Account
D. Automating the Weekly Data Transfer

B suggests replicating the data before redacting PII, which violates the requirement
C does not ensure that the PII is redacted before the data is stored in the development environment
E introduces additional infrastructure management and costs

---

**getadroit** (Sat 06 Jul 2024 22:49) - *Upvotes: 1*
redact should be done before

---

**getadroit** (Sat 06 Jul 2024 22:48) - *Upvotes: 2*
A & D
https://aws.amazon.com/blogs/security/how-to-use-amazon-macie-to-preview-sensitive-data-in-s3-buckets/

---


<br/>

## Question 242

*Date: July 5, 2024, 6:36 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to host its machine learning (ML) application. As the ML model and the container image size grow, the time that new pods take to start up has increased to several minutes.

A DevOps engineer needs to reduce the startup time to seconds. The solution must also reduce the startup time to seconds when the pod runs on nodes that were recently added to the cluster.

The DevOps engineer creates an Amazon EventBridge rule that invokes an automation in AWS Systems Manager. The automation prefetches the container images from an Amazon Elastic Container Registry (Amazon ECR) repository when new images are pushed to the repository. The DevOps engineer also configures tags to be applied to the cluster and the node groups.

What should the DevOps engineer do next to meet the requirements?

**Options:**
- A. Create an IAM role that has a policy that allows EventBridge to use Systems Manager to run commands in the EKS cluster's control plane nodes. Create a Systems Manager State Manager association that uses the control plane nodes' tags to prefetch corresponding container images.
- B. Create an IAM role that has a policy that allows EventBridge to use Systems Manager to run commands in the EKS cluster's nodes. Create a Systems Manager State Manager association that uses the nodes' machine size to prefetch corresponding container images.
- C. Create an IAM role that has a policy that allows EventBridge to use Systems Manager to run commands in the EKS cluster's nodes. Create a Systems Manager State Manager association that uses the nodes' tags to prefetch corresponding container images.
- D. Create an IAM role that has a policy that allows EventBridge to use Systems Manager to run commands in the EKS cluster's control plane nodes. Create a Systems Manager State Manager association that uses the nodes' tags to prefetch corresponding container images.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**trungtd** (Sun 14 Jul 2024 09:44) - *Upvotes: 7*
The control plane manages the Kubernetes cluster but does not run the application containers => A & D wrong
Machine size is not a practical or flexible approach to determining where images should be prefetched. Should be Tag =>B Wrong

---

**youonebe** (Thu 26 Dec 2024 19:09) - *Upvotes: 2*
control plane is fully managed by aws.

---

**tgv** (Fri 19 Jul 2024 07:59) - *Upvotes: 3*
---> C

---

**getadroit** (Sat 06 Jul 2024 22:56) - *Upvotes: 2*
C
https://aws.amazon.com/blogs/containers/start-pods-faster-by-prefetching-images/

---


<br/>

## Question 243

*Date: July 5, 2024, 6:31 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company's application has an API that retrieves workload metrics. The company needs to audit, analyze, and visualize these metrics from the application to detect issues at scale.

Which combination of steps will meet these requirements? (Choose three.)

**Options:**
- A. Configure an Amazon EventBridge schedule to invoke an AWS Lambda function that calls the API to retrieve workload metrics. Store the workload metric data in an Amazon S3 bucket.
- B. Configure an Amazon EventBridge schedule to invoke an AWS Lambda function that calls the API to retrieve workload metrics. Store the workload metric data in an Amazon DynamoDB table that has a DynamoDB stream enabled.
- C. Create an AWS Glue crawler to catalog the workload metric data in the Amazon S3 bucket. Create views in Amazon Athena for the cataloged data.
- D. Connect an AWS Glue crawler to the Amazon DynamoDB stream to catalog the workload metric data. Create views in Amazon Athena for the cataloged data.
- E. Create Amazon QuickSight datasets from the Amazon Athena views. Create a QuickSight analysis to visualize the workload metric data as a dashboard.
- F. Create an Amazon CloudWatch dashboard that has custom widgets that invoke AWS Lambda functions. Configure the Lambda functions to query the workload metrics data from the Amazon Athena views.

> **Suggested Answer:** ACE
> **Community Vote:** ACE (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Wed 31 Jul 2024 07:26) - *Upvotes: 4*
Option A: Using Amazon EventBridge to schedule an AWS Lambda function that retrieves workload metrics from the API and stores the data in Amazon S3 provides a scalable and automated way to collect and store the data.

Option C: AWS Glue can be used to catalog the data stored in Amazon S3, making it queryable using Amazon Athena. This step prepares the data for analysis by creating a schema and making it available for querying.

Option E: Amazon QuickSight can be used to create datasets from the Athena views and then visualize the data in dashboards. This provides the capability to analyze and visualize workload metrics at scale.

https://aws.amazon.com/blogs/mt/analyzing-amazon-cloudwatch-internet-monitor-measurement-logs-using-amazon-athena-amazon-quicksight/

---

**jamesf** (Wed 31 Jul 2024 07:30) - *Upvotes: 1*
Additional Note:
Not Option B: Storing workload metric data in an Amazon DynamoDB table with DynamoDB Streams enabled is an option, but it’s not ideal for large-scale metrics and querying. DynamoDB is better suited for high-speed key-value access and doesn’t provide the same level of querying capabilities for large datasets compared to Amazon S3 with Athena.

DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near-real time.

---

**tgv** (Fri 19 Jul 2024 08:00) - *Upvotes: 3*
---> A C E

---

**trungtd** (Sun 14 Jul 2024 09:51) - *Upvotes: 4*
Data Collection and Storage: EventBridge Schedule + Lambda + S3
Data Cataloging and Querying: Glue Crawler + Athena
Data Visualization: QuickSight

---

**getadroit** (Sat 06 Jul 2024 23:04) - *Upvotes: 2*
ACE
https://aws.amazon.com/blogs/mt/analyzing-amazon-cloudwatch-internet-monitor-measurement-logs-using-amazon-athena-amazon-quicksight/

---


<br/>

## Question 244

*Date: June 29, 2024, 2:48 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer is building the infrastructure for an application. The application needs to run on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that includes Amazon EC2 instances. The EC2 instances need to use an Amazon Elastic File System (Amazon EFS) file system as a storage backend. The Amazon EFS Container Storage Interface (CSI) driver is installed on the EKS cluster.

When the DevOps engineer starts the application, the EC2 instances do not mount the EFS file system.

Which solutions will fix the problem? (Choose three.)

**Options:**
- A. Switch the EKS nodes from Amazon EC2 to AWS Fargate.
- B. Add an inbound rule to the EFS file system’s security group to allow NFS traffic from the EKS cluster.
- C. Create an IAM role that allows the Amazon EFS CSI driver to interact with the file system
- D. Set up AWS DataSync to configure file transfer between the EFS file system and the EKS nodes.
- E. Create a mount target for the EFS file system in the subnet of the EKS nodes.
- F. Disable encryption or the EFS file system.

> **Suggested Answer:** BCE
> **Community Vote:** BCE (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Wed 31 Jul 2024 07:32) - *Upvotes: 3*
B: The EFS file system’s security group must allow inbound traffic on the NFS port (2049) from the EC2 instances in the EKS cluster. Without this rule, the EC2 instances won't be able to communicate with the EFS file system.

C: The EFS CSI driver needs permissions to interact with the EFS file system. This involves creating an IAM role with the necessary permissions and associating it with the EFS CSI driver.

E: EFS requires a mount target in each subnet where the EC2 instances reside. This mount target facilitates the network connectivity between the EFS file system and the EC2 instances.

---

**jamesf** (Wed 31 Jul 2024 07:33) - *Upvotes: 1*
Why Not for options below,
Not A: Switching from EC2 to AWS Fargate would not directly address the issue with EFS mounting. AWS Fargate does not support mounting EFS file systems natively.

Not D: AWS DataSync is used for data transfer tasks and is not required for mounting EFS file systems in EKS. It is not relevant to solving the problem of mounting EFS.

Not F: Disabling encryption is not necessary and might compromise security. Encryption of EFS file systems should not interfere with mounting unless there is a configuration issue, which is unlikely to be resolved by disabling encryption.

---

**tgv** (Fri 19 Jul 2024 08:02) - *Upvotes: 2*
---> B C E

---

**trungtd** (Sun 14 Jul 2024 10:00) - *Upvotes: 2*
B. EFS file system’s security group must allow inbound NFS traffic (typically on port 2049) from the security group or IP range of the EKS cluster nodes.
C. Ensure that the EFS CSI driver has the necessary IAM permissions to interact with the EFS file system, such as "elasticfilesystem:DescribeFileSystems", "elasticfilesystem:DescribeMountTargets", and other relevant permissions.

---

**KaranNishad** (Sat 29 Jun 2024 14:48) - *Upvotes: 3*
So, the correct solutions are:

B. Add an inbound rule to the EFS file system’s security group to allow NFS traffic from the EKS cluster.
C. Create an IAM role that allows the Amazon EFS CSI driver to interact with the file system.
E. Create a mount target for the EFS file system in the subnet of the EKS nodes.

---


<br/>

## Question 245

*Date: June 29, 2024, 2:49 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company deploys an application on on-premises devices in the company’s on-premises data center. The company uses an AWS Direct Connect connection between the data center and the company's AWS account. During initial setup of the on-premises devices and during application updates, the application needs to retrieve configuration files from an Amazon Elastic File System (Amazon EFS) file system.

All traffic from the on-premises devices to Amazon EFS must remain private and encrypted. The on-premises devices must follow the principle of least privilege for AWS access. The company's DevOps team needs the ability to revoke access from a single device without affecting the access of the other devices.

Which combination of steps will meet these requirements? (Choose two.)

**Options:**
- A. Create an IAM user that has an access key and a secret key for each device. Attach the AmazonElasticFileSystemFullAccess policy to all IAM users. Configure the AWS CLI on the on-premises devices to use the IAM user's access key and secret key.
- B. Generate certificates for each on-premises device in AWS Private Certificate Authority. Create a trust anchor in IAM Roles Anywhere that references an AWS Private CA. Create an IAM role that trust IAM Roles Anywhere. Attach the AmazonElasticFileSystemClientReadWriteAccess to the role. Create an IAM Roles Anywhere profile for the IAM role. Configure the AWS CLI on the on-premises devices to use the aws_signing_helper command to obtain credentials.
- C. Create an IAM user that has an access key and a secret key for all devices. Attach the AmazonElasticFileSystemClientReadWriteAccess policy to the IAM user. Configure the AWS CLI on the on-premises devices to use the IAM user's access key and secret key.
- D. Use the amazon-efs-utils package to mount the EFS file system.
- E. Use the native Linux NFS client to mount the EFS file system.

> **Suggested Answer:** BD
> **Community Vote:** BD (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**getadroit** (Sat 06 Jul 2024 23:07) - *Upvotes: 5*
BD:
https://aws.amazon.com/blogs/aws/amazon-efs-update-on-premises-access-via-direct-connect-vpc/

---

**KaranNishad** (Sat 29 Jun 2024 14:49) - *Upvotes: 5*
B. Generate certificates for each on-premises device in AWS Private Certificate Authority. Create a trust anchor in IAM Roles Anywhere that references an AWS Private CA. Create an IAM role that trusts IAM Roles Anywhere. Attach the AmazonElasticFileSystemClientReadWriteAccess policy to the role. Create an IAM Roles Anywhere profile for the IAM role. Configure the AWS CLI on the on-premises devices to use the aws_signing_helper command to obtain credentials.
D. Use the amazon-efs-utils package to mount the EFS file system.

---

**tgv** (Fri 19 Jul 2024 08:05) - *Upvotes: 2*
---> B D

---

**trungtd** (Sun 14 Jul 2024 10:10) - *Upvotes: 3*
A. Creating individual IAM users with full access does not follow the principle of least privilege => Wrong
C. Using a single IAM user for all devices does not allow the ability to revoke access from a single device without affecting others => Wrong
E. Technically feasible, but it does not inherently provide encryption in transit

---


<br/>

## Question 246

*Date: July 5, 2024, 6:14 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer is setting up an Amazon Elastic Container Service (Amazon ECS) blue/green deployment for an application by using AWS CodeDeploy and AWS CloudFormation. During the deployment window, the application must be highly available and CodeDeploy must shift 10% of traffic to a new version of the application every minute until all traffic is shifted.

Which configuration should the DevOps engineer add in the CloudFormation template to meet these requirements?

**Options:**
- A. Add an AppSpec file with the CodeDeployDefault.ECSLinearl OPercentEveryl Minutes deployment configuration.
- B. Add the AWS::CodeDeployBlueGreen transform and the AWS::CodeDeploy::BlueGreen hook parameter with the CodeDeployDefault.ECSLinear10PercentEvery1Minutes deployment configuration.
- C. Add an AppSpec file with the ECSCanary10Percent5Minutes deployment configuration.
- D. Add the AWS::CodeDeployBlueGreen transform and the AWS::CodeDepioy::BlueGreen hook parameter with the ECSCanary10Percent5Minutes deployment configuration.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**trungtd** (Sun 14 Jul 2024 10:11) - *Upvotes: 5*
obviously B

---

**tgv** (Sun 07 Jul 2024 20:30) - *Upvotes: 2*
--> BB

---


<br/>

## Question 247

*Date: July 5, 2024, 6:13 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses an organization in AWS Organizations to manage its AWS accounts. The company's DevOps team has developed an AWS Lambda function that calls the Organizations API to create new AWS accounts.

The Lambda function runs in the organization's management account. The DevOps team needs to move the Lambda function from the management account to a dedicated AWS account. The DevOps team must ensure that the Lambda function has the ability to create new AWS accounts only in Organizations before the team deploys the Lambda function to the new account.

Which solution will meet these requirements?

**Options:**
- A. In the management account, create a new IAM role that has the necessary permission to create new accounts in Organizations. Allow the role to be assumed by the Lambda execution role in the new AWS account. Update the Lambda function code to assume the role when the Lambda function creates new AWS accounts. Update the Lambda execution role to ensure that it has permission to assume the new role.
- B. In the management account, turn on delegated administration for Organizations. Create a new delegation policy that grants the new AWS account permission to create new AWS accounts in Organizations. Ensure that the Lambda execution role has the organizations:CreateAccount permission.
- C. In the management account, create a new IAM role that has the necessary permission to create new accounts in Organizations. Allow the role to be assumed by the Lambda service principal. Update the Lambda function code to assume the role when the Lambda function creates new AWS accounts. Update the Lambda execution role to ensure that it has permission to assume the new role.
- D. In the management account, enable AWS Control Tower. Turn on delegated administration for AWS Control Tower. Create a resource policy that allows the new AWS account to create new AWS accounts in AWS Control Tower. Update the Lambda function code to use the AWS Control Tower API in the new AWS account. Ensure that the Lambda execution role has the controltower:CreateManagedAccount permission.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Wed 31 Jul 2024 07:48) - *Upvotes: 5*
Create an IAM Role with Necessary Permissions:
- In the management account, create an IAM role with permissions to call the AWS Organizations API for creating new accounts.

Allow Role Assumption: - Configure this IAM role to be assumable by the Lambda execution role in the new AWS account. This way, the Lambda function in the new account can assume the role to gain the necessary permissions.

Update Lambda Function and Execution Role:
- Modify the Lambda function code in the new account to assume the role created in the management account when it needs to create new AWS accounts. Also, ensure the Lambda execution role in the new account has the permissions required to assume the role in the management account.

---

**jamesf** (Wed 31 Jul 2024 07:49) - *Upvotes: 5*
Why not following options:
B: Delegated administration in AWS Organizations typically refers to giving permissions to manage AWS Organizations itself, rather than delegating permissions to create new accounts. Creating new accounts via the Organizations API requires specific IAM permissions, not just a delegation policy.

C: Allowing the Lambda service principal to assume an IAM role is not a valid approach for cross-account role assumption. Lambda functions assume roles that are explicitly allowed by their execution role, not service principals.

D: AWS Control Tower manages accounts and governance but requires different permissions and APIs compared to AWS Organizations for creating new accounts. Control Tower also does not directly handle account creation in the way described; instead, it manages accounts and governance at a higher level.

---

**trungtd** (Sun 14 Jul 2024 10:18) - *Upvotes: 4*
- Create IAM Role in Management Account: include actions like "organizations:CreateAccount"
- Allow Role Assumption: specifying the ARN of the Lambda execution role in the new account in the trust policy of the IAM role.
- Using the AWS SDK to assume the role and get temporary credentials in Lambda's code
- Ensure that the Lambda execution role in the new account has the necessary permissions to assume the IAM role created in the management account.

---

**tgv** (Sun 07 Jul 2024 20:32) - *Upvotes: 2*
---> A

---


<br/>

## Question 248

*Date: July 7, 2024, 8:33 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has deployed an application in a single AWS Region. The application backend uses Amazon DynamoDB tables and Amazon S3 buckets.

The company wants to deploy the application in a secondary Region. The company must ensure that the data in the DynamoDB tables and the S3 buckets persists across both Regions. The data must also immediately propagate across Regions.

Which solution will meet these requirements with the MOST operational efficiency?

**Options:**
- A. Implement two-way S3 bucket replication between the primary Region's S3 buckets and the secondary Region’s S3 buckets. Convert the DynamoDB tables into global tables. Set the secondary Region as the additional Region.
- B. Implement S3 Batch Operations copy jobs between the primary Region and the secondary Region for all S3 buckets. Convert the DynamoDB tables into global tables. Set the secondary Region as the additional Region.
- C. Implement two-way S3 bucket replication between the primary Region's S3 buckets and the secondary Region's S3 buckets. Enable DynamoDB streams on the DynamoDB tables in both Regions. In each Region, create an AWS Lambda function that subscribes to the DynamoDB streams. Configure the Lambda function to copy new records to the DynamoDB tables in the other Region.
- D. Implement S3 Batch Operations copy jobs between the primary Region and the secondary Region for all S3 buckets. Enable DynamoDB streams on the DynamoDB tables in both Regions. In each Region, create an AWS Lambda function that subscribes to the DynamoDB streams. Configure the Lambda function to copy new records to the DynamoDB tables in the other Region.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**trungtd** (Sun 14 Jul 2024 10:21) - *Upvotes: 5*
B & D. S3 Batch Operations copy jobs are not immediate and are typically used for bulk copying of data. They do not provide the immediacy required for data propagation across regions.
C. DynamoDB streams with AWS Lambda functions to replicate data introduces additional complexity and operational overhead.

---

**jamesf** (Wed 31 Jul 2024 07:52) - *Upvotes: 5*
Two-Way S3 Bucket Replication:
- While two-way replication is not typically needed for most scenarios (one-way replication is generally sufficient), for this requirement, if both regions need to have copies of data and keep them synchronized, you would implement replication rules to ensure data consistency across S3 buckets in different regions.

Global DynamoDB Tables:
- DynamoDB global tables are designed specifically for multi-Region, fully replicated tables. When you convert your DynamoDB tables into global tables and add the secondary Region, DynamoDB handles the replication of data across Regions automatically and immediately. This provides efficient and consistent data replication without requiring custom solutions.

---

**jamesf** (Wed 31 Jul 2024 07:54) - *Upvotes: 2*
keywords:
- data must also immediately propagate across Regions.
- MOST operational efficiency?

---

**jamesf** (Wed 31 Jul 2024 07:53) - *Upvotes: 3*
why not following options:
B: S3 Batch Operations are used for bulk operations and are not suitable for continuous synchronization. DynamoDB streams and Lambda functions are also not necessary when using DynamoDB global tables, as global tables automatically manage replication.

C: Two-way S3 bucket replication is complex and typically unnecessary. Using DynamoDB streams and Lambda functions for replication can be operationally intensive and error-prone compared to using global tables.

D: Similar to option C, using S3 Batch Operations and DynamoDB streams with Lambda functions involves more operational overhead and complexity compared to using DynamoDB global tables.

---

**tgv** (Sun 07 Jul 2024 20:33) - *Upvotes: 2*
---> A

---


<br/>

## Question 249

*Date: July 7, 2024, 8:35 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has configured Amazon RDS storage autoscaling for its RDS DB instances. A DevOps team needs to visualize the autoscaling events on an Amazon CloudWatch dashboard.

Which solution will meet this requirement?

**Options:**
- A. Create an Amazon EventBridge rule that reacts to RDS storage autoscaling events from RDS events. Create an AWS Lambda function that publishes a CloudWatch custom metric. Configure the EventBridge rule to invoke the Lambda function. Visualize the custom metric by using the CloudWatch dashboard.
- B. Create a trail by using AWS CloudTrail with management events configured. Configure the trail to send the management events to Amazon CloudWatch Logs. Create a metric filter in CloudWatch Logs to match the RDS storage autoscaling events. Visualize the metric filter by using the CloudWatch dashboard.
- C. Create an Amazon EventBridge rule that reacts to RDS storage autoscaling events from the RDS events. Create a CloudWatch alarm. Configure the EventBridge rule to change the status of the CloudWatch alarm. Visualize the alarm status by using the CloudWatch dashboard.
- D. Create a trail by using AWS CloudTrail with data events configured. Configure the trail to send the data events to Amazon CloudWatch Logs. Create a metric filter in CloudWatch Logs to match the RDS storage autoscaling events. Visualize the metric filter by using the CloudWatch dashboard.

> **Suggested Answer:** A
> **Community Vote:** A (82%), B (18%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**spring21** (Thu 02 Jan 2025 02:11) - *Upvotes: 2*
AWS CloudTrail does log autoscaling operations by capturing API calls made to the Auto Scaling service, meaning all actions related to scaling your EC2 instances, including scaling up, down, or adjusting scaling policies, will be recorded in your CloudTrail logs.

---

**auxwww** (Fri 18 Oct 2024 01:07) - *Upvotes: 4*
B - Incorrect
"Autoscaling operations aren't logged by AWS CloudTrail. For more information on CloudTrail, see Monitoring Amazon RDS API calls in AWS CloudTrail."

---

**Shenannigan** (Thu 19 Sep 2024 15:46) - *Upvotes: 3*
This question is tricky as both A and B are correct
If it said in near real time then I would have chosen A, but it didn't so I am saying B as it is less complex and doesn't require writing a lambda function to use custom metrics.

Honestly on this question flip a coin and choose one or the other

---

**jamesf** (Wed 31 Jul 2024 07:57) - *Upvotes: 4*
keywords:
- Amazon EventBridge rule
- CloudWatch custom metric

---

**trungtd** (Sun 14 Jul 2024 10:25) - *Upvotes: 4*
While CloudTrail can capture RDS events and send them to CloudWatch Logs, creating a metric filter in CloudWatch Logs is more complex and indirect compared to using EventBridge and a Lambda function to publish custom metrics directly to CloudWatch.

---

**tgv** (Sun 07 Jul 2024 20:35) - *Upvotes: 2*
---> A

---


<br/>

## Question 250

*Date: July 5, 2024, 6:08 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses containers for its applications. The company learns that some container images are missing required security configurations.

A DevOps engineer needs to implement a solution to create a standard base image. The solution must publish the base image weekly to the us-west-2 Region, us-east-2 Region, and eu-central-1 Region.

Which solution will meet these requirements?

**Options:**
- A. Create an EC2 Image Builder pipeline that uses a container recipe to build the image. Configure the pipeline to distribute the image to an Amazon Elastic Container Registry (Amazon ECR) repository in us-west-2. Configure ECR replication from us-west-2 to us-east-2 and from us-east-2 to eu-central-1. Configure the pipeline to run weekly.
- B. Create an AWS CodePipeline pipeline that uses an AWS CodeBuild project to build the image. Use AWS CodeDeploy to publish the image to an Amazon Elastic Container Registry (Amazon ECR) repository in us-west-2. Configure ECR replication from us-west-2 to us-east-2 and from us-east-2 to eu-central-1. Configure the pipeline to run weekly.
- C. Create an EC2 Image Builder pipeline that uses a container recipe to build the image. Configure the pipeline to distribute the image to Amazon Elastic Container Registry (Amazon ECR) repositories in all three Regions. Configure the pipeline to run weekly.
- D. Create an AWS CodePipeline pipeline that uses an AWS CodeBuild project to build the image. Use AWS CodeDeploy to publish the image to Amazon Elastic Container Registry (Amazon ECR) repositories in all three Regions. Configure the pipeline to run weekly.

> **Suggested Answer:** C
> **Community Vote:** C (77%), A (15%), 8%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Wed 31 Jul 2024 08:01) - *Upvotes: 5*
EC2 Image Builder:
- This service is designed to automate the creation and distribution of container images. It supports defining container recipes and automating the build process.
Direct Distribution:
- EC2 Image Builder can be configured to distribute the images directly to multiple ECR repositories across different regions. This aligns with the requirement of publishing the base image to the us-west-2, us-east-2, and eu-central-1 regions.
Weekly Schedule:
- EC2 Image Builder can be scheduled to run on a weekly basis, meeting the requirement for regular updates.

---

**jamesf** (Wed 31 Jul 2024 08:02) - *Upvotes: 1*
Why not following Options:
Options A and B involve using ECR replication, which adds extra complexity and is not as streamlined as direct distribution through EC2 Image Builder.
Option D suggests using AWS CodePipeline and AWS CodeDeploy, which is less specialized for container image building and distribution compared to EC2 Image Builder.

---

**noisonnoiton** (Fri 12 Jul 2024 03:21) - *Upvotes: 5*
https://docs.aws.amazon.com/ko_kr/imagebuilder/latest/userguide/manage-distribution-settings.html

---

**sinanci** (Sat 14 Dec 2024 11:57) - *Upvotes: 1*
A is correct. It offers a more operationally efficient and AWS-native solution for distributing container images across multiple regions.

---

**heff_bezos** (Fri 27 Sep 2024 10:14) - *Upvotes: 1*
EC2 Image Builder is for AMIs not container images that go to ECR...

"A replication action only occurs once per image push. For example, if you configured cross-Region replication from us-west-2 to us-east-1 and from us-east-1 to us-east-2, an image pushed to us-west-2 replicates to only us-east-1, it doesn't replicate again to us-east-2. This behavior applies to both cross-Region and cross-account replication."

https://docs.aws.amazon.com/AmazonECR/latest/userguide/replication.html

---

**heff_bezos** (Fri 27 Sep 2024 10:18) - *Upvotes: 1*
Ehh nvm it might be C

---

**tgv** (Mon 15 Jul 2024 14:22) - *Upvotes: 4*
---> C
You can distributes container image to Amazon ECR repository in multiple regions.
---
https://docs.aws.amazon.com/imagebuilder/latest/userguide/cr-upd-container-distribution-settings.html

---

**siheom** (Fri 12 Jul 2024 03:26) - *Upvotes: 1*
VOTE A

---

**inturist** (Wed 10 Jul 2024 18:32) - *Upvotes: 2*
A replication action only occurs once per image push. For example, if you configured cross-Region replication from us-west-2 to us-east-1 and from us-east-1 to us-east-2, an image pushed to us-west-2 replicates to only us-east-1, it doesn't replicate again to us-east-2. This behavior applies to both cross-Region and cross-account replication.

---

**getadroit** (Sat 06 Jul 2024 23:14) - *Upvotes: 1*
B
https://aws.amazon.com/blogs/containers/cross-region-replication-in-amazon-ecr-has-landed/

---


<br/>

## Question 251

*Date: July 6, 2024, 7:29 a.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer needs to implement a solution to install antivirus software on all the Amazon EC2 instances in an AWS account. The EC2 instances run the most recent version of Amazon Linux.

The solution must detect all instances and must use an AWS Systems Manager document to install the software if the software is not present.

Which solution will meet these requirements?

**Options:**
- A. Create an association in Systems Manager State Manager. Target all the managed nodes. Include the software in the association. Configure the association to use the Systems Manager document.
- B. Set up AWS Config to record all the resources in the account. Create an AWS Config custom rule to determine if the software is installed on all the EC2 instances. Configure an automatic remediation action that uses the Systems Manager document for noncompliant EC2 instances.
- C. Activate Amazon EC2 scanning on Amazon Inspector to determine if the software is installed on all the EC2 instances. Associate the findings with the Systems Manager document.
- D. Create an Amazon EventBridge rule that uses AWS CloudTrail to detect the Runinstances API call. Configure inventory collection in Systems Manager Inventory to determine if the software is installed on the EC2 instances. Associate the Systems Manager inventory with the Systems Manager document.

> **Suggested Answer:** A
> **Community Vote:** A (80%), B (20%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**spring21** (Sun 15 Dec 2024 00:27) - *Upvotes: 1*
https://docs.aws.amazon.com/systems-manager/latest/userguide/state-manager-associations-creating.html

---

**limelight04** (Tue 27 Aug 2024 00:40) - *Upvotes: 1*
Given the requirement to detect instances and use an SSM document for installation, Option B seems most appropriate. It combines AWS Config for detection and Systems Manager for remediation.

---

**jamesf** (Wed 31 Jul 2024 08:36) - *Upvotes: 1*
AWS Systems Manager State Manager:
Automatic Detection:
- State Manager allows you to manage the desired state of your AWS resources, including EC2 instances. By targeting all managed nodes, you ensure that every EC2 instance under Systems Manager's management is included in the scope.
Software Installation:
- You can specify a Systems Manager document (SSM document) to define the steps required to install the antivirus software. The association will ensure that the software is installed on any instances where it is missing.
Continuous Compliance:
- State Manager can continuously enforce the desired state, which means it will periodically check for the presence of the software and reapply the document if necessary.

---

**jamesf** (Wed 31 Jul 2024 08:36) - *Upvotes: 1*
Use Case Alignment:
Managed Nodes Targeting:
- This allows for broad application across all instances, ensuring that no instances are missed, as long as they are configured as managed instances.
Ease of Configuration:
- Setting up an association in State Manager is straightforward and integrates well with the existing AWS Systems Manager services, making it a robust choice for managing configurations across instances.

---

**d0229a2** (Wed 24 Jul 2024 09:50) - *Upvotes: 1*
State Manager associations
A State Manager association is a configuration that you assign to your AWS resources. The configuration defines the state that you want to maintain on your resources. For example, an association can specify that antivirus software must be installed and running on a managed node, or that certain ports must be closed.

An association specifies a schedule for when to apply the configuration and the targets for the association. For example, an association for antivirus software might run once a day on all managed nodes in an AWS account. If the software isn't installed on a node, then the association could instruct State Manager to install it. If the software is installed, but the service isn't running, then the association could instruct State Manager to start the service.

---

**trungtd** (Wed 17 Jul 2024 14:29) - *Upvotes: 2*
By creating an association, you can ensure that all instances have the antivirus software installed and kept up-to-date.

---

**tgv** (Wed 17 Jul 2024 07:42) - *Upvotes: 1*
---> I'm between A & D
Not 100% sure about this but here are my 2 cents about DETECTING the instances that don't have the software installed:

A - it's a bit tricky because it states that it targets all managed nodes - but what if there are other nodes that are not managed? It just assumes that all instances are managed by AWS Systems Manager

B - How can Config determine if the software is installed?

C - Amazon Inspector is focused on security assessments and compliance checks, not on ensuring software is installed. It would require additional setup and is not designed for direct software installation.

D - it ensures that all instances are detected. It ensures that the installed software is tracked by using the AWS Systems Manager Inventory (which is designed for this kind of things). I'm not 100% sure about the phrase "Associate the Systems Manager inventory with the Systems Manager document." which I don't believe its technically possible

---


<br/>

## Question 252

*Date: July 14, 2024, 10:45 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company needs to increase the security of the container images that run in its production environment. The company wants to integrate operating system scanning and programming language package vulnerability scanning for the containers in its CI/CD pipeline. The CI/CD pipeline is an AWS CodePipeline pipeline that includes an AWS CodeBuild build project, AWS CodeDeploy actions, and an Amazon Elastic Container Registry (Amazon ECR) repository.

A DevOps engineer needs to add an image scan to the CI/CD pipeline. The CI/CD pipeline must deploy only images without CRITICAL and HIGH findings into production.

Which combination of steps will meet these requirements? (Choose two.)

**Options:**
- A. Use Amazon ECR basic scanning.
- B. Use Amazon ECR enhanced scanning.
- C. Configure Amazon ECR to submit a Rejected status to the CI/CD pipeline when the image scan returns CRITICAL or HIGH findings.
- D. Configure an Amazon EventBridge rule to invoke an AWS Lambda function when the image scan is completed. Configure the Lambda function to consume the Amazon Inspector scan status and to submit an Approved or Rejected status to the CI/CD pipeline.
- E. Configure an Amazon EventBridge rule to invoke an AWS Lambda function when the image scan is completed. Configure the Lambda function to consume the Clair scan status and to submit an Approved or Rejected status to the CI/CD pipeline.

> **Suggested Answer:** BD
> **Community Vote:** BD (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Wed 31 Jul 2024 08:44) - *Upvotes: 2*
B. Use Amazon ECR Enhanced Scanning
- Comprehensive Vulnerability Checks: Amazon ECR enhanced scanning is integrated with Amazon Inspector, providing thorough security checks on container images. It scans for both operating system vulnerabilities and application-level vulnerabilities in programming language packages, which basic scanning does not support.
- Integration with Amazon Inspector: Enhanced scanning leverages Amazon Inspector for deeper vulnerability analysis, ensuring the images are secure before deployment.
- CRITICAL and HIGH Severity Detection: The enhanced scanning option specifically identifies CRITICAL and HIGH vulnerabilities, aligning with the requirement to only deploy images that do not have these issues.

---

**d0229a2** (Wed 24 Jul 2024 09:44) - *Upvotes: 1*
All images pushed to Amazon ECR after enhanced scanning is turned on are continually scanned for the configured duration.

---

**tgv** (Fri 19 Jul 2024 13:00) - *Upvotes: 3*
---> B D

As per documentation, basic scanning use CVEs from the open-source Clair project. Enhanced scanning is an integration with Amazon Inspector. This suggests both options use different database/scanners.

https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning-enhanced.html

https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning-basic.html

---


<br/>

## Question 253

*Date: July 6, 2024, 7:19 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company's DevOps team manages a set of AWS accounts that are in an organization in AWS Organizations.

The company needs a solution that ensures that all Amazon EC2 instances use approved AM Is that the DevOps team manages. The solution also must remediate the usage of AMIs that are not approved. The individual account administrators must not be able to remove the restriction to use approved AMIs.

Which solution will meet these requirements?

**Options:**
- A. Use AWS CloudFormation StackSets to deploy an Amazon EventBridge rule to each account. Configure the rule to react to AWS CloudTrail events for Amazon EC2 and to send a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the DevOps team to the SNS topic.
- B. Use AWS CloudFormation StackSets to deploy the approved-amis-by-id AWS Config managed rule to each account. Configure the rule with the list of approved AMIs. Configure the rule to run the AWS-StopEC2Instance AWS Systems Manager Automation runbook for the noncompliant EC2 instances.
- C. Create an AWS Lambda function that processes AWS CloudTrail events for Amazon EC2. Configure the Lambda function to send a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the DevOps team to the SNS topic. Deploy the Lambda function in each account in the organization. Create an Amazon EventBridge rule in each account. Configure the EventBridge rules to react to AWS CloudTrail events for Amazon EC2 and to invoke the Lambda function.
- D. Enable AWS Config across the organization. Create a conformance pack that uses the approved-amis-by-id AWS Config managed rule with the list of approved AMIs. Deploy the conformance pack across the organization. Configure the rule to run the AWS-StopEC2lnstance AWS Systems Manager Automation runbook for the noncompliant EC2 instances.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**trungtd** (Sun 14 Jul 2024 10:49) - *Upvotes: 5*
A & C. only alert, not automatically remediate noncompliant instances
B. deploy via CloudFormation StackSets to individual accounts can still allow account administrators to modify or remove the rules.

---

**tgv** (Fri 19 Jul 2024 13:27) - *Upvotes: 4*
---> D

---


<br/>

## Question 254

*Date: July 6, 2024, 7:17 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company gives its employees limited rights to AWS. DevOps engineers have the ability to assume an administrator role. For tracking purposes, the security team wants to receive a near-real-time notification when the administrator role is assumed.

How should this be accomplished?

**Options:**
- A. Configure AWS Config to publish logs to an Amazon S3 bucket. Use Amazon Athena to query the logs and send a notification to the security team when the administrator role is assumed.
- B. Configure Amazon GuardDuty to monitor when the administrator role is assumed and send a notification to the security team.
- C. Create an Amazon EventBridge event rule using an AWS Management Console sign-in events event pattern that publishes a message to an Amazon SNS topic if the administrator role is assumed.
- D. Create an Amazon EventBridge events rule using an AWS API call that uses an AWS CloudTrail event pattern to invoke an AWS Lambda function that publishes a message to an Amazon SNS topic if the administrator role is assumed.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Wed 31 Jul 2024 08:54) - *Upvotes: 5*
Option D provides a robust and effective approach to tracking and alerting on the assumption of the administrator role by leveraging the power of AWS CloudTrail, Amazon EventBridge, AWS Lambda, and Amazon SNS.

Not Option C as Incorrect Event Pattern: This option specifies monitoring AWS Management Console sign-in events, which are unrelated to the AssumeRole API call used when assuming a role programmatically. It wouldn't detect role assumptions made through CLI or SDKs.

---

**teo2157** (Wed 15 Jan 2025 12:15) - *Upvotes: 2*
I select D because C is refering just to Console sign-in events but why a lambda function is required when an EventBridge rule can publish directly to an SNS topic?

---

**ericphl** (Sat 27 Jul 2024 07:05) - *Upvotes: 4*
Vote D.
A is not near-real-time solution.
B. GuardDuty is designed for threat detection. not for monitoring role assuming.
C. while C use the EventBridge, it monitoring console sign-in event only. rather than API call for assuming roles.

---

**tgv** (Mon 15 Jul 2024 20:15) - *Upvotes: 2*
---> D

---


<br/>

## Question 255

*Date: July 6, 2024, 7:13 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company needs a strategy for failover and disaster recovery of its data and application. The application uses a MySQL database and Amazon EC2 instances. The company requires a maximum RPO of 2 hours and a maximum RTO of 10 minutes for its data and application at all times.

Which combination of deployment strategies will meet these requirements? (Choose two.)

**Options:**
- A. Create an Amazon Aurora Single-AZ cluster in multiple AWS Regions as the data store. Use Aurora's automatic recovery capabilities in the event of a disaster.
- B. Create an Amazon Aurora global database in two AWS Regions as the data store. In the event of a failure, promote the secondary Region to the primary for the application. Update the application to use the Aurora cluster endpoint in the secondary Region.
- C. Create an Amazon Aurora cluster in multiple AWS Regions as the data store. Use a Network Load Balancer to balance the database traffic in different Regions.
- D. Set up the application in two AWS Regions. Use Amazon Route 53 failover routing that points to Application Load Balancers in both Regions. Use health checks and Auto Scaling groups in each Region.
- E. Set up the application in two AWS Regions. Configure AWS Global Accelerator to point to Application Load Balancers (ALBs) in both Regions. Add both ALBs to a single endpoint group. Use health checks and Auto Scaling groups in each Region.

> **Suggested Answer:** BD
> **Community Vote:** BD (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**GripZA** (Mon 21 Apr 2025 20:54) - *Upvotes: 1*
B - With the Amazon Aurora Global Database feature, you set up multiple Aurora DB clusters that span multiple AWS Regions. Aurora automatically synchronizes all changes made in the primary DB cluster to one or more secondary clusters. An Aurora global database has a primary DB cluster in one Region, and up to five secondary DB clusters in different Regions. This multi-Region configuration provides fast recovery from the rare outage that might affect an entire AWS Region. Having a full copy of all your data in multiple geographic locations also enables low-latency read operations for applications that connect from widely separated locations around the world.

D - Failover routing lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy. The primary and secondary records can route traffic to anything from an Amazon S3 bucket that is configured as a website to a complex tree of records.

---

**VerRi** (Thu 07 Nov 2024 06:59) - *Upvotes: 2*
E. Global Accelerator could provide faster failover speed but not necessary in this case.

---

**jamesf** (Wed 31 Jul 2024 08:56) - *Upvotes: 2*
keywords: Amazon Aurora global database , Route 53

---

**tgv** (Mon 15 Jul 2024 20:12) - *Upvotes: 2*
---> BD

---

**trungtd** (Sun 14 Jul 2024 10:54) - *Upvotes: 2*
Only BD meets the requirements

---


<br/>

## Question 256

*Date: July 6, 2024, 7:03 a.m.
Disclaimers:
- ExamTopics website is not rel*

A developer is using the AWS Serverless Application Model (AWS SAM) to create a prototype for an AWS Lambda function. The AWS SAM template contains an AWS::Serverless::Function resource that has the CodeUri property that points to an Amazon S3 location. The developer wants to identify the correct commands for deployment before creating a CI/CD pipeline.

The developer creates an archive of the Lambda function code named package.zip. The developer uploads the .zip file archive to the S3 location specified in the CodeUri property. The developer runs the sam deploy command and deploys the Lambda function. The developer updates the Lambda function code and uses the same steps to deploy the new version of the Lambda function. The sam deploy command fails and returns an error of no changes to deploy.

Which solutions will deploy the new version? (Choose two.)

**Options:**
- A. Use the aws cloudformation update-stack command instead of the sam deploy command.
- B. Use the aws cloudformation update-stack-instances command instead of the sam deploy command.
- C. Update the CodeUri property to reference the local application code folder. Use the sam deploy command.
- D. Update the CodeUri property to reference the local application code folder. Use the aws cloudformation create-change-set command and the aws cloudformation execute-change-set command.
- E. Update the CodeUri property to reference the local application code folder. Use the aws cloudformation package command and the aws cloudformation deploy command.

> **Suggested Answer:** CE
> **Community Vote:** CE (85%), AC (15%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**trungtd** (Sun 14 Jul 2024 11:02) - *Upvotes: 6*
C. Update the CodeUri property to reference the local application code folder, AWS SAM will handle packaging and uploading the code to S3 during the "sam deploy" command execution.
E.
- "aws cloudformation package" command packages the local artifacts (such as Lambda function code) and uploads them to an S3 bucket. It then generates a CloudFormation template that references these artifacts.
- "aws cloudformation deploy" command deploys the generated CloudFormation template.

A. "aws cloudformation update-stack": without the packaging step, it won't recognize changes in the Lambda function code
B. used for stack set instances
D. without proper packaging of the local code, it may not detect changes correctly.

---

**GripZA** (Mon 21 Apr 2025 21:07) - *Upvotes: 1*
C: When you set the CodeUri in your AWS SAM template to point to a local directory (e.g., CodeUri: ./src/), the sam deploy command automatically packages your application:​

It uploads the local code to an Amazon S3 bucket
It updates the cfn template with the new S3 URI
It deploys the updated application

E: this approach is similar to C but uses AWS cfn commands directly:​

aws cloudformation package uploads your local code to S3 and generates a new template with the updated CodeUri
aws cloudformation deploy uses the updated template to deploy your application.

this is more manual compared to using sam deploy but achieves the same result

---

**jamesf** (Wed 31 Jul 2024 09:16) - *Upvotes: 4*
Both Option C and Option E provide efficient and reliable methods to deploy updated Lambda function code using AWS SAM and CloudFormation. They address the deployment issue by ensuring that changes are recognized and appropriately handled, facilitating successful code updates in a CI/CD context.

By updating the CodeUri to reference the local folder, both approaches ensure that SAM or CloudFormation acknowledges code changes, effectively resolving the "no changes to deploy" error and enabling seamless deployments.

---

**jamesf** (Wed 31 Jul 2024 09:18) - *Upvotes: 1*
Not A as Misfit for Code Changes:
- aws cloudformation update-stack is primarily used for updating CloudFormation stack configurations, not for detecting code changes in deployment packages.
- It relies on an already packaged and uploaded S3 file specified in the template. Since no template changes are detected, the command would not recognize code updates.

Not B as Incorrect Command Context:
- aws cloudformation update-stack-instances is designed for AWS CloudFormation StackSets, which are used to manage resources across multiple AWS accounts and regions, not for deploying Lambda functions or single stack updates.
- This command is irrelevant to the deployment of a single Lambda function and won't address the issue at hand.

---

**awsaz** (Tue 23 Jul 2024 13:16) - *Upvotes: 2*
The two correct solutions to deploy the new version of the Lambda function code when sam deploy reports no changes are:

A. Use the aws cloudformation update-stack command instead of the sam deploy command.
C. Update the CodeUri property to reference the local application code folder. Use the sam deploy command.
These approaches ensure that changes to your Lambda function code are correctly identified and deployed without encountering the "no changes to deploy" error.

---


<br/>

## Question 257

*Date: July 12, 2024, 2:28 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company runs its container workloads in AWS App Runner. A DevOps engineer manages the company's container repository in Amazon Elastic Container Registry (Amazon ECR).

The DevOps engineer must implement a solution that continuously monitors the container repository. The solution must create a new container image when the solution detects an operating system vulnerability or language package vulnerability.

Which solution will meet these requirements?

**Options:**
- A. Use EC2 Image Builder to create a container image pipeline. Use Amazon ECR as the target repository. Turn on enhanced scanning on the ECR repository. Create an Amazon EventBridge rule to capture an Inspector? finding event. Use the event to invoke the image pipeline. Re-upload the container to the repository.
- B. Use EC2 Image Builder to create a container image pipeline. Use Amazon ECR as the target repository. Enable Amazon GuardDuty Malware Protection on the container workload. Create an Amazon EventBridge rule to capture a GuardDuty finding event. Use the event to invoke the image pipeline.
- C. Create an AWS CodeBuild project to create a container image. Use Amazon ECR as the target repository. Turn on basic scanning on the repository. Create an Amazon EventBridge rule to capture an ECR image action event. Use the event to invoke the CodeBuild project. Re-upload the container to the repository.
- D. Create an AWS CodeBuild project to create a container image. Use Amazon ECR as the target repository. Configure AWS Systems Manager Compliance to scan all managed nodes. Create an Amazon EventBridge rule to capture a configuration compliance state change event. Use the event to invoke the CodeBuild project.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**TEC1** (Fri 12 Jul 2024 14:28) - *Upvotes: 6*
Turn on enhanced scanning in the Amazon ECR repository settings. This enables Amazon Inspector to scan images for vulnerabilities.

---

**TEC1** (Fri 12 Jul 2024 14:29) - *Upvotes: 1*
https://docs.aws.amazon.com/inspector/latest/user/scanning-ecr.html#:~:text=To%20configure%20your%20enhanced%20scanning%20settings&text=Open%20the%20Amazon%20ECR%20console,registry%2C%20and%20then%20choose%20Settings.

---

**jamesf** (Wed 31 Jul 2024 09:20) - *Upvotes: 5*
Keywords: Enhanced scanning, Amazon ECR, Amazon Inspector, vulnerabilities

---

**tgv** (Mon 15 Jul 2024 20:11) - *Upvotes: 4*
---> A

---

**trungtd** (Sun 14 Jul 2024 11:05) - *Upvotes: 4*
Enhanced scanning provides deep and comprehensive scanning for vulnerabilities in container images using Amazon Inspector.

---


<br/>

## Question 258

*Date: July 6, 2024, 6:50 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company wants to use AWS Systems Manager documents to bootstrap physical laptops for developers. The bootstrap code is stored in GitHub. A DevOps engineer has already created a Systems Manager activation, installed the Systems Manager agent with the registration code, and installed an activation ID on all the laptops.

Which set of steps should be taken next?

**Options:**
- A. Configure the Systems Manager document to use the AWS-RunShellScript command to copy the files from GitHub to Amazon S3, then use the aws-downloadContent plugin with a sourceType of S3.
- B. Configure the Systems Manager document to use the aws-configurePackage plugin with an install action and point to the Git repository.
- C. Configure the Systems Manager document to use the aws-downloadContent plugin with a sourceType of GitHub and sourceInfo with the repository details.
- D. Configure the Systems Manager document to use the aws:softwareInventory plugin and run the script from the Git repository.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Wed 31 Jul 2024 09:24) - *Upvotes: 4*
aws:downloadContent
(Schema version 2.0 or later) Download SSM documents and scripts from remote locations. GitHub Enterprise repositories are not supported. This plugin is supported on Linux and Windows Server operating systems.

---

**tgv** (Wed 17 Jul 2024 07:53) - *Upvotes: 3*
---> C

---

**trungtd** (Sun 14 Jul 2024 16:52) - *Upvotes: 4*
The aws-downloadContent plugin is specifically designed to download content from various sources, including GitHub.
Setting the sourceType to GitHub and providing the repository details in sourceInfo to directly download the bootstrap code from GitHub to the laptops.

---

**getadroit** (Sat 06 Jul 2024 23:36) - *Upvotes: 1*
c
https://docs.aws.amazon.com/systems-manager/latest/userguide/documents-running-remote-github-s3.html

---


<br/>

## Question 259

*Date: July 5, 2024, 8:28 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company's development team uses AWS CloudFormation to deploy its application resources. The team must use CloudFormation for all changes to the environment. The team cannot use the AWS Management Console or the AWS CLI to make manual changes directly.

The team uses a developer IAM role to access the environment. The role is configured with the AdministratorAccess managed IAM policy. The company has created a new CloudFormationDeployment IAM role that has the following policy attached:



The company wants to ensure that only CloudFormation can use the new role. The development team cannot make any manual changes to the deployed resources.

Which combination of steps will meet these requirements? (Choose three.)

**Options:**
- A. Remove the AdministratorAccess policy. Assign the ReadOnlyAccess managed IAM policy to the developer role. Instruct the developers to use the CloudFormationDeployment role as a CloudFormation service role when the developers deploy new stacks.
- B. Update the trust policy of the CloudFormationDeployment role to allow the developer IAM role to assume the CloudFormationDeployment role.
- C. Configure the developer IAM role to be able to get and pass the CloudFormationDeployment role if iam:PassedToService equals . Configure the CloudFormationDeployment role to allow all cloudformation actions for all resources.
- D. Update the trust policy of the CloudFormationDeployment role to allow the cloudformation.amazonaws.com AWS principal to perform the iam:AssumeRole action.
- E. Remove the AdministratorAccess policy. Assign the ReadOnlyAccess managed IAM policy to the developer role. Instruct the developers to assume the CloudFormationDeployment role when the developers deploy new stacks.
- F. Add an IAM policy to the CloudFormationDeployment role to allow cloudformation:* on all resources. Add a policy that allows the iam:PassRole action for the ARN of the CloudFormationDeployment role if iam:PassedToService equals cloudformation.amazonaws.com.

> **Suggested Answer:** ADF
> **Community Vote:** ADF (89%), 11%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**limelight04** (Tue 27 Aug 2024 02:08) - *Upvotes: 1*
While Option F seems reasonable at first glance, it has a potential issue. By allowing cloudformation:* on all resources, you grant broad permissions to CloudFormation actions, which may not align with the goal of restricting manual changes. It’s essential to strike a balance between security and functionality.

The combination of Options A, B, and D ensures that only CloudFormation can assume the CloudFormationDeployment role, and the ReadOnlyAccess policy for developers prevents unintended modifications. This approach maintains a more controlled and secure environment.

---

**jamesf** (Wed 31 Jul 2024 09:31) - *Upvotes: 3*
A. Remove the AdministratorAccess policy. Assign the ReadOnlyAccess managed IAM policy to the developer role. Instruct the developers to use the CloudFormationDeployment role as a CloudFormation service role when the developers deploy new stacks.

D. Update the trust policy of the CloudFormationDeployment role to allow the cloudformation.amazonaws.com AWS principal to perform the iam:AssumeRole action.

F. Add an IAM policy to the CloudFormationDeployment role to allow cloudformation:* on all resources. Add a policy that allows the iam:PassRole action for the ARN of the CloudFormationDeployment role if iam:PassedToService equals cloudformation.amazonaws.com.

---

**jamesf** (Wed 31 Jul 2024 09:32) - *Upvotes: 3*
Not Option E: Developer Assumption of CloudFormationDeployment Role
- Manual Role Assumption: Similar to option B, this option would allow developers to directly assume the CloudFormationDeployment role. It introduces the risk of developers bypassing CloudFormation for changes, which violates the requirement to prevent manual modifications.

---

**jamesf** (Wed 31 Jul 2024 09:32) - *Upvotes: 4*
Why Other Options Are Less Suitable:
Not Option B: Trust Policy with Developer IAM Role
- Misaligned Trust Policy: Allowing the developer IAM role to assume the CloudFormationDeployment role would directly enable developers to assume the role, contradicting the requirement of not allowing them to make manual changes. This option bypasses the control we want to establish by having CloudFormation handle the role assumption.

Not Option C: Conditional Role Passing
- Incorrect Logic: While this option attempts to create a condition for passing the role, it does not align with using CloudFormation as the sole entity allowed to assume the role. It implies a developer-driven role assumption rather than a service-driven one.

---

**tgv** (Wed 17 Jul 2024 07:58) - *Upvotes: 1*
---> A D F

---

**trungtd** (Sun 14 Jul 2024 17:01) - *Upvotes: 4*
A. ensures that developers cannot make manual changes to the environment.
D. ensures that only CloudFormation can assume this role.
F. ensures that the role can only be passed to CloudFormation, not to any other service or user.

---


<br/>

## Question 260

*Date: July 5, 2024, 8:09 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is developing a web application's infrastructure using AWS CloudFormation. The database engineering team maintains the database resources in a CloudFormation template, and the software development team maintains the web application resources in a separate CloudFormation template. As the scope of the application grows, the software development team needs to use resources maintained by the database engineering team. However, both teams have their own review and lifecycle management processes that they want to keep. Both teams also require resource-level change-set reviews. The software development team would like to deploy changes to this template using their CI/CD pipeline.

Which solution will meet these requirements?

**Options:**
- A. Create a stack export from the database CloudFormation template and import those references into the web application CloudFormation template.
- B. Create a CloudFormation nested stack to make cross-stack resource references and parameters available in both stacks.
- C. Create a CloudFormation stack set to make cross-stack resource references and parameters available in both stacks.
- D. Create input parameters in the web application CloudFormation template and pass resource names and IDs from the database stack.

> **Suggested Answer:** A
> **Community Vote:** A (93%), 7%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**trungtd** (Sun 14 Jul 2024 17:29) - *Upvotes: 5*
B. Nested stacks combine all stacks into a single stack, which can complicate the independent lifecycle and review processes for each team.
C. Stack sets are typically used for deploying stacks across multiple AWS accounts and Regions
D. could work but require manual intervention to update the parameter values whenever there is a change in the database stack resources.

---

**nickp84** (Mon 19 May 2025 19:36) - *Upvotes: 1*
option D is wrong:
D. Pass input parameters manually: Feasible, but error-prone and lacks validation, automation, and traceability. Requires manual coordination between teams, which violates the goal of independent lifecycle management.

---

**tdlAws** (Mon 20 Jan 2025 22:43) - *Upvotes: 1*
Option D is the most suitable solution because it allows separation of management processes, minimizes hard dependencies between templates, and is easily integrated into the development team's CI/CD pipeline.

---

**jamesf** (Wed 31 Jul 2024 09:39) - *Upvotes: 4*
A. Create a stack export from the database CloudFormation template and import those references into the web application CloudFormation template.
- Decoupled Management: Each team retains its management process, aligning with their specific workflows.
- Cross-Stack Referencing: Utilizes CloudFormation's Exports and Fn::ImportValue to reference resources between stacks efficiently.
- Resource-Level Change Sets: Supports detailed change-set reviews, enabling teams to preview changes before deployment.
- CI/CD Pipeline Compatibility: Works seamlessly with CI/CD pipelines by allowing modular updates to stacks without direct dependencies.

---

**xdkonorek2** (Fri 05 Jul 2024 20:09) - *Upvotes: 4*
A. Create a stack export from the database CloudFormation template and import those references into the web application CloudFormation template.

---


<br/>

## Question 261

*Date: July 14, 2024, 5:37 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an organization in AWS Organizations. A DevOps engineer needs to maintain multiple AWS accounts that belong to different OUs in the organization. All resources, including IAM policies and Amazon S3 policies within an account, are deployed through AWS CloudFormation. All templates and code are maintained in an AWS CodeCommit repository. Recently, some developers have not been able to access an S3 bucket from some accounts in the organization.

The following policy is attached to the S3 bucket:



What should the DevOps engineer do to resolve this access issue?

**Options:**
- A. Modify the S3 bucket policy. Turn off the S3 Block Public Access setting on the S3 bucket. In the S3 policy, add the aws:SourceAccount condition. Add the AWS account IDs of all developers who are experiencing the issue.
- B. Verify that no IAM permissions boundaries are denying developers access to the S3 bucket. Make the necessary changes to IAM permissions boundaries. Use an AWS Config recorder in the individual developer accounts that are experiencing the issue to revert any changes that are blocking access. Commit the fix back into the CodeCommit repository. Invoke deployment through CloudFormation to apply the changes.
- C. Configure an SCP that stops anyone from modifying IAM resources in developer OUs. In the S3 policy, add the aws:SourceAccount condition. Add the AWS account IDs of all developers who are experiencing the issue. Commit the fix back into the CodeCommit repository. Invoke deployment through CloudFormation to apply the changes.
- D. Ensure that no SCP is blocking access for developers to the S3 bucket. Ensure that no IAM policy permissions boundaries are denying access to developer IAM users. Make the necessary changes to the SCP and IAM policy permissions boundaries in the CodeCommit repository. Invoke deployment through CloudFormation to apply the changes.

> **Suggested Answer:** D
> **Community Vote:** D (93%), 7%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**trungtd** (Sun 14 Jul 2024 17:37) - *Upvotes: 5*
Option D is the most comprehensive and aligns with the requirements:
- It ensures that both SCPs and IAM policies are correctly configured.
- It adheres to the use of CloudFormation for all changes.
- It addresses the immediate issue while providing a scalable and manageable approach.

---

**teo2157** (Wed 15 Jan 2025 15:27) - *Upvotes: 1*
Going with B as if there was an SCP in place, it affects all developers and not some of them. Furthermore, with the config recorded you can trace the changes done in the iam policies for the users that are not able to access the S3 bucket and fix it.

---

**jamesf** (Wed 31 Jul 2024 09:45) - *Upvotes: 4*
- Comprehensive approach: Reviews both SCPs and IAM permissions boundaries that could block access.
- Changes are committed to CodeCommit and deployed through CloudFormation, maintaining the required deployment pipeline.
- By checking both SCPs and permissions boundaries, this solution covers potential organizational and account-level restrictions that could impact access.

---

**tgv** (Mon 15 Jul 2024 20:08) - *Upvotes: 4*
---> D

---


<br/>

## Question 262

*Date: July 14, 2024, 5:53 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an organization in AWS Organizations for its multi-account environment. A DevOps engineer is developing an AWS CodeArtifact based strategy for application package management across the organization. Each application team at the company has its own account in the organization. Each application team also has limited access to a centralized shared services account.

Each application team needs full access to download, publish, and grant access to its own packages. Some common library packages that the application teams use must also be shared with the entire organization.


Which combination of steps will meet these requirements with the LEAST administrative overhead? (Choose three.)

**Options:**
- A. Create a domain in each application team's account. Grant each application team's account full read access and write access to the application team's domain.
- B. Create a domain in the shared services account. Grant the organization read access and CreateRepository access.
- C. Create a repository in each application team’s account. Grant each application team’s account full read access and write access to its own repository.
- D. Create a repository in the shared services account. Grant the organization read access to the repository in the shared services account Set the repository as the upstream repository in each application team's repository.
- E. For teams that require shared packages, create resource-based policies that allow read access to the repository from other application teams' accounts.
- F. Set the other application teams' repositories as upstream repositories.

> **Suggested Answer:** BCD
> **Community Vote:** BCD (76%), BDE (18%), 6%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Wed 31 Jul 2024 09:49) - *Upvotes: 5*
B: Establish a centralized domain in the shared services account and provide organizational access to common libraries.
D: Create a repository for common libraries in the shared services account, allow organization-wide read access, and configure upstream repositories.
C: Create individual repositories in each team’s account and grant full access to manage their own packages.

---

**jamesf** (Wed 31 Jul 2024 09:50) - *Upvotes: 1*
Keywords: LEAST Administrative overhead
Why not following options:
A: (Optional) Creating a domain in each team’s account if they need isolated domains (generally, a centralized domain in the shared services account might be more efficient).
E: (Optional) Configure resource-based policies for cross-account access if specific repositories need access by other teams' accounts beyond the shared services domain.
F: (Optional) Set other repositories as upstream if required, though this may be redundant if the shared services account repository is already upstream.

---

**0ac7838** (Wed 12 Nov 2025 09:01) - *Upvotes: 1*
The AWS standard answer for this is:

B – Central domain with org-wide CreateRepository.

D – Shared repo for common libraries (org read, upstream).

E – Teams can set resource policies to share their repos with other teams (fulfills “grant access to its own packages”).

Even though E adds some overhead, it’s the only option that allows teams to manage sharing of their own packages.

---

**limelight04** (Tue 27 Aug 2024 02:27) - *Upvotes: 1*
While Option D is a valid approach, it introduces additional complexity by requiring each application team to set up their repositories with the shared services account as an upstream repository. This can lead to more administrative overhead and potential misconfigurations.

In contrast, Options B, C, and E provide a simpler and more direct way to achieve the desired outcome. By centralizing the domain in the shared services account, granting organization-wide access, and allowing resource-based policies for shared packages, you can efficiently manage package distribution without relying on individual repository configurations.

---

**CHRIS12722222** (Tue 24 Dec 2024 20:08) - *Upvotes: 1*
"Some common library packages that the application teams use must also be shared with the entire organization."

looks like it is option D

---

**tgv** (Wed 17 Jul 2024 08:28) - *Upvotes: 4*
---> BCD

---

**TEC1** (Tue 16 Jul 2024 21:06) - *Upvotes: 2*
I will go with BDE

---

**trungtd** (Sun 14 Jul 2024 17:53) - *Upvotes: 4*
B allows for centralized control and management of common packages, and the organization can easily access and create repositories within this domain.
C ensures that each team has full control over their packages
D allows all teams to access common packages

---


<br/>

## Question 263

*Date: July 6, 2024, 11:30 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company deploys an application to Amazon EC2 instances. The application runs Amazon Linux 2 and uses AWS CodeDeploy. The application has the following file structure for its code repository:



The appspec.yml file has the following contents in the files section:



What will the result be for the deployment of the config.txt file?

**Options:**
- A. The config.txt file will be deployed to only /var/www/html/config/config.txt.
- B. The config.txt file will be deployed to /usr/local/src/config.txt and to /var/www/html/config/config.txt.
- C. The config.txt file will be deployed to only /usr/local/src/config.txt.
- D. The config.txt file will be deployed to /usr/local/src/config.txt and to /var/www/html/application/web/config.txt.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Moumita** (Sat 20 Jul 2024 16:22) - *Upvotes: 5*
First Entry:

source: config/config.txt
destination: /usr/local/src/config.txt
This rule specifically copies the config/config.txt file from the source repository to /usr/local/src/config.txt on the EC2 instance.

Second Entry:

source: /
destination: /var/www/html
This rule copies the entire contents of the root directory of the repository to /var/www/html on the EC2 instance. This includes the config/config.txt file, so it will be copied to /var/www/html/config/config.txt.

Given these two rules:

The config/config.txt file will be copied to /usr/local/src/config.txt by the first rule.
The same file will also be copied to /var/www/html/config/config.txt due to the second rule.
So, the correct answer is indeed B. The config.txt file will be deployed to /usr/local/src/config.txt and to /var/www/html/config/config.txt.

---

**jamesf** (Wed 31 Jul 2024 10:01) - *Upvotes: 4*
The config/config.txt file will be copied to /usr/local/src/config.txt based on the first mapping.
Since the entire source directory (/) is also mapped to /var/www/html, config/config.txt will also be copied to /var/www/html/config/config.txt.

---

**tgv** (Thu 18 Jul 2024 12:39) - *Upvotes: 4*
---> B

---


<br/>

## Question 264

*Date: July 6, 2024, 11:26 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has set up AWS CodeArtifact repositories with public upstream repositories. The company's development team consumes open source dependencies from the repositories in the company's internal network.

The company's security team recently discovered a critical vulnerability in the most recent version of a package that the development team consumes. The security team has produced a patched version to fix the vulnerability. The company needs to prevent the vulnerable version from being downloaded. The company also needs to allow the security team to publish the patched version.

Which combination of steps will meet these requirements? (Choose two.)

**Options:**
- A. Update the status of the affected CodeArtifact package version to unlisted.
- B. Update the status of the affected CodeArtifact package version to deleted.
- C. Update the status of the affected CodeArtifact package version to archived.
- D. Update the CodeArtifact package origin control settings to allow direct publishing and to block upstream operations.
- E. Update the CodeArtifact package origin control settings to block direct publishing and to allow upstream operations.

> **Suggested Answer:** CD
> **Community Vote:** CD (70%), BD (30%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**BvGVAXeAMP** (Tue 30 Jul 2024 00:06) - *Upvotes: 5*
A - unlisted does not prevent download
B - deleted is not a valid code artifact package version status
C- archived will prevent download

https://docs.aws.amazon.com/codeartifact/latest/ug/packages-overview.html#package-version-status

---

**Weninka** (Fri 19 Jul 2024 11:39) - *Upvotes: 5*
I had this question in my exam and checking what was the correct option for the package version led me here. C - archived seems to be the right one.
A - unlisted will only remove the package version from the list of versions returned to package managers, but it WILL NOT prevent the download.
B - deleted - it's not a valid package version status (https://docs.aws.amazon.com/codeartifact/latest/ug/packages-overview.html#package-version-status)
C - archived - will block the package version download.
D - Allow direct publishing will give the internal team permissions to upload the new version of the package
E - block direct publishing means the package version are updated from external (public) repos
More on the packages origin control settings here: https://docs.aws.amazon.com/codeartifact/latest/ug/package-origin-controls.html

---

**luisfsm_111** (Wed 18 Dec 2024 12:56) - *Upvotes: 1*
If there's a critical vulnerability, there's no reason to archive instead of deleting

https://docs.aws.amazon.com/codeartifact/latest/ug/delete-package.html

---

**aws_god** (Sat 14 Sep 2024 07:01) - *Upvotes: 3*
There is no delete version status - https://docs.aws.amazon.com/codeartifact/latest/ug/packages-overview.html#package-version-status

---

**ApacheKafkaAWS** (Thu 29 Aug 2024 16:09) - *Upvotes: 1*
you have to delete it not archive it

---

**limelight04** (Tue 27 Aug 2024 02:48) - *Upvotes: 1*
Option B: Update the status of the affected CodeArtifact package version to deleted. This action will prevent the vulnerable version from being accessible.
Option D: Update the CodeArtifact package origin control settings to allow direct publishing and block upstream operations. This ensures that only the security team can publish the patched version directly.

---

**jamesf** (Wed 31 Jul 2024 10:16) - *Upvotes: 3*
C. Update the status of the affected CodeArtifact package version to archived.
- Reason: Setting the package version status to Archived will prevent it from being downloaded while still retaining its metadata. This ensures that the vulnerable version cannot be accessed or used but allows you to track or potentially restore it later if needed.

D. Update the CodeArtifact package origin control settings to allow direct publishing and to block upstream operations.
- Reason: Allowing direct publishing and blocking upstream operations will enable the security team to publish the patched version directly to your repository without being blocked by upstream restrictions. This ensures that the patched version can be made available while preventing any interference from upstream repositories.

---

**jamesf** (Wed 31 Jul 2024 10:17) - *Upvotes: 1*
Why not B as "deleted" is not a valid code artifact package version status
https://docs.aws.amazon.com/codeartifact/latest/ug/packages-overview.html#package-version-status

---

**tgv** (Mon 15 Jul 2024 15:12) - *Upvotes: 1*
---> BD

---

**trungtd** (Sun 14 Jul 2024 18:19) - *Upvotes: 1*
By allowing direct publishing, the security team can publish the patched version directly to the CodeArtifact repository. Blocking upstream operations ensures that only the patched version is available and prevents the vulnerable version from being pulled from the upstream repository.

---


<br/>

## Question 265

*Date: July 15, 2024, 3:45 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company is running a custom-built application that processes records. All the components run on Amazon EC2 instances that run in an Auto Scaling group. Each record's processing is a multistep sequential action that is compute-intensive. Each step is always completed in 5 minutes or less.

A limitation of the current system is that if any steps fail, the application has to reprocess the record from the beginning. The company wants to update the architecture so that the application must reprocess only the failed steps.

What is the MOST operationally efficient solution that meets these requirements?

**Options:**
- A. Create a web application to write records to Amazon S3. Use S3 Event Notifications to publish to an Amazon Simple Notification Service (Amazon SNS) topic. Use an EC2 instance to poll Amazon SNS and start processing. Save intermediate results to Amazon S3 to pass on to the next step.
- B. Perform the processing steps by using logic in the application. Convert the application code to run in a container. Use AWS Fargate to manage the container instances. Configure the container to invoke itself to pass the state from one step to the next.
- C. Create a web application to pass records to an Amazon Kinesis data stream. Decouple the processing by using the Kinesis data stream and AWS Lambda functions.
- D. Create a web application to pass records to AWS Step Functions. Decouple the processing into Step Functions tasks and AWS Lambda functions.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Wed 31 Jul 2024 10:27) - *Upvotes: 2*
https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html

---

**jamesf** (Wed 31 Jul 2024 10:28) - *Upvotes: 1*
Operational Efficiency:
- AWS Step Functions abstracts much of the complexity involved in managing the state and flow of tasks. This leads to a more operationally efficient solution, as you don't have to manually handle retries, error states, or intermediate results.

---

**jamesf** (Wed 31 Jul 2024 10:28) - *Upvotes: 1*
State Management and Fault Tolerance:
- AWS Step Functions is a service that allows you to coordinate multiple AWS services into serverless workflows. It manages the state and execution of your tasks, ensuring that each step is completed in sequence.
If a step fails, Step Functions can retry the failed step or handle errors based on the defined retry policies or error catchers. This means that only the failed steps will be reprocessed, rather than starting from the beginning.

Decoupling and Scalability:
- Using AWS Lambda functions in combination with Step Functions allows you to run each step of the process as a separate Lambda function. This provides scalability and makes it easier to handle compute-intensive tasks as they can be distributed across multiple Lambda invocations.

---

**tgv** (Fri 19 Jul 2024 14:18) - *Upvotes: 2*
---> D

---

**trungtd** (Mon 15 Jul 2024 03:45) - *Upvotes: 3*
Step Functions and Lambda:
- Decoupling Tasks
- Error Handling and Retry Logic
- State Management

---


<br/>

## Question 266

*Date: July 6, 2024, 11:06 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company is migrating its on-premises Windows applications and Linux applications to AWS. The company will use automation to launch Amazon EC2 instances to mirror the on-premises configurations. The migrated applications require access to shared storage that uses SMB for Windows and NFS for Linux.

The company is also creating a pilot light disaster recovery (DR) environment in another AWS Region. The company will use automation to launch and configure the EC2 instances in the DR Region. The company needs to replicate the storage to the DR Region.

Which storage solution will meet these requirements?

**Options:**
- A. Use Amazon S3 for the application storage. Create an S3 bucket in the primary Region and an S3 bucket in the DR Region. Configure S3 Cross-Region Replication (CRR) from the primary Region to the DR Region.
- B. Use Amazon Elastic Block Store (Amazon EBS) for the application storage. Create a backup plan in AWS Backup that creates snapshots of the EBS volumes that are in the primary Region and replicates the snapshots to the DR Region.
- C. Use a Volume Gateway in AWS Storage Gateway for the application storage. Configure Cross-Region Replication (CRR) of the Volume Gateway from the primary Region to the DR Region.
- D. Use Amazon FSx for NetApp ONTAP for the application storage. Create an FSx for ONTAP instance in the DR Region. Configure NetApp SnapMirror replication from the primary Region to the DR Region.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Wed 31 Jul 2024 10:35) - *Upvotes: 4*
Amazon FSx for NetApp ONTAP support Multi-protocol access to data using the Network File System (NFS), Server Message Block (SMB), Internet Small Computer Systems Interface (iSCSI), and Non-Volatile Memory Express (NVMe) protocols
https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/what-is-fsx-ontap.html#features-overview

---

**tgv** (Fri 19 Jul 2024 14:18) - *Upvotes: 3*
---> D

---

**trungtd** (Mon 15 Jul 2024 03:50) - *Upvotes: 4*
NetApp ONTAP:
Multi-Protocol Support:
- SMB for Windows: Fully supports the SMB protocol required by your Windows applications.
- NFS for Linux: Fully supports the NFS protocol required by your Linux applications.

Cross-Region Replication:
NetApp SnapMirror: Provides efficient and reliable replication of data between ONTAP instances in different regions

---

**TEC1** (Wed 10 Jul 2024 22:26) - *Upvotes: 4*
Amazon FSx for NetApp ONTAP supports both SMB and NFS protocols, making it suitable for both Windows and Linux applications

---


<br/>

## Question 267

*Date: July 6, 2024, 10:41 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company's application uses a fleet of Amazon EC2 On-Demand Instances to analyze and process data. The EC2 instances are in an Auto Scaling group. The Auto Scaling group is a target group for an Application Load Balancer (ALB). The application analyzes critical data that cannot tolerate interruption. The application also analyzes noncritical data that can withstand interruption.

The critical data analysis requires quick scalability in response to real-time application demand. The noncritical data analysis involves memory consumption. A DevOps engineer must implement a solution that reduces scale-out latency for the critical data. The solution also must process the noncritical data.

Which combination of steps will meet these requirements? (Choose two.)

**Options:**
- A. For the critical data, modify the existing Auto Scaling group. Create a warm pool instance in the stopped state. Define the warm pool size. Create a new version of the launch template that has detailed monitoring enabled. Use Spot Instances.
- B. For the critical data, modify the existing Auto Scaling group. Create a warm pool instance in the stopped state. Define the warm pool size. Create a new version of the launch template that has detailed monitoring enabled. Use On-Demand Instances.
- C. For the critical data, modify the existing Auto Scaling group. Create a lifecycle hook to ensure that bootstrap scripts are completed successfully. Ensure that the application on the instances is ready to accept traffic before the instances are registered. Create a new version of the launch template that has detailed monitoring enabled.
- D. For the noncritical data, create a second Auto Scaling group that uses a launch template. Configure the launch template to install the unified Amazon CloudWatch agent and to configure the CloudWatch agent with a custom memory utilization metric. Use Spot Instances. Add the new Auto Scaling group as the target group for the ALB. Modify the application to use two target groups for critical data and noncritical data.
- E. For the noncritical data, create a second Auto Scaling group. Choose the predefined memory utilization metric type for the target tracking scaling policy. Use Spot Instances. Add the new Auto Scaling group as the target group for the ALB. Modify the application to use two target groups for critical data and noncritical data.

> **Suggested Answer:** BD
> **Community Vote:** BD (87%), 7%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**trungtd** (Mon 15 Jul 2024 04:00) - *Upvotes: 5*
AWS Auto Scaling does not provide a predefined memory utilization metric type

---

**Rs123x** (Thu 24 Apr 2025 21:38) - *Upvotes: 1*
This combination ensures quick scaling and uninterrupted processing for critical data and cost-efficient, memory-optimized scaling for noncritical data.

---

**jamesf** (Wed 31 Jul 2024 10:41) - *Upvotes: 4*
Option B (For critical data): Creates a warm pool and ensures quick scaling with On-Demand Instances, addressing the need for low latency in scaling.
Option D (For noncritical data): Uses Spot Instances with memory-based scaling policies to handle noncritical data efficiently.

---

**jamesf** (Mon 05 Aug 2024 08:25) - *Upvotes: 1*
For D, Spot Instance, Using Cloudwatch with Custom Memory Utilization Metric
https://aws.amazon.com/blogs/mt/create-amazon-ec2-auto-scaling-policy-memory-utilization-metric-linux/

Not E as Auto Scaling does not provide predefined memory utilization.

---

**tgv** (Fri 19 Jul 2024 14:19) - *Upvotes: 4*
---> B D

---

**TEC1** (Wed 10 Jul 2024 22:37) - *Upvotes: 1*
On-Demand Instances: For critical data that cannot tolerate interruption, On-Demand Instances are reliable and provide the required stability without the risk of termination

Spot Instances: Utilising Spot Instances for noncritical data processing can significantly reduce costs since these workloads can tolerate interruptions.

This combination ensures that the critical data analysis benefits from reduced scale-out latency and reliability, while noncritical data processing leverages cost-effective Spot Instances and is scaled based on memory usage.

---


<br/>

## Question 268

*Date: July 6, 2024, 10:38 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company recently migrated its application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that uses Amazon EC2 instances. The company configured the application to automatically scale based on CPU utilization.

The application produces memory errors when it experiences heavy loads. The application also does not scale out enough to handle the increased load. The company needs to collect and analyze memory metrics for the application over time.

Which combination of steps will meet these requirements? (Choose three.)

**Options:**
- A. Attach the CloudWatchAgentServerPolicy managed IAM policy to the IAM instance profile that the cluster uses.
- B. Attach the CloudWatchAgentServerPolicy managed IAM policy to a service account role for the cluster.
- C. Collect performance metrics by deploying the unified Amazon CloudWatch agent to the existing EC2 instances in the cluster. Add the agent to the AMI for any new EC2 instances that are added to the cluster.
- D. Collect performance logs by deploying the AWS Distro for OpenTelemetry collector as a DaemonSet.
- E. Analyze the pod_memory_utilization Amazon CloudWatch metric in the ContainerInsights namespace by using the Service dimension.
- F. Analyze the node_memory_utilization Amazon CloudWatch metric in the ContainerInsights namespace by using the ClusterName dimension.

> **Suggested Answer:** ACE
> **Community Vote:** ACE (82%), Other, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**trungtd** (Mon 15 Jul 2024 04:12) - *Upvotes: 9*
A. This policy grants the necessary permissions for the Amazon CloudWatch agent to collect and publish metrics from the EC2 instances.
C. The unified Amazon CloudWatch agent can collect both CPU and memory utilization metrics. Deploying it ensures you capture memory metrics across all EC2 instances in the EKS cluster.
E. pod_memory_utilization metric provides detailed insights into memory usage at the pod level

B. service account role is more relevant for applications running within Kubernetes pods needing AWS permissions.
D irrelevant
F Node-level metrics do not provide the granularity needed to diagnose pod-level memory issues effectively

---

**nickp84** (Thu 22 May 2025 21:25) - *Upvotes: 1*
D. Collect performance logs by deploying the AWS Distro for OpenTelemetry collector as a DaemonSet.
The AWS Distro for OpenTelemetry (ADOT) collector is typically used for collecting traces, metrics, and logs from applications, often at the application or pod level, and is deployed as a Kubernetes workload (e.g., DaemonSet). While ADOT can collect certain metrics, it is primarily focused on application-level telemetry (e.g., custom metrics or traces) rather than system-level memory metrics for EC2 instances. The requirement emphasizes memory metrics, which are more effectively collected by the CloudWatch agent at the node level for EC2-based EKS clusters.

Not Relevant: ADOT is less suited for collecting system-level memory metrics compared to the CloudWatch agent.

---

**jamesf** (Wed 31 Jul 2024 11:01) - *Upvotes: 4*
After check, feel Option A better
- provides necessary permissions at the EC2 instance level, which is where the CloudWatch agent runs.
- is directly suitable for metrics collection because it ensures that EC2 instances can send metrics to CloudWatch. The CloudWatch agent on the EC2 instances needs the IAM policy to push metrics and logs to CloudWatch.

Hope someone can explain further if choose option B instead of A.

---

**jamesf** (Wed 07 Aug 2024 03:47) - *Upvotes: 1*
it seen BCE better as option B
- attached the policy to EKS service account role for better Granular Control, Scalability and Management
- this policy targets permissions at the Kubernetes level, granting specific pods or services within the cluster the ability to collect and send metrics.

---

**noisonnoiton** (Thu 18 Jul 2024 03:18) - *Upvotes: 1*
B - control permission with service account
C - cloudwatch agent on k8s worker nodes
E - monitoring with k8s service (pods)

---

**TEC1** (Tue 16 Jul 2024 21:50) - *Upvotes: 1*
I will go with B C E

---

**TEC1** (Tue 16 Jul 2024 21:53) - *Upvotes: 1*
B- Necessary permissions
C- Cloud watch agent installed
E - understanding performance and scaling of the application within Kubernetes Enviro

---

**komorebi** (Sat 13 Jul 2024 01:14) - *Upvotes: 1*
Answer : C E F

---


<br/>

## Question 269

*Date: July 6, 2024, 10:15 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company's video streaming platform usage has increased from 10,000 users each day to 50,000 users each day in multiple countries. The company deploys the streaming platform on Amazon Elastic Kubernetes Service (Amazon EKS). The EKS workload scales up to thousands of nodes during peak viewing time.

The company's users report occurrences of unauthorized logins. Users also report sudden interruptions and logouts from the platform.

The company wants additional security measures for the entire platform. The company also needs a summarized view of the resource behaviors and interactions across the company's entire AWS environment. The summarized view must show login attempts, API calls, and network traffic. The solution must permit network traffic analysis while minimizing the overhead of managing logs. The solution must also quickly investigate any potential malicious behavior that is associated with the EKS workload.

Which solution will meet these requirements?

**Options:**
- A. Enable Amazon GuardDuty for EKS Audit Log Monitoring. Enable AWS CloudTrail logs. Store the EKS audit logs and CloudTrail log files in an Amazon S3 bucket. Use Amazon Athena to create an external table. Use Amazon QuickSight to create a dashboard.
- B. Enable Amazon GuardDuty for EKS Audit Log Monitoring. Enable Amazon Detective in the company's AWS account. Enable EKS audit logs from optional source packages in Detective.
- C. Enable Amazon CloudWatch Container Insights. Enable AWS CloudTrail logs. Store the EKS audit logs and CloudTrail log files in an Amazon S3 bucket. Use Amazon Athena to create an external table. Use Amazon QuickSight to create a dashboard.
- D. Enable Amazon GuardDuty for EKS Audit Log Monitoring. Enable Amazon CloudWatch Container Insights and VPC Flow Logs. Enable AWS CloudTrail logs.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Wed 31 Jul 2024 11:05) - *Upvotes: 2*
Amazon Detective helps you quickly analyze and investigate security events across one or more AWS accounts by generating data visualizations that represent the ways your resources behave and interact over time. Detective creates visualizations of GuardDuty findings.
https://docs.aws.amazon.com/guardduty/latest/ug/detective-integration.html

Amazon EKS audit logs is an optional data source package that can be added to your Detective behavior graph.
https://docs.aws.amazon.com/detective/latest/userguide/source-data-types-EKS.html

---

**TEC1** (Tue 16 Jul 2024 22:02) - *Upvotes: 2*
B- Guardduty any potential malicious behavior and Amazon Detective summarised view must show login attempts, API calls, and network traffic

---

**trungtd** (Mon 15 Jul 2024 04:34) - *Upvotes: 3*
https://docs.aws.amazon.com/guardduty/latest/ug/detective-integration.html
https://docs.aws.amazon.com/detective/latest/userguide/source-data-types-EKS.html

---

**siheom** (Fri 12 Jul 2024 04:59) - *Upvotes: 2*
vote B

---

**getadroit** (Sun 07 Jul 2024 00:04) - *Upvotes: 3*
D
https://aws.amazon.com/blogs/security/how-to-use-new-amazon-guardduty-eks-protection-findings/

---


<br/>

## Question 270

*Date: July 6, 2024, 9:46 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses AWS Organizations to manage hundreds of AWS accounts. The company has a team that is responsible for AWS Identity and Access Management (IAM).

The IAM team wants to implement AWS IAM Identity Center (AWS Single Sign-On). The IAM team must have only the minimum needed permissions to manage IAM Identity Center. The IAM team must not be able to gain unneeded access to the Organizations management account. The IAM team must be able to provision new IAM Identity Center permission sets and assignments for existing and new member accounts.

Which combination of steps will meet these requirements? (Choose three.)

**Options:**
- A. Create a new AWS account for the IAM team. In the new account, enable IAM Identity Center. In the Organizations management account, register the new account as a delegated administrator for IAM Identity Center.
- B. Create a new AWS account for the IAM team. In the Organizations management account, enable IAM Identity Center. In the Organizations management account, register the new account as a delegated administrator for IAM Identity Center.
- C. In IAM Identity Center, create users and a group for the IAM team. Add the users to the group. Create a new permission set. Attach the AWSSSODirectoryAdministrator managed IAM policy to the group.
- D. In IAM Identity Center, create users and a group for the IAM team. Add the users to the group. Create a new permission set. Attach the AWSSSOMemberAccountAdministrator managed IAM policy to the group.
- E. Assign the permission set to the Organizations management account. Allow the IAM team group to use the permission set.
- F. Assign the permission set to the new AWS account. Allow the IAM team group to use the permission set.

> **Suggested Answer:** BDF
> **Community Vote:** BDF (42%), ADF (40%), Other, Other, A (35%), C (25%), B (20%), Other

### Discussions

**Shenannigan** (Thu 12 Sep 2024 22:04) - *Upvotes: 8*
For B see this: https://aws.amazon.com/blogs/security/getting-started-with-aws-sso-delegated-administration/
For D: compare the two policies AWSSSODirectoryAdministrator does not grant managment of permission sets
F: makes sure that the IAM team has the necessary permissions within their designated account

---

**trungtd** (Mon 15 Jul 2024 04:44) - *Upvotes: 7*
A ensures that the IAM team operates within their own account, isolating their permissions and activities from the Organizations management account.
D provides the IAM team with the necessary permissions to manage IAM Identity Center across member accounts, without granting broader access.
*Note that AWSSSODirectoryAdministrator policy grants broader permissions than necessary
F. ensures that the IAM team has the necessary permissions within their designated account

B. "The IAM team must not be able to gain unneeded access to the Organizations management account" => So B is wrong
C contradicting the principle of least privilege.
E should be avoided to prevent the IAM team from gaining unneeded access.

---

**ryuhei** (Mon 25 Aug 2025 12:21) - *Upvotes: 1*
A, D, F are optimal because:A: Creates a new account for IAM Identity Center, enabling it and registering it as a delegated administrator, isolating management from the management account.
D: Attaches AWSSSOMemberAccountAdministrator policy to the IAM team, granting minimal permissions for provisioning permission sets and assignments.
F: Assigns permission sets to the new account, allowing the IAM team to manage assignments for member accounts without accessing the management account.

---

**AWSLoverLoverLoverLoverLover** (Sat 24 May 2025 10:36) - *Upvotes: 1*
Selected Answer: BDF

---

**AWSLoverLoverLoverLoverLover** (Thu 22 May 2025 12:40) - *Upvotes: 1*
BDF are correct

---

**luisfsm_111** (Wed 18 Dec 2024 13:05) - *Upvotes: 2*
Really helpful

https://www.youtube.com/watch?v=aXqRKlvK160

---

**CHRIS12722222** (Wed 25 Dec 2024 00:24) - *Upvotes: 1*
The video suggests it is B, as identity centre was already enabled before delegating to the new account

---

**teo2157** (Fri 13 Dec 2024 09:56) - *Upvotes: 6*
B. It's required to enable AWS IAM Identity Center in the Management Account and later delegate administration of AWS IAM Identity Center to a specific member account where the IAM team operates.
D. To meet the requirements of the IAM team needing to provision new IAM Identity Center permission sets and assignments for existing and new member accounts, while ensuring they do not gain unneeded access to the Organizations management account, you should use the `AWSSSOMemberAccountAdministrator` policy. This policy provides the necessary permissions to manage AWS SSO settings and assignments within member accounts without granting full administrative access to the AWS SSO directory.
F. The IAM roles or users must be created in the delegated member account for the IAM team to prevent the IAM team from gaining unneeded access.

---

**tinyshare** (Fri 15 Nov 2024 03:04) - *Upvotes: 4*
B D F is correct.
You enable the Identity Center in the management account first, not in the new account.
For assignment, you need AWSSSOMemberAccountAdministrator, not AWSSSODirectoryAdministrator.

---

**tinyshare** (Fri 15 Nov 2024 03:08) - *Upvotes: 1*
For B: https://aws.amazon.com/blogs/security/getting-started-with-aws-sso-delegated-administration/
For D: https://docs.aws.amazon.com/singlesignon/latest/userguide/security-iam-awsmanpol.html

---


<br/>

## Question 271

*Date: July 6, 2024, 6:16 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses an organization in AWS Organizations that has all features enabled. The company uses AWS Backup in a primary account and uses an AWS Key Management Service (AWS KMS) key to encrypt the backups.

The company needs to automate a cross-account backup of the resources that AWS Backup backs up in the primary account. The company configures cross-account backup in the Organizations management account. The company creates a new AWS account in the organization and configures an AWS Backup backup vault in the new account. The company creates a KMS key in the new account to encrypt the backups. Finally, the company configures a new backup plan in the primary account. The destination for the new backup plan is the backup vault in the new account.

When the AWS Backup job in the primary account is invoked, the job creates backups in the primary account. However, the backups are not copied to the new account's backup vault.

Which combination of steps must the company take so that backups can be copied to the new account's backup vault? (Choose two.)

**Options:**
- A. Edit the backup vault access policy in the new account to allow access to the primary account.
- B. Edit the backup vault access policy in the primary account to allow access to the new account.
- C. Edit the backup vault access policy in the primary account to allow access to the KMS key in the new account.
- D. Edit the key policy of the KMS key in the primary account to share the key with the new account.
- E. Edit the key policy of the KMS key in the new account to share the key with the primary account.

> **Suggested Answer:** AD
> **Community Vote:** AD (73%), AE (27%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**auxwww** (Wed 31 Jul 2024 18:56) - *Upvotes: 8*
https://docs.aws.amazon.com/aws-backup/latest/devguide/create-cross-account-backup.html

In your destination account, you must create a backup vault. Then, you assign a customer managed key to encrypt backups in the destination account, and a resource-based access policy to allow AWS Backup to access the resources you would like to copy. In the source account, if your resources are encrypted with a customer managed key, you must share this customer managed key with the destination account. You can then create a backup plan and choose a destination account that is part of your organizational unit in AWS Organizations.

---

**xdkonorek2** (Sat 06 Jul 2024 18:16) - *Upvotes: 5*
backup a backup using aws backup to backup account :)
AD
second paragraph: https://docs.aws.amazon.com/aws-backup/latest/devguide/create-cross-account-backup.html

---

**teo2157** (Thu 16 Jan 2025 07:59) - *Upvotes: 2*
@auxwww expplanation is perfect

---

**luisfsm_111** (Mon 09 Dec 2024 13:07) - *Upvotes: 1*
In my view, D is not needed because the backups in the new account will use the KMS key in the new account, not the primary account’s key.

---

**auxwww** (Fri 18 Oct 2024 02:03) - *Upvotes: 3*
A,D - Correct

"n your destination account, you must create a backup vault. Then, you assign a customer managed key to encrypt backups in the destination account, and a resource-based access policy to allow AWS Backup to access the resources you would like to copy. In the source account, if your resources are encrypted with a customer managed key, you must share this customer managed key with the destination account. You can then create a backup plan and choose a destination account that is part of your organizational unit in AWS Organizations."

---

**limelight04** (Wed 28 Aug 2024 00:25) - *Upvotes: 2*
Option A: Edit the backup vault access policy in the new account to allow access to the primary account. This step ensures that the primary account has the necessary permissions to copy backups into the new account’s backup vault.

Option E: Edit the key policy of the KMS key in the new account to share the key with the primary account. This step allows the primary account to use the KMS key in the new account for encryption during the backup copy process

---

**jamesf** (Wed 31 Jul 2024 11:35) - *Upvotes: 2*
I prefer AE as
1. the company need cross-account backup but not cross-account copy.
2. And the KMS key created in new account for backup encryption.
highlighted keys:
- The company configures cross-account backup in the Organizations management account.
- The company creates a new AWS account in the organization and configures an AWS Backup backup vault in the new account.
- The company creates a KMS key in the new account to encrypt the backups.
- Finally, the company configures a new backup plan in the primary account.
- The destination for the new backup plan is the backup vault in the new account.

---

**jamesf** (Mon 05 Aug 2024 09:33) - *Upvotes: 1*
Seen like D correct
- For the resources that aren't fully managed by AWS Backup, the backups use the same KMS key as the source resource.
- For the resources that are fully managed by AWS Backup, the backups are encrypted with encryption key of the backup vault.
https://repost.aws/knowledge-center/backup-troubleshoot-cross-account-copy

---

**jamesf** (Wed 31 Jul 2024 11:45) - *Upvotes: 1*
A. Edit the backup vault access policy in the new account to allow access from the primary account.
E. Edit the key policy of the KMS key in the new account to share the key with the primary account.

Backup Plan and Resource located in Management Account.
Backup Vault and KMS Key located in new account.

Based on URLs below, still confusing as the KMS key in new account (Destination) already
https://docs.aws.amazon.com/aws-backup/latest/devguide/create-cross-account-backup.html
https://repost.aws/knowledge-center/backup-troubleshoot-cross-account-copy

Hope someone choose option D can explain further why option D but not E.

---

**d9iceguy** (Tue 23 Jul 2024 18:34) - *Upvotes: 5*
D - https://docs.aws.amazon.com/aws-backup/latest/devguide/create-cross-account-backup.html#backup-cab-encryption
During a cross-account copy, the source account KMS key policy must allow the destination account on the KMS key policy.

---


<br/>

## Question 272

*Date: July 6, 2024, 2:37 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company runs an application that uses an Amazon S3 bucket to store images. A DevOps engineer needs to implement a multi-Region strategy for the objects that are stored in the S3 bucket. The company needs to be able to fail over to an S3 bucket in another AWS Region. When an image is added to either S3 bucket, the image must be replicated to the other S3 bucket within 15 minutes.

The DevOps engineer enables two-way replication between the S3 buckets.

Which combination of steps should the DevOps engineer take next to meet the requirements? (Choose three.)

**Options:**
- A. Enable S3 Replication Time Control (S3 RTC) on each replication rule.
- B. Create an S3 Multi-Region Access Point in an active-passive configuration.
- C. Call the SubmitMultiRegionAccessPointRoutes operation in the AWS API when the company needs to fail over to the S3 bucket in the other Region.
- D. Enable S3 Transfer Acceleration on both S3 buckets.
- E. Configure a routing control in Amazon Route 53 Recovery Controller. Add the S3 buckets in an active-passive configuration.
- F. Call the UpdateRoutingControlStates operation in the AWS API when the company needs to fail over to the S3 bucket in the other Region.

> **Suggested Answer:** ABC
> **Community Vote:** ABC (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**DKM** (Tue 18 Mar 2025 19:03) - *Upvotes: 1*
aws s3control submit-multi-region-access-point-routes \
--account-id <account-id> \
--mrap <multi-region-access-point-arn> \
--route-updates '[{"Bucket":"<bucket-name>","Region":"<region-name>","TrafficDialPercentage":100},{"Bucket":"<bucket-name>","Region":"<region-name>","TrafficDialPercentage":0}]'

---

**jamesf** (Wed 31 Jul 2024 14:32) - *Upvotes: 2*
https://aws.amazon.com/getting-started/hands-on/getting-started-with-amazon-s3-multi-region-access-points/

---

**trungtd** (Mon 15 Jul 2024 05:23) - *Upvotes: 3*
UpdateRoutingControlStates is for Route53

---

**TEC1** (Wed 10 Jul 2024 22:02) - *Upvotes: 4*
A. Enable S3 Replication Time Control (S3 RTC) on each replication rule. This ensures that 99.99% of objects are replicated within 15 minutes.

B. Create an S3 Multi-Region Access Point in an active-passive configuration. This is crucial for managing access to data across multiple S3 buckets in different AWS Regions and facilitating failover.

C. Call the SubmitMultiRegionAccessPointRoutes operation in the AWS API when the company needs to fail over to the S3 bucket in the other Region. This operation updates the routes to direct traffic to the designated S3 bucket, enabling failover.

---

**getadroit** (Sun 07 Jul 2024 00:22) - *Upvotes: 4*
ABC
https://aws.amazon.com/getting-started/hands-on/getting-started-with-amazon-s3-multi-region-access-points/?ref=docs_gateway/amazons3/MultiRegionAccessPoints.html

---


<br/>

## Question 273

*Date: July 6, 2024, 1:32 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses the AWS Cloud Development Kit (AWS CDK) to define its application. The company uses a pipeline that consists of AWS CodePipeline and AWS CodeBuild to deploy the CDK application.

The company wants to introduce unit tests to the pipeline to test various infrastructure components. The company wants to ensure that a deployment proceeds if no unit tests result in a failure.

Which combination of steps will enforce the testing requirement in the pipeline? (Choose two.)

**Options:**
- A. Update the CodeBuild build phase commands to run the tests then to deploy the application. Set the OnFailure phase property to ABORT.
- B. Update the CodeBuild build phase commands to run the tests then to deploy the application. Add the --rollback true flag to the cdk deploy command.
- C. Update the CodeBuild build phase commands to run the tests then to deploy the application. Add the --require-approval any-change flag to the cdk deploy command.
- D. Create a test that uses the AWS CDK assertions module. Use the template.hasResourceProperties assertion to test that resources have the expected properties.
- E. Create a test that uses the cdk diff command. Configure the test to fail if any resources have changed.

> **Suggested Answer:** AD
> **Community Vote:** AD (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Wed 07 Aug 2024 04:18) - *Upvotes: 3*
Option A: <OnFailure phase>
- This option involves configuring the build phase in CodeBuild to run tests before deployment. Setting the OnFailure phase property to ABORT ensures that if any test fails, the build process is stopped, and the deployment doesn't proceed.
- This ensures that the build and deployment process only proceeds if the tests are successful, which is crucial for maintaining application integrity.

Option D: <CDK assertions>
- This option involves writing unit tests using the AWS CDK assertions module. The template.hasResourceProperties assertion checks if CloudFormation templates have resources with expected properties, ensuring the infrastructure's logical correctness.
- This enforces unit testing by checking that the CloudFormation templates have the expected properties, ensuring the application’s infrastructure is as intended.

---

**trungtd** (Mon 15 Jul 2024 05:31) - *Upvotes: 3*
I'm not sure but A D seems right

---

**TEC1** (Wed 10 Jul 2024 22:06) - *Upvotes: 3*
A. This step is crucial because:

It integrates the unit tests into the build phase of CodeBuild.
By setting the OnFailure property to ABORT, it ensures that the pipeline stops if any tests fail, preventing deployment of potentially faulty infrastructure.
If all tests pass, the deployment will proceed as normal.

D. This step is important because:

It utilizes the AWS CDK assertions module, which is specifically designed for testing CDK applications.
The template.hasResourceProperties assertion allows you to verify that the resources defined in your CDK code have the expected properties.
This type of test can catch issues in your infrastructure definition before deployment.

---


<br/>

## Question 274

*Date: July 6, 2024, 1:19 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an application that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2 instances are in multiple Availability Zones. The application was misconfigured in a single Availability Zone, which caused a partial outage of the application.

A DevOps engineer made changes to ensure that the unhealthy EC2 instances in one Availability Zone do not affect the healthy EC2 instances in the other Availability Zones. The DevOps engineer needs to test the application's failover and shift where the ALB sends traffic. During failover, the ALB must avoid sending traffic to the Availability Zone where the failure has occurred.

Which solution will meet these requirements?

**Options:**
- A. Turn off cross-zone load balancing on the ALB. Use Amazon Route 53 Application Recovery Controller to start a zonal shift away from the Availability Zone.
- B. Turn off cross-zone load balancing on the ALB’s target group. Use Amazon Route 53 Application Recovery Controller to start a zonal shift away from the Availability Zone.
- C. Create an Amazon Route 53 Application Recovery Controller resource set that uses the DNS hostname of the ALB. Start a zonal shift for the resource set away from the Availability Zone.
- D. Create an Amazon Route 53 Application Recovery Controller resource set that uses the ARN of the ALB’s target group. Create a readiness check that uses the ElbV2TargetGroupsCanServeTraffic rule.

> **Suggested Answer:** A
> **Community Vote:** A (48%), B (41%), 10%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**inturist** (Tue 16 Jul 2024 13:51) - *Upvotes: 7*
For me the correct answer is A:"Note that the Elastic Load Balancing resources must have cross-zone load balancing turned off to use this capability."
https://docs.aws.amazon.com/r53recovery/latest/dg/arc-zonal-shift.html

---

**Exto1124** (Sun 28 Jul 2024 16:35) - *Upvotes: 1*
"With Application Load Balancers, cross-zone load balancing is always turned on at the load balancer level, and cannot be turned off. For target groups, the default is to use the load balancer setting, but you can override the default by explicitly turning cross-zone load balancing off at the target group level."
https://docs.aws.amazon.com/elasticloadbalancing/latest/application/disable-cross-zone.html#:~:text=The%20nodes%20for%20your%20load,in%20all%20registered%20Availability%20Zones.

---

**elivan_coelho** (Wed 29 Oct 2025 16:19) - *Upvotes: 1*
For me the correct answer is B:
"You can turn on cross-zone load balancing for your Application Load Balancer target groups at any time. The cross-zone load balancing setting at the target group level overrides the setting at the load balancer level."
B is corret

https://docs.aws.amazon.com/elasticloadbalancing/latest/application/edit-target-group-attributes.html#cross_zone_console_disable

---

**nickp84** (Mon 19 May 2025 20:35) - *Upvotes: 1*
The statement is incorrect because cross-zone load balancing for an Application Load Balancer (ALB) is configured at the ALB level, not the target group level. To meet the requirements from the previous question:
Disable cross-zone load balancing on the ALB using the load_balancing.cross_zone.enabled=false attribute to isolate traffic to each AZ, ensuring unhealthy instances in one AZ do not receive traffic (assuming health checks mark them unhealthy).

Use Amazon Route 53 Application Recovery Controller to start a zonal shift to redirect ALB traffic away from the affected AZ during failover testing.

The correct solution remains Option A

---

**Rs123x** (Thu 24 Apr 2025 22:00) - *Upvotes: 1*
This solution leverages Amazon Route 53 Application Recovery Controller to initiate a zonal shift and ensures that traffic is redirected away from the failing Availability Zone while keeping the ALB in operation.

---

**jojewi8143** (Sun 02 Feb 2025 12:05) - *Upvotes: 1*
i choose a

---

**CHRIS12722222** (Wed 25 Dec 2024 12:04) - *Upvotes: 4*
Cant turn off cross zone load balancing at ALB level

---

**spring21** (Mon 16 Dec 2024 19:03) - *Upvotes: 2*
To turn off cross-zone load balancing on an Application Load Balancer (ALB), you cannot directly disable it at the ALB level; instead, you need to modify the target group settings and explicitly disable cross-zone load balancing within the target group attributes

---

**Shenannigan** (Sat 14 Sep 2024 08:50) - *Upvotes: 4*
Answer is B
You can not turn of cross-zone load balancing on an ALB: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/application-load-balancers.html#cross-zone-load-balancing

You can turn it off at the Target Group: https://aws.amazon.com/about-aws/whats-new/2022/11/application-load-balancers-turning-off-cross-zone-load-balancing-per-target-group/

zonal shift:
https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html#cross-zone-load-balancing#:~:text=zonal%20shift

---

**limelight04** (Wed 28 Aug 2024 00:45) - *Upvotes: 2*
A is the most appropriate answer due to the reasons below;

Option A: Turning off cross-zone load balancing on the ALB and using Amazon Route 53 Application Recovery Controller to start a zonal shift away from the affected Availability Zone ensures that traffic is not sent to the unhealthy instances in the problematic zone. This directly addresses the need to avoid sending traffic to the Availability Zone where the failure has occurred.

Option D: While creating a resource set and readiness check with Amazon Route 53 Application Recovery Controller is useful for monitoring and ensuring traffic is routed to healthy instances, it doesn’t explicitly mention turning off cross-zone load balancing, which is crucial for isolating the affected Availability Zone.

---


<br/>

## Question 275

*Date: July 6, 2024, 1:08 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company sends its AWS Network Firewall flow logs to an Amazon S3 bucket. The company then analyzes the flow logs by using Amazon Athena.

The company needs to transform the flow logs and add additional data before the flow logs are delivered to the existing S3 bucket.

Which solution will meet these requirements?

**Options:**
- A. Create an AWS Lambda function to transform the data and to write a new object to the existing S3 bucket. Configure the Lambda function with an S3 trigger for the existing S3 bucket. Specify all object create events for the event type. Acknowledge the recursive invocation.
- B. Enable Amazon EventBridge notifications on the existing S3 bucket. Create a custom EventBridge event bus. Create an EventBridge rule that is associated with the custom event bus. Configure the rule to react to all object create events for the existing S3 bucket and to invoke an AWS Step Functions workflow. Configure a Step Functions task to transform the data and to write the data into a new S3 bucket.
- C. Create an Amazon EventBridge rule that is associated with the default EventBridge event bus. Configure the rule to react to all object create events for the existing S3 bucket. Define a new S3 bucket as the target for the rule. Create an EventBridge input transformation to customize the event before passing the event to the rule target.
- D. Create an Amazon Kinesis Data Firehose delivery stream that is configured with an AWS Lambda transformer. Specify the existing S3 bucket as the destination. Change the Network Firewall logging destination from Amazon S3 to Kinesis Data Firehose.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jamesf** (Wed 31 Jul 2024 15:06) - *Upvotes: 4*
Amazon Kinesis Data Firehose:
Kinesis Data Firehose is designed for real-time streaming data delivery and transformation. It can ingest data, process it with a Lambda function, and deliver the transformed data to destinations like Amazon S3, Redshift, or Elasticsearch.
https://aws.amazon.com/firehose/faqs/

AWS Lambda Transformer:
By configuring a Lambda function as a transformer within Kinesis Data Firehose, you can implement custom logic to transform the flow logs and add any additional data required before the logs are written to the existing S3 bucket.

---

**jamesf** (Wed 31 Jul 2024 15:07) - *Upvotes: 3*
keywords: transform, flow logs

---

**d9iceguy** (Tue 23 Jul 2024 19:11) - *Upvotes: 3*
D for me

---

**trungtd** (Mon 15 Jul 2024 05:49) - *Upvotes: 4*
D for me

---

**getadroit** (Sun 07 Jul 2024 00:31) - *Upvotes: 3*
Dhttps://aws.amazon.com/firehose/faqs/

---


<br/>

## Question 276

*Date: July 6, 2024, 1:04 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer needs to implement integration tests into an existing AWS CodePipeline CI/CD workflow for an Amazon Elastic Container Service (Amazon ECS) service. The CI/CD workflow retrieves new application code from an AWS CodeCommit repository and builds a container image. The Cl/CD workflow then uploads the container image to Amazon Elastic Container Registry (Amazon ECR) with a new image tag version.

The integration tests must ensure that new versions of the service endpoint are reachable and that various API methods return successful response data. The DevOps engineer has already created an ECS cluster to test the service.

Which combination of steps will meet these requirements with the LEAST management overhead? (Choose three.)

**Options:**
- A. Add a deploy stage to the pipeline. Configure Amazon ECS as the action provider.
- B. Add a deploy stage to the pipeline. Configure AWS CodeDeploy as the action provider.
- C. Add an appspec.yml file to the CodeCommit repository.
- D. Update the image build pipeline stage to output an imagedefinitions.json file that references the new image tag.
- E. Create an AWS Lambda function that runs connectivity checks and API calls against the service. Integrate the Lambda function with CodePipeline by using a Lambda action stage.
- F. Write a script that runs integration tests against the service. Upload the script to an Amazon S3 bucket. Integrate the script in the S3 bucket with CodePipeline by using an S3 action stage.

> **Suggested Answer:** ADE
> **Community Vote:** ADE (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**xdkonorek2** (Sat 06 Jul 2024 13:04) - *Upvotes: 7*
ADE
https://docs.aws.amazon.com/codepipeline/latest/userguide/ecs-cd-pipeline.html

---

**youonebe** (Fri 27 Dec 2024 04:19) - *Upvotes: 2*
ADE for sure

---

**jamesf** (Wed 07 Aug 2024 04:25) - *Upvotes: 3*
keywords: LEAST management overhead

A. Add a deploy stage to the pipeline. Configure Amazon ECS as the action provider.
- Directly deploys the container image to ECS, ensuring the service is updated with the latest code without unnecessary complexity.

D. Update the image build pipeline stage to output an image definitions.json file that references the new image tag.
- Necessary for ECS to recognize and deploy the new image version, facilitating automated updates.

E. Create an AWS Lambda function that runs connectivity checks and API calls against the service. Integrate the Lambda function with CodePipeline by using a Lambda action stage.
- Provides a low-management solution for running integration tests, leveraging AWS Lambda's serverless capabilities.

---


<br/>

## Question 277

*Date: July 6, 2024, 12:59 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company runs applications on Windows and Linux Amazon EC2 instances. The instances run across multiple Availability Zones in an AWS Region. The company uses Auto Scaling groups for each application.

The company needs a durable storage solution for the instances. The solution must use SMB for Windows and must use NFS for Linux. The solution must also have sub-millisecond latencies. All instances will read and write the data.

Which combination of steps will meet these requirements? (Choose three.)

**Options:**
- A. Create an Amazon Elastic File System (Amazon EFS) file system that has targets in multiple Availability Zones.
- B. Create an Amazon FSx for NetApp ONTAP Multi-AZ file system.
- C. Create a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume to use for shared storage.
- D. Update the user data for each application’s launch template to mount the file system.
- E. Perform an instance refresh on each Auto Scaling group.
- F. Update the EC2 instances for each application to mount the file system when new instances are launched.

> **Suggested Answer:** BDE
> **Community Vote:** BDE (81%), BDF (19%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**raycomh** (Sun 14 Jul 2024 06:19) - *Upvotes: 5*
To meet the requirements of the scenario, the company should take the following steps:

Create an Amazon FSx for NetApp ONTAP Multi-AZ file system (Option B): Amazon FSx for NetApp ONTAP supports both SMB (for Windows) and NFS (for Linux), and it provides sub-millisecond latencies. It also supports Multi-AZ configurations for high availability and durability.
Update the user data for each application’s launch template to mount the file system (Option D): This ensures that every new instance launched by the Auto Scaling group will have the file system mounted.
Perform an instance refresh on each Auto Scaling group (Option E): This will update the existing instances with the new launch template configuration, ensuring that they have the file system mounted.

---

**VerRi** (Thu 07 Nov 2024 15:26) - *Upvotes: 2*
D has updated UserData, new EC2 should be auto-mounted, why F?

---

**limelight04** (Wed 28 Aug 2024 01:07) - *Upvotes: 1*
Option B: Create an Amazon FSx for NetApp ONTAP Multi-AZ file system. This provides high-performance storage with support for both SMB and NFS protocols.

Option D: Update the user data for each application’s launch template to mount the file system. This ensures that the file system is automatically mounted when new instances are launched.

Option F: Update the EC2 instances for each application to mount the file system when new instances are launched. This ensures that all instances can read and write data to the file system

---

**jamesf** (Wed 31 Jul 2024 15:42) - *Upvotes: 4*
after review and check, BDE will be better
Option E focuses on updating existing instances with the new configurations by replacing them with new instances based on the updated launch template. This is particularly useful when you want all instances, including those currently running, to immediately adhere to new configurations.

B. Amazon FSx for NetApp ONTAP Multi-AZ file system.
- SMB and NFS Support
- Sub-Millisecond Latency
- Multi-AZ Availability: ensures HA and fault tolerance, as the data is replicated across multiple Availability Zones, which aligns with the requirement for a durable storage solution.

D. Update the user data for each application’s launch template to mount the file system.
- Automated Mounting the FSx file system at startup
- Protocol-Specific Commands: For Windows instances, mount the SMB share, while for Linux instances, mount the NFS share

---

**jamesf** (Wed 31 Jul 2024 15:43) - *Upvotes: 1*
Not A. Amazon EFS as supports only NFS, not SMB support for Windows.

Not C. Amazon EBS volume not natively support SMB or NFS, and EBS is block storage devices that attach to individual EC2 instances and do not support being shared directly across multiple instances.

---

**jamesf** (Wed 31 Jul 2024 15:47) - *Upvotes: 1*
Not Option F "Update the EC2 instances for each application to mount the file system when new instances are launched."
- because this is more about ensuring that future instances have the correct configuration, without immediately affecting running instances. It sets the stage for consistency moving forward but does not address existing instances.
- and future instances also take care by Option D

---

**jamesf** (Wed 31 Jul 2024 15:27) - *Upvotes: 1*
B. Amazon FSx for NetApp ONTAP Multi-AZ file system.
- SMB and NFS Support
- Sub-Millisecond Latency
- Multi-AZ Availability: ensures HA and fault tolerance, as the data is replicated across multiple Availability Zones, which aligns with the requirement for a durable storage solution.

D. Update the user data for each application’s launch template to mount the file system.
- Automated Mounting the FSx file system at startup
- Protocol-Specific Commands: For Windows instances, mount the SMB share, while for Linux instances, mount the NFS share

F. Update the EC2 instances for each application to mount the file system when new instances are launched.
- Configuration Consistency: ensures that every new instance launched as part of the Auto Scaling group auto mounts the FSx file system.
- Ease of Management: By automating the mounting process, you reduce the administrative overhead and potential for errors, ensuring a consistent and reliable setup.

---

**jamesf** (Wed 31 Jul 2024 15:30) - *Upvotes: 1*
Not A. Amazon EFS as supports only NFS, not SMB support for Windows.

Not C. Amazon EBS volume not natively support SMB or NFS, and EBS is block storage devices that attach to individual EC2 instances and do not support being shared directly across multiple instances.

Not E, Perform an instance refresh on each Auto Scaling group.
- Not Necessary for Mounting: An instance refresh primarily updates instances in the Auto Scaling group to use new configurations or launch templates. While useful in other scenarios, it doesn't directly relate to mounting the file system or affect the requirement for shared storage.
- Redundant with Updated Launch Templates: If launch templates and user data scripts are updated correctly, new instances will automatically mount the file system, making a manual refresh unnecessary unless other updates are needed.

---

**jamesf** (Wed 31 Jul 2024 15:44) - *Upvotes: 1*
Hi Moderator, please delete this comment, thank you.

---

**tgv** (Mon 15 Jul 2024 14:23) - *Upvotes: 2*
---> BDE

---


<br/>

## Question 278

*Date: Aug. 21, 2024, 3:20 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses an organization in AWS Organizations that a security team and a DevOps team manage. Both teams access the accounts by using AWS IAM Identity Center.

A dedicated group has been created for each team. The DevOps team's group has been assigned a permission set named DevOps. The permission set has the AdministratorAccess managed IAM policy attached. The permission set has been applied to all accounts in the organization.

The security team wants to ensure that the DevOps team does not have access to IAM Identity Center in the organization's management account. The security team has attached the following SCP to the organization root:



After implementing the policy, the security team discovers that the DevOps team can still access IAM Identity Center.

Which solution will fix the problem?

**Options:**
- A. In the organization's management account, create a new OU. Move the organization's management account to the new OU. Detach the SCP from the organization root. Attach the SCP to the new OU.
- B. In the organization's management account, update the SCP condition reference to the ARN of the DevOps team's group role to include the AWS account ID of the organization's management account.
- C. In IAM Identity Center, create a new permission set. Ensure that the assigned policy has full access but explicitly denies permission for the sso:* action and the sso-directory:* action. Update the assigned permission set for the DevOps team's group role in the organization's management account. Delete the SCP.
- D. In IAM Identity Center, update the DevOps permission set. Ensure that the assigned policy has full access but explicitly denies permission for the sso:* action and the sso-directory:* action. In the Deny statement, add a StringEquals condition that compares the aws:SourceAccount global condition context key with the organization's management account IDelete the SCP.

> **Suggested Answer:** D
> **Community Vote:** D (60%), B (27%), 7%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**0ac7838** (Fri 14 Nov 2025 06:15) - *Upvotes: 1*
I will go for C

A/B SCP do not affect management account
D SourceIP is used in service to service scenarios not by console or SSO logins

C skips SCP and blocks DevOps role on IAM Center level

---

**Impromptu** (Fri 22 Nov 2024 13:28) - *Upvotes: 3*
A and B do not work. SCPs do not apply to the management account.
https://docs.aws.amazon.com/organizations/latest/userguide/orgs_best-practices_mgmt-acct.html
Best practice is to limit access to the management account, and/or to delegate to Identity Centro to another member account. But since that's not an option given these answers, limiting the permission set is the next best thing.

---

**tinyshare** (Fri 15 Nov 2024 05:12) - *Upvotes: 2*
D is correct. Identity Center uses permission sets.

---

**VerRi** (Thu 07 Nov 2024 15:57) - *Upvotes: 2*
D might work, but SCPs are used to limit permissions and permission sets are used to grant permissions.

---

**ApacheKafkaAWS** (Thu 29 Aug 2024 15:20) - *Upvotes: 2*
It's B according to chatGPT

---

**limelight04** (Wed 28 Aug 2024 01:20) - *Upvotes: 2*
Option D

In IAM Identity Center, update the DevOps permission set. Ensure that the assigned policy has full access but explicitly denies permission for the sso:* action and the sso-directory:* action. In the Deny statement, add a StringEquals condition that compares the aws:SourceAccount global condition context key with the organization’s management account. Delete the SCP.

This approach ensures that the DevOps team retains necessary permissions while explicitly denying access to IAM Identity Center actions in the management account. Adding the StringEquals condition ensures that the policy is applied specifically to the management account, effectively preventing access.

---

**siheom** (Mon 26 Aug 2024 06:41) - *Upvotes: 2*
vote D

---

**hzaki** (Wed 21 Aug 2024 15:20) - *Upvotes: 1*
The right answer is A

---

**hzaki** (Mon 02 Sep 2024 08:41) - *Upvotes: 1*
Sorry, It's D

---


<br/>

## Question 279

*Date: Aug. 24, 2024, 4:19 p.m.
Disclaimers:
- ExamTopics website is not rel*

An Amazon EC2 Auto Scaling group manages EC2 instances that were created from an AMI. The AMI has the AWS Systems Manager Agent installed. When an EC2 instance is launched into the Auto Scaling group, tags are applied to the EC2 instance.

EC2 instances that are launched by the Auto Scaling group must have the correct operating system configuration.

Which solution will meet these requirements?

**Options:**
- A. Create a Systems Manager Run Command document that configures the desired instance configuration. Set up Systems Manager Compliance to invoke the Run Command document when the EC2 instances are not in compliance with the most recent patches.
- B. Create a Systems Manager State Manager association that links to the Systems Manager command document. Create a tag query that runs immediately.
- C. Create a Systems Manager Run Command task that specifies the desired instance configuration. Create a maintenance window in Systems Manager Maintenance Windows that runs daily. Register the Run Command task against the maintenance window. Designate the targets.
- D. Create a Systems Manager Patch Manager patch baseline and a patch group that use the same tags that the Auto Scaling group applies. Register the patch group with the patch baseline. Define a Systems Manager command document to patch the instances Invoke the document by using Systems Manager Run Command.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**limelight04** (Wed 28 Aug 2024 01:26) - *Upvotes: 3*
Option B: Create a Systems Manager State Manager association that links to the Systems Manager command document. Create a tag query that runs immediately.

Here’s why:

State Manager allows you to define and maintain consistent configuration of your instances. By creating an association that links to the command document, you can ensure that the desired configuration is applied as soon as the instances are launched.
Using a tag query ensures that the configuration is applied to the correct instances based on their tags, which are applied by the Auto Scaling group.

---

**chinchin97** (Mon 26 Aug 2024 07:43) - *Upvotes: 2*
State Manager ensures that all instances launched by the Auto Scaling group automatically receive the desired configuration which immediately applies when instances are launched.

While the primary focus of Option D is on patch management (applying updates and patches) rather than configuring the overall state of the operating system. Using Run Command is very AD-hoc which means you need to time the launching of the EC2 instance with the command and might not provide the continuous assurance that the configuration is maintained.

---

**Kushab94** (Sat 24 Aug 2024 16:19) - *Upvotes: 2*
B. Create a Systems Manager State Manager association that links to the Systems Manager command document. Create a tag query that runs immediately.

---


<br/>

## Question 280

*Date: Aug. 21, 2024, 4:03 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses AWS Organizations to manage its AWS accounts. The organization root has a child OU that is named Department. The Department OU has a child OU that is named Engineering. The default FullAWSAccess policy is attached to the root, the Department OU, and the Engineering OU.

The company has many AWS accounts in the Engineering OU. Each account has an administrative IAM role with the AdministratorAccess IAM policy attached. The default FullAWSAccessPolicy is also attached to each account.

A DevOps engineer plans to remove the FullAWSAccess policy from the Department OU. The DevOps engineer will replace the policy with a policy that contains an Allow statement for all Amazon EC2 API operations.

What will happen to the permissions of the administrative 1AM roles as a result of this change?

**Options:**
- A. All API actions on all resources will be allowed.
- B. All API actions on EC2 resources will be allowed. All other API actions will be denied.
- C. All API actions on all resources will be denied.
- D. All API actions on EC2 resources will be denied. All other API actions will be allowed.

> **Suggested Answer:** B
> **Community Vote:** B (77%), A (23%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**teo2157** (Mon 16 Dec 2024 09:40) - *Upvotes: 2*
It's B based on this url
https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html

---

**aws_god** (Sat 14 Sep 2024 08:10) - *Upvotes: 2*
The default FullAWSAccess policy is attached to the root, the Department OU, and the Engineering OU. So even if it is removed from the Department OU, it is still attached on the Engineering OU.

---

**ApacheKafkaAWS** (Thu 29 Aug 2024 15:23) - *Upvotes: 2*
I'ts B

---

**siheom** (Mon 26 Aug 2024 06:34) - *Upvotes: 2*
vote B..

---

**hzaki** (Mon 26 Aug 2024 02:19) - *Upvotes: 4*
When the FullAWSAccess policy is replaced with a policy that allows only EC2 actions, this new SCP will act as a boundary. Even if an IAM role or user within the account has a broader permission set (like AdministratorAccess), the SCP limits what can be done.

---

**hzaki** (Wed 21 Aug 2024 16:03) - *Upvotes: 1*
The answer is A
Still, the root has attached a full access policy.

---

**hzaki** (Mon 26 Aug 2024 02:20) - *Upvotes: 1*
Sorry the Answer: B
When the FullAWSAccess policy is replaced with a policy that allows only EC2 actions, this new SCP will act as a boundary. Even if an IAM role or user within the account has a broader permission set (like AdministratorAccess), the SCP limits what can be done.

---


<br/>

## Question 281

*Date: Aug. 21, 2024, 9:23 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company manages AWS accounts in AWS Organizations. The company needs a solution to send Amazon CloudWatch Logs data to an Amazon S3 bucket in a dedicated AWS account. The solution must support all existing and future CloudWatch Logs log groups.

Which solution will meet these requirements?

**Options:**
- A. Enable Organizations backup policies to back up all log groups to a dedicated S3 bucket. Add an S3 bucket policy that allows access from all accounts that belong to the company.
- B. Create a backup plan in AWS Backup. Specify a dedicated S3 bucket as a backup vault. Assign all CloudWatch Logs log group resources to the backup plan. Create resource assignments in the backup plan for all accounts that belong to the company.
- C. Create a backup plan in AWS Backup. Specify a dedicated S3 bucket as a backup vault. Assign all existing log groups to the backup plan. Create resource assignments in the backup plan for all accounts that belong to the company. Create an AWS Systems Manager Automation runbook to assign log groups to a backup plan. Create an AWS Config rule that has an automatic remediation action for all noncompliant log groups. Specify the runbook as the rule's target.
- D. Create a CloudWatch Logs destination and an Amazon Kinesis Data Firehose delivery stream in the dedicated AWS account. Specify the S3 bucket as the destination of the delivery stream. Create subscription filters for all existing log groups in all accounts. Create an AWS Lambda function to call the CloudWatch Logs PutSubscriptionFilter API operation. Create an Amazon EventBridge rule to invoke the Lambda function when a CreateLogGroup event occurs.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**aws_god** (Sat 14 Sep 2024 08:21) - *Upvotes: 2*
C seems like the most elegant solution, but I was not able to find any documentation that supports it.

For option D I found this - https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#FirehoseExample

---

**hzaki** (Wed 21 Aug 2024 15:08) - *Upvotes: 2*
Correct answer D

---

**hzaki** (Wed 21 Aug 2024 09:23) - *Upvotes: 2*
Correct answer D

---


<br/>

## Question 282

*Date: Dec. 16, 2024, 9:59 a.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer manages a Java-based application that runs in an Amazon Elastic Container Service (Amazon ECS) cluster on AWS Fargate. Auto scaling has not been configured for the application.

The DevOps engineer has determined that the Java Virtual Machine (JVM) thread count is a good indicator of when to scale the application. The application serves customer traffic on port 8080 and makes JVM metrics available on port 9404.

Application use has recently increased. The DevOps engineer needs to configure auto scaling for the application.

Which solution will meet these requirements with the LEAST operational overhead? (Choose two.)

**Options:**
- A. Deploy the Amazon CloudWatch agent as a container sidecar. Configure the CloudWatch agent to retrieve JVM metrics from port 9404. Create CloudWatch alarms on the JVM thread count metric to scale the application. Add a step scaling policy in Fargate to scale up and scale down based on the CloudWatch alarms.
- B. Deploy the Amazon CloudWatch agent as a container sidecar. Configure a metric filter for the JVM thread count metric on the CloudWatch log group for the CloudWatch agent. Add a target tracking policy in Fargate. Select the metric from the metric filter as a scale target.
- C. Create an Amazon Managed Service for Prometheus workspace. Deploy AWS Distro for OpenTelemetry as a container sidecar to publish the JVM metrics from port 9404 to the Prometheus workspace. Configure rules for the workspace to use the JVM thread count metric to scale the application. Add a step scaling policy in Fargate. Select the Prometheus rules to scale up and scaling down.
- D. Create an Amazon Managed Service for Prometheus workspace. Deploy AWS Distro for OpenTelemetry as a container sidecar to retrieve JVM metrics from port 9404 to publish the JVM metrics from port 9404 to the Prometheus workspace. Add a target tracking policy in Fargate. Select the Prometheus metric as a scale target.

> **Suggested Answer:** AD
> **Community Vote:** AD (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**0ac7838** (Fri 14 Nov 2025 06:25) - *Upvotes: 1*
Why A is correct

Deploying the CloudWatch agent as a sidecar to scrape the JVM metrics from port 9404 is straightforward and low-overhead (agent runs with the task).

Publishing the JVM thread-count metric to CloudWatch, then creating CloudWatch alarms that drive an ECS/Fargate step scaling policy works reliably.

This approach uses native CloudWatch → Application Auto Scaling integration and is simple operationally.

Why D is correct

Amazon Managed Service for Prometheus (AMP) + AWS Distro for OpenTelemetry (ADOT) sidecar is the managed, scalable way to publish Prometheus-style JVM metrics from the container to AMP.

Using AMP metrics as a scale target with a target-tracking policy gives a low-ops autoscaling experience (target tracking adjusts capacity automatically to hit the metric target).

Target tracking is typically less operational overhead than manually tuning step policies.

---

**teo2157** (Mon 16 Dec 2024 09:59) - *Upvotes: 3*
Both A and D are feasible based on this links:
https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-step-scaling-policies.html
https://aws.amazon.com/blogs/containers/autoscaling-amazon-ecs-services-based-on-custom-metrics-with-application-auto-scaling/

---


<br/>

## Question 283

*Date: Dec. 15, 2024, 12:02 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an application that runs in a single AWS Region. The application runs on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster and connects to an Amazon Aurora MySQL cluster. The application is built in an AWS CodeBuild project. The container images are published to Amazon Elastic Container Registry (Amazon ECR).

The company needs to replicate the state of the application for the container images and the database to a second Region.

Which solution will meet these requirements in the MOST operationally efficient way?

**Options:**
- A. Turn on Amazon S3 Cross-Region Replication (CRR) on the bucket that holds the ECR container images. Deploy the application to an EKS cluster in the second Region by referencing the new S3 bucket object URL for the container image in a Kubernetes deployment file. Configure a cross-Region Aurora Replica in the second Region. Configure the new application deployment to use the endpoints for the cross-Region Aurora Replica.
- B. Create an Amazon EventBridge rule that reacts to image pushes to the ECR repository. Configure the EventBridge rule to invoke an AWS Lambda function to replicate the image to a new ECR repository in the second Region. Deploy the application to an EKS cluster in the second Region by referencing the new ECR repository in a Kubernetes deployment file. Configure a cross-Region Aurora Replica in the second Region. Configure the new application deployment to use the endpoints for the cross-Region Aurora Replica.
- C. Turn on Cross-Region Replication to replicate the ECR repository to the second Region. Deploy the application to an EKS cluster in the second Region by referencing the new ECR repository in a Kubernetes deployment file. Configure an Aurora global database with clusters in the initial Region and the second Region. Configure the new application deployment to use the endpoints for the second Region's cluster in the Aurora global database.
- D. Configure the CodeBuild project to also push the container image to an ECR repository in the second Region. Deploy the application to an EKS cluster in the second Region by referencing the new ECR repository in a Kubernetes deployment file. Configure an Aurora MySQL cluster in the second Region as the target for binary log replication from the Aurora MySQL cluster in the initial Region. Configure the new application deployment to use the endpoints for the second Region's cluster.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**teo2157** (Mon 16 Dec 2024 11:04) - *Upvotes: 2*
Agree with C based on this link:
https://aws.amazon.com/blogs/containers/cross-region-replication-in-amazon-ecr-has-landed/

---

**Ky_24** (Sun 15 Dec 2024 12:02) - *Upvotes: 2*
Why this is the most operationally efficient:

• Minimal Management Overhead: AWS manages the replication for both the ECR images and the Aurora database. There is no need to configure or maintain additional Lambda functions or EventBridge rules.
• Scalability: Aurora global databases and ECR replication are designed for production-grade systems and can scale to meet large workloads.
• Reliability: The solution leverages AWS-native features, reducing the chance of errors or operational disruptions.

---


<br/>

## Question 284

*Date: Dec. 15, 2024, 12:19 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is building a serverless application that uses AWS Lambda functions to process data.

A BeginResponse Lambda function initializes data in response to specific application events. The company needs to ensure that a large number of Lambda functions are invoked after the BeginResponse Lambda function runs. Each Lambda function must be invoked in parallel and depends on only the outputs of the BeginResponse Lambda function. Each Lambda function has retry logic for invocation and must be able to fine-tune concurrency without losing data.

Which solution will meet these requirements with the MOST operational efficiency?

**Options:**
- A. Create an Amazon Simple Notification Service (Amazon SNS) topic. Modify the BeginResponse Lambda function to publish to the SNS topic before the BeginResponse Lambda function finishes running. Subscribe all Lambda functions that need to invoke after the BeginResponse Lambda function runs to the SNS topic. Subscribe any new Lambda functions to the SNS topic.
- B. Create an Amazon Simple Queue Service (Amazon SQS) queue for each Lambda function that needs to run after the BeginResponse Lambda function runs. Subscribe each Lambda function to its own SQS queue. Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe each SQS queue to the SNS topic. Modify the BeginResponse function to publish to the SNS topic when it finishes running.
- C. Create an Amazon Simple Queue Service (Amazon SQS) queue for each Lambda function that needs to run after the BeginResponse Lambda function runs. Subscribe the Lambda function to the SQS queue. Create an Amazon Simple Notification Service (Amazon SNS) topic for each SQS queue. Subscribe the SQS queues to the SNS topics. Modify the BeginResponse function to publish to the SNS topics when the function finishes running.
- D. Create an AWS Step Functions Standard Workflow. Configure states in the workflow to invoke the Lambda functions sequentially. Create an Amazon Simple Notification Service (Amazon SNS) topic. Modify the BeginResponse Lambda function to publish to the SNS topic before the Lambda function finishes running. Create a new Lambda function that is subscribed to the SNS topic and that invokes the Step Functions workflow.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**youonebe** (Fri 27 Dec 2024 15:27) - *Upvotes: 2*
Fan-out

---

**Ky_24** (Sun 15 Dec 2024 12:19) - *Upvotes: 4*
How B Works:

1. BeginResponse Function: Publishes to a single SNS topic when it completes its processing.
2. SNS Fan-Out: The SNS topic sends the message to multiple SQS queues, one for each dependent Lambda function.
3. SQS Queues and Lambda: Each Lambda function is subscribed to its respective SQS queue and processes messages independently.
4. Retry and Concurrency: SQS queues ensure messages are retried if processing fails, and Lambda concurrency can be tuned for each queue.

---


<br/>

## Question 285

*Date: Dec. 16, 2024, 11:33 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company operates a globally deployed product out of multiple AWS Regions. The company's DevOps team needs to use Amazon API Gateway to deploy an API to support the product.

The API must be deployed redundantly. The deployment must provide independent availability from each company location. The deployment also must respond to a custom domain URL and must optimize performance for the API user requests.

Which solution will meet these requirements?

**Options:**
- A. Deploy an API Gateway edge-optimized API endpoint in the us-east-1 Region. Create an API Gateway custom domain for the API. Create an Amazon Route 53 record set with a geoproximity routing policy for the API's custom domain. Increase the geographic bias to the maximum allowed value.
- B. Deploy an API Gateway regional API endpoint in the us-east-1 Region. Integrate the API Gateway API with a public Application Load Balancer (ALB). Create an AWS Global Accelerator standard accelerator. Associate the endpoint with the ALCreate an Amazon Route 53 alias record set that points the custom domain name to the DNS name that is assigned to the accelerator.
- C. Deploy an API Gateway regional API endpoint in every AWS Region where the company's product is deployed. Create an API Gateway custom domain in each Region for the deployed API Gateway API. Create an Amazon Route 53 record set that has a latency routing policy for every deployed API Gateway custom domain.
- D. Deploy an API Gateway edge-optimized API endpoint in the us-east-1 Region. Create an Amazon CloudFront distribution. Configure the CloudFront distribution with an alternate domain name. Specify the API Gateway Invoke URL as the origin domain. Create an Amazon Route 53 alias record set with a simple routing policy. Point the routing policy to the CloudFront distribution domain name.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**teo2157** (Mon 16 Dec 2024 11:33) - *Upvotes: 3*
1. **Deploy Amazon API Gateway in Multiple Regions**:
- Deploy your API Gateway in multiple AWS Regions to ensure redundancy and independent availability.
2. **Set Up a Custom Domain Name for API Gateway**:
- Create a custom domain name in API Gateway and map it to your APIs in each Region.
3. **Use Amazon Route 53 for Global DNS and Latency-Based Routing**:
- Use Amazon Route 53 to create a global DNS entry for your custom domain and configure latency-based routing to direct user requests to the nearest API Gateway deployment.

---


<br/>

## Question 286

*Date: Dec. 2, 2024, 5:48 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer uses AWS CodeBuild to frequently produce software packages. The CodeBuild project builds large Docker images that the DevOps engineer can use across multiple builds.

The DevOps engineer wants to improve build performance and minimize costs.

Which solution will meet these requirements?

**Options:**
- A. Store the Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository. Implement a local Docker layer cache for CodeBuild.
- B. Cache the Docker images in an Amazon S3 bucket that is available across multiple build hosts. Expire the cache by using an S3 Lifecycle policy.
- C. Store the Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository. Modify the CodeBuild project runtime configuration to always use the most recent image version.
- D. Create custom AMIs that contain the cached Docker images. In the CodeBuild build, launch Amazon EC2 instances from the custom AMIs.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**DKM** (Tue 18 Mar 2025 18:09) - *Upvotes: 1*
The LOCAL_DOCKER_LAYER_CACHE is a feature in Docker that allows you to store build cache layers locally on your filesystem. This can significantly speed up the build process by reusing layers that haven't changed, reducing the need to rebuild them from scratch

---

**teo2157** (Mon 16 Dec 2024 11:47) - *Upvotes: 3*
1. **Use Amazon ECR to Store Docker Images**:
- Push the built Docker images to Amazon ECR after a successful build.
- Pull the Docker images from Amazon ECR at the start of each build to use as a cache.

2. **Configure CodeBuild to Use Docker Layer Caching**:
- Enable Docker layer caching in your CodeBuild project to further improve build performance.

---

**Ky_24** (Sun 15 Dec 2024 12:54) - *Upvotes: 2*
1. Amazon ECR for Centralized Storage:
• Storing Docker images in Amazon ECR allows you to maintain and reuse images across builds. This reduces the need to rebuild base images repeatedly, improving build performance.
• ECR integrates seamlessly with AWS CodeBuild, making it a scalable and cost-effective solution for managing container images.
2. Local Docker Layer Caching:
• Docker layer caching significantly reduces build time by avoiding the re-download or re-creation of unchanged layers.
• CodeBuild supports enabling local caching with type: LOCAL_DOCKER_LAYER_CACHE. This caching stores intermediate image layers locally on the build host, which speeds up subsequent builds.

---


<br/>

## Question 287

*Date: t200*

A large company recently acquired a small company. The large company invited the small company to join the large company's existing organization in AWS Organizations as a new OU.

A DevOps engineer determines that the small company needs to launch t3.small Amazon EC2 instance types for the company's application workloads. The small company needs to deploy the instances only within US-based AWS Regions.

The DevOps engineer needs to use an SCP in the small company's new OU to ensure that the small company can launch only the required instance types.

Which solution will meet these requirements?

**Options:**
- A. Configure a statement to deny the ec2:RunInstances action for all EC2 instance resources when the ec2:InstanceType condition is not equal to t3.small.
Configure another statement to deny the ec2:RunInstances action for all EC2 instance resources when the aws:RequestedRegion condition is not equal to us-*.
- B. Configure a statement to allow the ec2:RunInstances action for all EC2 instance resources when the ec2:InstanceType condition is not equal to t3.small.
Configure another statement to allow the ec2:RunInstances action for all EC2 instance resources when the aws:RequestedRegion condition is not equal to us-*.
- C. Configure a statement to deny the ec2:RunInstances action for all EC2 instance resources when the ec2:InstanceType condition is equal to t3.small.
Configure another statement to deny the ec2:RunInstances action for all EC2 instance resources when the aws:RequestedRegion condition is equal to us-*.
- D. Configure a statement to allow the ec2:RunInstances action for all EC2 instance resources when the ec2:InstanceType condition is equal to t3.small.
Configure another statement to allow the ec2:RunInstances action for all EC2 instance resources when the aws:RequestedRegion condition is equal to us-*.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**c87b433** (Tue 04 Feb 2025 16:22) - *Upvotes: 2*
Very important:
• An Allow statement in an SCP permits the Resource element to only have a "*" entry.
• An Allow statement in an SCP can't have a Condition element at all.
Because of the second point, B and D are not right because these are adding conditions in the allow statement. C does not mean and deny the requirements so A is the right answer.

---

**CHRIS12722222** (Wed 25 Dec 2024 22:43) - *Upvotes: 3*
A

deny the ec2:RunInstances action for all EC2 instance resources when the ec2:InstanceType condition is not equal to t3.small

deny the ec2:RunInstances action for all EC2 instance resources when the aws:RequestedRegion condition is not equal to us-*

---


<br/>

## Question 288

*Date: Dec. 16, 2024, 12:02 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps team manages infrastructure for an application. The application uses long-running processes to process items from an Amazon Simple Queue Service (Amazon SQS) queue. The application is deployed to an Auto Scaling group.

The application recently experienced an issue where items were taking significantly longer to process. The queue exceeded the expected size, which prevented various business processes from functioning properly. The application records all logs to a third-party tool.

The team is currently subscribed to an Amazon Simple Notification Service (Amazon SNS) topic that the team uses for alerts. The team needs to be alerted if the queue exceeds the expected size.

Which solution will meet these requirements with the MOST operational efficiency?

**Options:**
- A. Create an Amazon CloudWatch metric alarm with a period of 1 hour and a static threshold to alarm if the average of the ApproximateNumberOfMessagesDelayed metric is greater than the expected value. Configure the alarm to notify the SNS topic.
- B. Create an Amazon CloudWatch metric alarm with a period of 1 hour and a static threshold to alarm if the sum of the ApproximateNumberOfMessagesVisible metric is greater than the expected value. Configure the alarm to notify the SNS topic.
- C. Create an AWS Lambda function that retrieves the ApproximateNumberOfMessages SQS queue attribute value and publishes the value as a new CloudWatch custom metric. Create an Amazon EventBridge rule that is scheduled to run every 5 minutes and that invokes the Lambda function. Configure a CloudWatch metrics alarm with a period of 1 hour and a static threshold to alarm if the sum of the new custom metric is greater than the expected value.
- D. Create an AWS Lambda function that checks the ApproximateNumberOfMessagesDelayed SQS queue attribute and compares the value to a defined expected size in the function. Create an Amazon EventBridge rule that is scheduled to run every 5 minutes and that invokes the Lambda function. When the ApproximateNumberOfMessagesDelayed SQS queue attribute exceeds the expected size, send a notification the SNS topic.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**CHRIS12722222** (Wed 25 Dec 2024 22:53) - *Upvotes: 4*
ApproximateNumberOfMessagesVisible, which provides the number of messages available for retrieval from the queue. This is the primary metric that reflects how many messages are waiting to be processed.

---

**teo2157** (Mon 16 Dec 2024 12:02) - *Upvotes: 3*
Between CloudWatch alarms and Lambda functios is more operational the CloudWatch alarms. The key point here is that ApproximateNumberOfMessagesVisible corresponds to
The number of messages to be processed while ApproximateNumberOfMessagesDelayed is the number of messages in the queue that are delayed and not available for reading immediately. Based on that it's B.

---


<br/>

## Question 289

*Date: Dec. 16, 2024, 12:18 p.m.
Disclaimers:
- ExamTopics website is not rel*

A large company runs critical workloads in multiple AWS accounts. The AWS accounts are managed under AWS Organizations with all features enabled. The company stores confidential customer data in an Amazon S3 bucket. Access to the S3 bucket requires multiple levels of approval.

The company wants to monitor when the S3 bucket is accessed by using the AWS CLI. The company also wants insights into the various activities performed by other users on all other S3 buckets in the AWS accounts to detect any issues.

Which solution will meet these requirements?

**Options:**
- A. Create an AWS CloudTrail trail that is delivered to Amazon CloudWatch in each AWS account. Enable data events logs for all S3 buckets. Use Amazon GuardDuty for anomaly detection in all the AWS accounts. Use Amazon Athena to perform SQL queries on the custom metrics created from the CloudTrail logs.
- B. Create an AWS CloudTrail organization trail that is delivered to Amazon CloudWatch in the Organizations management account. Enable data events logs for all S3 buckets. Use Amazon CloudWatch anomaly detection in all the AWS accounts. Use Amazon Athena to perform SQL queries on the custom metrics created from the CloudTrail logs.
- C. Create an AWS CloudTrail organization trail that is delivered to Amazon CloudWatch in the Organizations management account. Enable data events logs for all S3 buckets. Use Amazon CloudWatch anomaly detection in all the AWS accounts. Use Amazon CloudWatch Metrics Insights to perform SQL queries on the custom metrics created from the CloudTrail logs.
- D. Create an AWS CloudTrail trail that is delivered to Amazon CloudWatch in each AWS account. Enable data events logs for all S3 buckets. Use a custom solution for anomaly detection in all the AWS accounts. Use Amazon CloudWatch Metrics Insights to perform SQL queries on the custom metrics created from the CloudTrail logs.

> **Suggested Answer:** C
> **Community Vote:** C (78%), B (22%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**jojewi8143** (Sun 02 Feb 2025 12:45) - *Upvotes: 2*
Athena can only perform queries in S3 buckets, not in cloudwatch metrics.

---

**teo2157** (Thu 30 Jan 2025 08:15) - *Upvotes: 2*
Athena can only permorm queries in S3 buckets, not in cloudwatch metrics. Based on that, it's C.

---

**Slays** (Fri 03 Jan 2025 10:21) - *Upvotes: 1*
Athena allows for ad-hoc analysis of log data, enabling you to investigate specific events or trends without the need to set up complex data processing pipelines.

---

**matt200** (Mon 30 Dec 2024 08:22) - *Upvotes: 3*
Amazon CloudWatch Metrics Insights can perform SQL queries

---

**spring21** (Tue 17 Dec 2024 21:04) - *Upvotes: 1*
You have now set up an AWS CloudTrail organization trail that sends logs to CloudWatch, enabled anomaly detection on the CloudTrail logs, and configured Amazon Athena to query those logs with SQL. You can further optimize this setup by incorporating Lambda functions, setting more complex anomaly detection configurations, or using AWS Security Hub for better monitoring and automation.

---


<br/>

## Question 290

*Date: Dec. 15, 2024, 1:30 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps team is deploying microservices for an application on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The cluster uses managed node groups. The DevOps team wants to enable auto scaling for the microservice Pods based on a specific CPU utilization percentage. The DevOps team has already installed the Kubernetes Metrics Server on the cluster.

Which solution will meet these requirements in the MOST operationally efficient way?

**Options:**
- A. Edit the Auto Scaling group that is associated with the worker nodes of the EKS cluster. Configure the Auto Scaling group to use a target tracking scaling policy to scale when the average CPU utilization of the Auto Scaling group reaches a specific percentage.
- B. Deploy the Kubernetes Horizontal Pod Autoscaler (HPA) and the Kubernetes Vertical Pod Autoscaler (VPA) in the cluster. Configure the HPA to scale based on the target CPU utilization percentage. Configure the VPA to use the recommender mode setting.
- C. Run the AWS Systems Manager AWS-UpdateEKSManagedNodeGroup Automation document. Modify the values for NodeGroupDesiredSize, NodeGroupMaxSize, and NodeGroupMinSize to be based on an estimate for the required node size.
- D. Deploy the Kubernetes Horizontal Pod Autoscaler (HPA) and the Kubernetes Cluster Autoscaler in the cluster. Configure the HPA to scale based on the target CPU utilization percentage. Configure the Cluster Autoscaler to use the auto-discovery setting.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Ky_24** (Sun 15 Dec 2024 13:30) - *Upvotes: 3*
Why Option D is Correct:

Horizontal Pod Autoscaler (HPA):
HPA automatically adjusts the number of Pods in a Kubernetes Deployment, ReplicaSet, or StatefulSet based on observed CPU or memory utilization or custom metrics.
With the Kubernetes Metrics Server already installed, HPA can monitor CPU utilization metrics and scale the Pods to maintain the target CPU usage percentage.
Cluster Autoscaler:
When HPA increases the number of Pods and there aren’t enough resources in the cluster, Cluster Autoscaler adds nodes to the managed node group dynamically.
The auto-discovery feature enables Cluster Autoscaler to automatically detect the appropriate node group and scale it up or down as needed.
Operational Efficiency:
This combination ensures Pods scale first at the workload level (HPA) and then at the infrastructure level (Cluster Autoscaler) only if required. This approach minimizes cost and ensures optimal resource utilization.

---


<br/>

## Question 291

*Date: Nov. 8, 2024, 8:06 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has multiple AWS accounts. The company uses AWS IAM Identity Center that is integrated with a third-party SAML 2.0 identity provider (IdP).

The attributes for access control feature is enabled in IAM Identity Center. The attribute mapping list maps the department key from the IdP to the ${path:enterprise.department} attribute. All existing Amazon EC2 instances have a d1, d2, d3 department tag that corresponds to three company’s departments.

A DevOps engineer must create policies based on the matching attributes. The policies must grant each user access to only the EC2 instances that are tagged with the user’s respective department name.

Which condition key should the DevOps engineer include in the custom permissions policies to meet these requirements?

**Options:**
- A.
- B.
- C.
- D.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**spring21** (Tue 17 Dec 2024 21:22) - *Upvotes: 2*
it must also be named exactly the same in your aws:PrincipalTag condition key (that is, "ec2:ResourceTag/CostCenter": "${aws:PrincipalTag/CostCenter}").

---

**Impromptu** (Fri 22 Nov 2024 14:55) - *Upvotes: 3*
Answer is indeed C
https://docs.aws.amazon.com/singlesignon/latest/userguide/configure-abac-attributes.html

---

**koo_kai** (Sun 10 Nov 2024 13:32) - *Upvotes: 2*
it's c

---

**Jefff9997** (Fri 08 Nov 2024 20:06) - *Upvotes: 3*
The answer is C. Not B.

---


<br/>

## Question 292

*Date: t200*

A security team wants to use AWS CloudTrail to monitor all actions and API calls in multiple accounts that are in the same organization in AWS Organizations. The security team needs to ensure that account users cannot turn off CloudTrail in the accounts.

Which solution will meet this requirement?

**Options:**
- A. Apply an SCP to all OUs to deny the cloudtrail:StopLogging action and the cloudtrail:DeleteTrail action.
- B. Create IAM policies in each account to deny the cloudtrail:StopLogging action and the cloudtrail:DeleteTrail action.
- C. Set up Amazon CloudWatch alarms to notify the security team when a user disables CloudTrail in an account.
- D. Use AWS Config to automatically re-enable CloudTrail if a user disables CloudTrail in an account.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**matt200** (Mon 30 Dec 2024 08:39) - *Upvotes: 2*
should be A

---


<br/>

## Question 293

*Date: Nov. 10, 2024, 1:41 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer needs to configure a blue/green deployment for an existing three-tier application. The application runs on Amazon EC2 instances and uses an Amazon RDS database. The EC2 instances run behind an Application Load Balancer (ALB) and are in an Auto Scaling group.

The DevOps engineer has created launch templates, Auto Scaling groups, and ALB target groups for the blue environment and the green environment. Each target group specifies which application version, blue or green, will be loaded on the EC2 instances. An Amazon Route 53 record for www.example.com points to the ALB.

The deployment must shift traffic all at once from the blue environment to the green environment.

Which solution will meet these requirements?

**Options:**
- A. Starta rolling restart of the Auto Scaling group for the green environment to deploy the new application version to the green environment's EC2 instances. When the rolling restart is complete, use an AWS CLI command to update the ALB to send traffic to the green environment's target group.
- B. Use an AWS CLI command to update the ALB to send traffic to the green environments target group. Start a rolling restart of the Auto Scaling group for the green environment to deploy the new application version to the green environment's EC2 instances.
- C. Update the launch template to deploy the green environment's application version to the blue environment's EC2 instances. Do not change the target groups or the Auto Scaling groups in either environment. Perform a rolling restart of the blue environments EC2 instances.
- D. Starta rolling restart of the Auto Scaling group for the green environment to deploy the new application version to the green environment's EC2 instances. When the rolling restart is complete, update Route 53 to point to the green environment's endpoint on the ALB.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**koo_kai** (Sun 10 Nov 2024 13:41) - *Upvotes: 6*
it's A
Q.120

---

**youonebe** (Fri 27 Dec 2024 16:27) - *Upvotes: 2*
Should be A

---

**eugene2owl** (Mon 09 Dec 2024 13:05) - *Upvotes: 2*
For sure, it's "A". First we update new (meaning "green") cluster with a new version. And after that we shift traffic of ALB from first target group to the second.

---

**fnameni** (Tue 26 Nov 2024 03:49) - *Upvotes: 2*
A it's correct

---


<br/>

## Question 294

*Date: Dec. 15, 2024, 2:34 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an application that runs on Amazon EC2 instances in an Auto Scaling group. The application processes a high volume of messages from an Amazon Simple Queue Service (Amazon SQS) queue.

A DevOps engineer noticed that the application took several hours to process a group of messages from the SQS queue. The average CPU utilization of the Auto Scaling group did not cross the threshold of a target tracking scaling policy when processing the messages. The application that processes the SQS queue publishes logs to ‘Amazon CloudWatch Logs.

The DevOps engineer needs to ensure that the queue is processed quickly.

Which solution meets these requirements with the LEAST operational overhead?

**Options:**
- A. Create an AWS Lambda function. Configure the Lambda function to publish a custom metric by using the ApproximateNumberOfMessagesVisible SQS queue attribute and the GroupInServiceInstances Auto Scaling group attribute to publish the queue messages for each instance. Schedule an Amazon EventBridge rule to run the Lambda function every hour. Create a target tracking scaling policy for the Auto Scaling group that uses the custom metric to scale in and out.
- B. Create an AWS Lambda function. Configure the Lambda function to publish a custom metric by using the ApproximateNumberOfMessagesVisible SQS queue attribute and the GroupInServiceInstances Auto Scaling group attribute to publish the queue messages for each instance. Create a CloudWatch subscription filter for the application logs with the Lambda function as the target. Create a target tracking scaling policy for the Auto Scaling group that uses the custom metric to scale in and out.
- C. Create a target tracking scaling policy for the Auto Scaling group. In the target tracking policy, use the ApproximateNumberOfMessagesVisible SQS queue attribute and the GroupInServiceInstances Auto Scaling group attribute to calculate how many messages are in the queue for each number of instances by using metric math. Use the calculated attribute to scale in and out.
- D. Create an AWS Lambda function that logs the ApproximateNumberOfMessagesVisible attribute of the SQS queue to a CloudWatch Logs log group. Schedule an Amazon EventBridge rule to run the Lambda function every 5 minutes. Create a metric filer to count the number of log events from a CloudWatch logs group. Create a target tracking scaling policy for the Auto Scaling group that uses the custom metric to scale in and out.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**GripZA** (Tue 22 Apr 2025 12:50) - *Upvotes: 1*
both these metrics available in CloudWatch:
GroupInServiceInstances: The number of instances that are running as part of the Auto Scaling group. This metric does not include instances that are pending or terminating.
ApproximateNumberOfMessagesVisible: The number of messages to be processed.

no need for Lambda

---

**Ky_24** (Sun 15 Dec 2024 14:34) - *Upvotes: 4*
Why Option C is Correct:
• Target Tracking Scaling Policy: The question specifies that the scaling policy should help process the queue quickly. Using metric math to combine the ApproximateNumberOfMessagesVisible SQS queue attribute with the GroupInServiceInstances Auto Scaling group attribute provides an efficient way to scale based on the actual number of messages and the available instances.
• Direct Integration with Auto Scaling: This solution integrates directly with the Auto Scaling group’s target tracking policy, so the scaling of EC2 instances happens automatically and dynamically based on the metrics from SQS, ensuring faster processing of messages.

---


<br/>

## Question 295

*Date: t200*

A company has a single AWS account that runs hundreds of Amazon EC2 instances in a single AWS Region. The company launches and terminates new EC2 instances every hour. The account includes existing EC2 instances that have been running for longer than a week.

The company's security policy requires all running EC2 instances to have an EC2 instance profile attached. The company has created a default EC2 instance profile. The default EC2 instance profile must be attached to any EC2 instances that do not have a profile attached.

Which solution will meet these requirements?

**Options:**
- A. Configure an Amazon EventBridge rule that matches the Amazon EC2 RunInstances API calls. Configure the rule to invoke an AWS Lambda function to attach the default instance profile to the EC2 instances.
- B. Configure AWS Config. Deploy an AWS Config ec2-instance-profile-attached managed rule. Configure an automatic remediation action that invokes an AWS Systems Manager Automation runbook to attach the default instance profile to the EC2 instances.
- C. Configure an Amazon EventBridge rule that matches the Amazon EC2 StartInstances API calls. Configure the rule to invoke an AWS Systems Manager Automation runbook to attach the default instance profile to the EC2 instances.
- D. Configure AWS Config. Deploy an AWS Config iam-role-managed-policy-check managed rule. Configure an automatic remediation action that invokes an AWS Lambda function to attach the default instance profile to the EC2 instances.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Adzz** (Sun 25 May 2025 08:10) - *Upvotes: 1*
option b

---

**CHRIS12722222** (Wed 25 Dec 2024 23:43) - *Upvotes: 3*
https://docs.aws.amazon.com/config/latest/developerguide/ec2-instance-profile-attached.html

---


<br/>

## Question 296

*Date: Nov. 22, 2024, 3:06 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses AWS Organizations to manage hundreds of AWS accounts. The company has a team that is responsible for AWS Identity and Access Management (IAM).

The IAM team wants to implement AWS IAM Identity Center. The IAM team must have only the minimum required permissions to manage IAM Identity Center. The IAM team must not be able to gain unnecessary access to the Organizations management account. The IAM team must be able to provision new IAM Identity Center permission sets and assignments for new and existing member accounts.

Which combination of steps will meet these requirements? (Choose three.)

**Options:**
- A. Create a new AWS account for the IAM team. Enable IAM Identity Center in the new account. In the Organizations management account, register the new account as a delegated administrator for IAM Identity Center.
- B. Create a new AWS account for the IAM team. Enable IAM Identity Center in the Organizations management account. In the Organizations management account, register the new account as a delegated administrator for IAM Identity Center.
- C. Create an SCP in Organizations. Create a new OU for the Organizations management account, and link the new SCP to the OU. Configure the SCP to deny all access to IAM Identity Center.
- D. Create IAM users and an IAM group for the IAM team in IAM Identity Center. Add the users to the group. Create a new permission set. Attach the AWSSSOMemberAccountAdministrator managed IAM policy to the group.
- E. Assign the new permission set to the Organizations management account. Allow the IAM team's group to use the permission set.
- F. Assign the new permission set to the new AWS account. Allow the IAM team's group to use the permission set.

> **Suggested Answer:** BDF
> **Community Vote:** BDF (91%), 9%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**tinyshare** (Wed 04 Dec 2024 00:24) - *Upvotes: 5*
A is wrong. you need to enable the Identity Center in the management account first, then create a delegated account.
C is wrong SCP is different from IdP
E is wrong. The permission set should be assigned to members, not the management account.

---

**Impromptu** (Fri 22 Nov 2024 15:06) - *Upvotes: 5*
Should be B instead of A: https://docs.aws.amazon.com/singlesignon/latest/userguide/get-set-up-for-idc.html
Although the administration can be delegated to a member account, the enabling of Identity Center is still in the management account.

---

**ryuhei** (Tue 26 Aug 2025 07:00) - *Upvotes: 1*
The correct combination of steps is A, C, and F. This allows the IAM team to effectively manage the IAM Identity Center, operate with least privilege, and restrict access to administrative accounts.

---


<br/>

## Question 297

*Date: Dec. 18, 2024, 1:15 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses an Amazon Aurora PostgreSQL global database that has two secondary AWS Regions. A DevOps engineer has configured the database parameter group to guarantee an RPO of 60 seconds. Write operations on the primary cluster are occasionally blocked because of the RPO setting.

The DevOps engineer needs to reduce the frequency of blocked write operations.

Which solution will meet these requirements?

**Options:**
- A. Add an additional secondary cluster to the global database.
- B. Enable write forwarding for the global database.
- C. Remove one of the secondary clusters from the global database.
- D. Configure synchronous replication for the global database.

> **Suggested Answer:** C
> **Community Vote:** C (54%), B (31%), D (15%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**spring21** (Wed 18 Dec 2024 01:15) - *Upvotes: 5*
To reduce the frequency of blocked write operations in an Amazon Aurora PostgreSQL global database, particularly when you're using a 60-second Recovery Point Objective (RPO), the goal is to improve replication performance so that the lag between the primary cluster and its secondary regions does not exceed the 60-second threshold.

---

**0ac7838** (Fri 14 Nov 2025 07:21) - *Upvotes: 1*
C. Remove one of the secondary clusters from the global database.

Fewer secondary clusters → less replication overhead on the primary.

This can reduce write blocking caused by the replication requirement to meet RPO.

This is a viable solution.

---

**DKM** (Tue 18 Mar 2025 17:06) - *Upvotes: 1*
Removing a secondary cluster can help alleviate write contention on the primary cluster, improving overall performance.

---

**teo2157** (Thu 16 Jan 2025 16:16) - *Upvotes: 4*
Enabling write forwarding for an Amazon Aurora global database can be a viable solution to improve write performance and reduce the frequency of blocked write operations. Write forwarding allows secondary clusters in a global database to forward write requests to the primary cluster. This can help distribute the write load and improve the overall performance of the global database.

---

**4eaa7a4** (Mon 24 Nov 2025 06:22) - *Upvotes: 1*
not sure how the repl of the secondary cluster write will work but given that it will eventually have to be replicated to two other clusters its RPO will likely go over 60 seconds as well

---

**youonebe** (Fri 27 Dec 2024 16:51) - *Upvotes: 2*
Explanation: Aurora global databases use asynchronous replication by default to provide fast cross-region replication. However, asynchronous replication can cause delays in synchronizing data between regions, and if the RPO is set too aggressively, it can cause write operations to block when replication is behind.

By configuring synchronous replication, writes to the primary region will only be acknowledged once the changes are successfully replicated to the secondary regions, reducing the likelihood of replication lag. This means that write operations will be blocked if the replication is lagging and the RPO setting is not met, but the frequency of blocked writes may be reduced because synchronous replication guarantees data consistency across regions.

---

**4eaa7a4** (Mon 24 Nov 2025 06:17) - *Upvotes: 1*
i reckon RPO make sense in an async replication mode where you need to know how far "back" the secondary clusters are compared to the primary; with a sync replication, it is the client posting of the query/command to the primary cluster which will have to wait/be blocked for/until all clusters to be in sync; as such RPO doesn't make sense in this case

---


<br/>

## Question 298

*Date: t200*

A company has a web application that is hosted on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster runs on AWS Fargate that is available through an internet-facing Application Load Balancer.

The application is experiencing stability issues that lead to longer response times. A DevOps engineer needs to configure observability in Amazon CloudWatch to troubleshoot the issue. The solution must provide only the minimum necessary permissions.

Which combination of steps will meet these requirements? (Choose three.)

**Options:**
- A. Deploy the CloudWatch agent as a Kubernetes StatefulSet to the EKS cluster.
- B. Deploy the AWS Distro for OpenTelemetry Collector as a Kubernetes DaemonSet to the EKS cluster.
- C. Associate a Kubernetes service account with an IAM role by using IAM roles for service accounts in Amazon EKS. Use the CloudWatchAgentServerPolicy AWS managed policy.
- D. Associate a Kubernetes service account with an IAM role by using IAM roles for service accounts in Amazon EKS. Use the CloudWatchAgentAdminPolicy AWS managed policy.
- E. Configure an IAM OpenID Connect (OIDC) provider for the EKS cluster.
- F. Enable EKS control plane logging for the EKS cluster.

> **Suggested Answer:** ACF
> **Community Vote:** ACF (57%), BCE (33%), 5%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**matt200** (Mon 30 Dec 2024 09:10) - *Upvotes: 6*
B: AWS Distro for OpenTelemetry (ADOT) is the recommended solution for collecting metrics and traces from EKS clusters
E: OIDC provider is required to use IRSA

A: Incorrect because For EKS on Fargate, ADOT is the recommended solution
F: Incorrect because this alone won't provide the application-level observability needed

---

**CHRIS12722222** (Thu 26 Dec 2024 00:31) - *Upvotes: 6*
https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/deploy-container-insights-EKS.html

- No daemonsets for fargate
- CloudWatchAgentServerPolicy is correct
- controlplane logging

---

**syaldram** (Wed 17 Sep 2025 21:20) - *Upvotes: 1*
I say ACF because AWS Distro is mainly used for tracing and not logging. I think CW agent is the way to go for logging purposes.

---

**GripZA** (Tue 22 Apr 2025 13:29) - *Upvotes: 1*
B not A - it's best practice to use Container Insights to collect metrics on Amazon EKS and Kubernetes. To use Container Insights with Fargate, you must use AWS Distro for OpenTelemetry.

C not D for minimum necessary permissions.

F - EKS control plane logging provides audit and diagnostic logs directly from the Amazon EKS control plane to CloudWatch Logs in your account

---

**SysOps** (Fri 28 Feb 2025 22:38) - *Upvotes: 1*
A instead of B only because Daemonsets aren’t supported on Fargate

---

**jojewi8143** (Sun 16 Feb 2025 18:58) - *Upvotes: 5*
Changed mind to ACF. Daemonsets arent supported on Fargate, B is not possible. https://docs.aws.amazon.com/eks/latest/userguide/fargate.html

---

**jojewi8143** (Sun 02 Feb 2025 13:03) - *Upvotes: 1*
im for bce

---


<br/>

## Question 299

*Date: t200*

A company stores its Python-based application code in AWS CodeCommit. The company uses AWS CodePipeline to deploy the application. The CodeCommit repository and the CodePipeline pipeline are deployed to the same AWS account.

The company's security team requires all code to be scanned for vulnerabilities before the code is deployed to production. If any vulnerabilities are found, the deployment must stop.

Which solution will meet these requirements?

**Options:**
- A. Create a new CodeBuild project. Configure the project to run a security scan on the code by using Amazon CodeGuru Security. Configure the CodeBuild project to raise an error if CodeGuru Security finds vulnerabilities. Create a new IAM role that has sufficient permissions to run CodeGuru Security scans. Assign the role to the CodeBuild project. In the CodePipeline pipeline, add a new stage before the deployment stage. Select AWS CodeBuild as the action provider for the new stage. Use the source artifact from the CodeCommit repository. Configure the action to use the CodeBuild project.
- B. Create a new CodeBuild project. Configure the project to run a security scan on the code by using Amazon Inspector. Configure the CodeBuild project to raise an error if Amazon Inspector finds vulnerabilities. Create a new IAM role that has sufficient permissions to run Amazon Inspector scans. Assign the role to the CodeBuild project. In the CodePipeline pipeline, add a new stage before the deployment stage. Select AWS CodeBuild as the action provider for the new stage. Use the source artifact from the CodeCommit repository. Configure the action to use the CodeBuild project.
- C. Update the IAM role that is attached to CodePipeline to include sufficient permissions to invoke Amazon DevOps Guru. In the CodePipeline pipeline, add a new stage before the deployment stage. Select DevOps Guru as the action provider for the new stage. Use the source artifact from the CodeCommit repository.
- D. Update the IAM role that is attached to CodePipeline to include sufficient permissions to invoke Amazon DevOps Guru. In the CodePipeline pipeline, add a new stage before the deployment stage. Select CodeGuru Security as the action provider for the new stage. Use the source artifact from the CodeCommit repository.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**CHRIS12722222** (Thu 26 Dec 2024 00:37) - *Upvotes: 5*
it is codeguru for static code analysis

NOT devops guru
No need for inspector

---

**teo2157** (Wed 12 Feb 2025 08:10) - *Upvotes: 3*
Amazon CodeGuru Security detects, tracks, and fixes code security vulnerabilities anywhere in the development cycle using ML and automated reasoning

---


<br/>

## Question 300

*Date: Nov. 17, 2024, 10:15 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer deploys an application to a fleet of Amazon Linux EC2 instances. The DevOps engineer needs to monitor system metrics across the fleet. The DevOps engineer wants to monitor the relationship between network traffic and memory utilization for the application code. The DevOps engineer wants to track the data on a 60 second interval.

Which solution will meet these requirements?

**Options:**
- A. Use Amazon CloudWatch basic monitoring to collect the NetworkIn metric and the MemoryBytesUsed metric. Graph the metrics in CloudWatch.
- B. Use Amazon CloudWatch detailed monitoring to collect the NetworkIn metric and the MemoryBytesUsed metric. Graph the metrics in CloudWatch.
- C. Use Amazon CloudWatch detailed monitoring to collect the NetworkIn metric. Install the CloudWatch agent on the EC2 instances to collect the mem_used metric. Graph the metrics in CloudWatch.
- D. Use Amazon CloudWatch basic monitoring to collect the built-in NetworkIn metric. Install the CloudWatch agent on the EC2 instances to collect the mem_used metric. Graph the metrics in CloudWatch.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**f4b18ba** (Sun 17 Nov 2024 22:15) - *Upvotes: 9*
Answer: C
Detailed Monitoring for High Resolution: Detailed monitoring provides metrics at a 1-minute frequency, which meets the requirement of tracking data on a 60-second interval. Basic monitoring only provides 5-minute data.
NetworkIn Metric: The NetworkIn metric is a standard metric available through detailed monitoring, providing insights into incoming network traffic.
CloudWatch Agent for Memory Utilization: While MemoryBytesUsed is available with basic monitoring, the CloudWatch agent allows you to collect more specific memory metrics like mem_used, which might be more relevant for analyzing application code memory usage. This provides greater flexibility and granularity in monitoring memory.
Correlating Metrics: By collecting both NetworkIn and mem_used in CloudWatch, you can graph and analyze them together to understand the relationship between network traffic and memory utilization for your application.

---

**CHRIS12722222** (Thu 26 Dec 2024 00:39) - *Upvotes: 2*
Use Amazon CloudWatch detailed monitoring to collect the NetworkIn metric. Install the CloudWatch agent on the EC2 instances to collect the mem_used metric. Graph the metrics in CloudWatch.

---


<br/>

## Question 301

*Date: Nov. 28, 2024, 9:31 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses AWS Systems Manager to manage a fleet of Amazon Linux EC2 instances that have SSM Agent installed. All EC2 instances are configured to use Instance Metadata Service Version 2 (IMDSv2) and are running in the same AWS account and AWS Region. Company policy requires developers to use only Amazon Linux.

The company wants to ensure that all new EC2 instances are automatically managed by Systems Manager after creation.

Which solution will meet these requirements with the MOST operational efficiency?

**Options:**
- A. Create an IAM role that has a trust policy that allows Systems Manager to assume the role. Attach the AmazonSSMManagedEC2InstanceDefaultPolicy policy to the role. Configure the default-ec2-instance-management-role SSM service setting to use the role.
- B. Ensure that AWS Config is set up. Create an AWS Config rule that validates if an EC2 instance has SSM Agent installed. Configure the rule to run on EC2 configuration changes. Configure automatic remediation for the rule to run the AWS-InstallSSMAgent SSM document to install SSM Agent.
- C. Configure Systems Manager Patch Manager. Create a patch baseline that automatically installs SSM Agent on all new EC2 instances. Create a patch group for all EC2 instances. Attach the patch baseline to the patch group. Create a maintenance window and maintenance window task to start installing SSM Agent daily.
- D. Create an EC2 instance role that has a trust policy that allows Amazon EC2 to assume the role. Attach the AmazonSSMManagedInstanceCore policy to the role. Ensure that AWS Config is set up. Use the ec2-instance-profile-attached managed AWS Config rule to validate if an EC2 instance has the role attached. Configure the rule to run on EC2 configuration changes. Configure automatic remediation for the rule to run the AWS-SetupManagedRoleOnEc2Instance SSM document to attach the role to the EC2 instance.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Mon 31 Mar 2025 02:33) - *Upvotes: 1*
By setting the default-ec2-instance-management-role service setting, new EC2 instances will automatically assume the correct role, allowing seamless management by AWS Systems Manager.

Thus, Option A is the best choice.

---

**Ky_24** (Sun 15 Dec 2024 20:59) - *Upvotes: 3*
1. Automatic Role Association:
• AWS Systems Manager supports a default instance management role that is automatically attached to new EC2 instances upon creation.
• By configuring the default-ec2-instance-management-role SSM service setting, any new EC2 instance will automatically be associated with the specified IAM role.
2. IAM Role and Policy:
• The AmazonSSMManagedEC2InstanceDefaultPolicy provides the necessary permissions for SSM Agent to manage instances, including access to Systems Manager services, Amazon S3, and AWS Config logs.
3. Operational Efficiency:
• This solution ensures new EC2 instances are automatically registered with Systems Manager without requiring additional manual steps or configuration changes.
• It eliminates the need for AWS Config rules, patch baselines, or remediation documents, simplifying the management process.

---

**ArunRav** (Thu 28 Nov 2024 09:31) - *Upvotes: 3*
Amazon Linux has the agent already installed. So A perform the rest of the steps to manage the instances using SSM

---


<br/>

## Question 302

*Date: Nov. 20, 2024, 5:10 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company configured an Amazon S3 event source for an AWS Lambda function. The company needs the Lambda function to run when a new object is created or an existing object is modified in a specific S3 bucket. The Lambda function will use the S3 bucket name and the S3 object key of the incoming event to read the contents of the new or modified S3 object. The Lambda function will parse the contents and save the parsed contents to an Amazon DynamoDB table.

The Lambda function's execution role has permissions to read from the S3 bucket and to write to the DynamoDB table. During testing, a DevOps engineer discovers that the Lambda function does not run when objects are added to the S3 bucket or when existing objects are modified.

Which solution will resolve these problems?

**Options:**
- A. Create an S3 bucket policy for the S3 bucket that grants the S3 bucket permission to invoke the Lambda function.
- B. Create a resource policy for the Lambda function to grant Amazon S3 permission to invoke the Lambda function on the S3 bucket.
- C. Configure an Amazon Simple Queue Service (Amazon SQS) queue as an OnFailure destination for the Lambda function. Update the Lambda function to process messages from the SQS queue and the S3 event notifications.
- D. Configure an Amazon Simple Queue Service (Amazon SQS) queue as the destination for the S3 bucket event notifications. Update the Lambda function's execution role to have permission to read from the SQS queue. Update the Lambda function to consume messages from the SQS queue.

> **Suggested Answer:** B
> **Community Vote:** B (90%), 10%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Tue 01 Apr 2025 02:22) - *Upvotes: 1*
Event Trigger Issue:
The Lambda function is not being triggered when objects are created or modified in the S3 bucket.
A common reason for this issue is that Amazon S3 does not have the necessary permissions to invoke the Lambda function.
Lambda Function Resource Policy:
Unlike some AWS services (e.g., SNS, SQS), which can automatically invoke Lambda if they have permissions in their service role, S3 requires an explicit resource-based policy on the Lambda function to grant it invocation permissions.
This means S3 needs explicit permission to trigger the Lambda function when events occur.
Why Option B Works:
Adding a resource-based policy to the Lambda function allows S3 to invoke the function when an event (PUT/POST operation) occurs.
The policy should include an "s3.amazonaws.com" principal and an action of "lambda:InvokeFunction", granting S3 the ability to trigger Lambda.

---

**Ky_24** (Sun 15 Dec 2024 21:19) - *Upvotes: 2*
1. S3-to-Lambda Invocation:
• When you configure Amazon S3 to trigger an AWS Lambda function, S3 must have explicit permission to invoke the function. This is done by attaching a resource-based policy to the Lambda function.
• Without this policy, even if S3 event notifications are configured, the Lambda function will not be triggered because S3 does not have the necessary permissions to invoke the function.
2. Solution Details:
• A resource policy for the Lambda function specifies that the S3 bucket is allowed to invoke the Lambda function. This is configured by using the AWS CLI, AWS SDKs, or directly in the AWS Management Console.

---

**ArunRav** (Thu 28 Nov 2024 09:41) - *Upvotes: 3*
Lambda should allow to be invoked by S3 bucket Hence B

---

**Impromptu** (Fri 22 Nov 2024 15:22) - *Upvotes: 3*
https://docs.aws.amazon.com/lambda/latest/dg/access-control-resource-based.html

The lambda resource policy should allow S3. Or in cloudformation terms: adding the AWS::Lambda::Permission

---

**f4b18ba** (Wed 20 Nov 2024 17:10) - *Upvotes: 1*
Event Source Mapping and Permission Requirements:

For Amazon S3 to invoke a Lambda function, the Lambda function must have a resource-based policy that grants Amazon S3 permission to invoke it.
Without this resource-based policy, even if the event source is correctly configured, the Lambda function will not be triggered because S3 is not authorized to invoke it.
Resource Policy for Lambda Function:

A resource policy on the Lambda function explicitly allows Amazon S3 to invoke the function for the specified bucket events (e.g., object creation or modification).
This resolves the issue by enabling S3 to trigger the Lambda function

Action - lambda:InvokeFunction allows S3 to invoke the Lambda function.

https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html

---

**Impromptu** (Fri 22 Nov 2024 15:19) - *Upvotes: 1*
In your link it states "To invoke your function, Amazon S3 needs permission from the function's resource-based policy."

Note "FUNCTION's resource-based policy"

So therefore it is B. We do not have to edit the bucket policy for this.

---


<br/>

## Question 303

*Date: Nov. 17, 2024, 10:32 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company recently configured AWS Control Tower in its organization in AWS Organizations. The company enrolled all existing AWS accounts in AWS Control Tower. The company wants to ensure that all new AWS accounts are automatically enrolled in AWS Control Tower.

The company has an existing AWS Step Functions workflow that creates new AWS accounts and performs any actions required as part of account creation. The Step Functions workflow is defined in the same AWS account as AWS Control Tower.

Which combination of steps should the company add to the Step Functions workflow to meet these requirements? (Choose two.)

**Options:**
- A. Create an Amazon EventBridge event that has an aws.controltower source and a CreateManagedAccount detail-type. Add the details of the new AWS account to the detail field of the event.
- B. Create an Amazon EventBridge event that has an aws.controltower source and a SetupLandingZone detail-type. Add the details of the new AWS account to the detail field of the event.
- C. Create an AWSControlTowerExecution role in the new AWS account. Configure the role to allow the AWS Control Tower administrator account to assume the role.
- D. Call the AWS Service Catalog ProvisionProduct API operation with the details of the new AWS account.
- E. Call the Organizations EnableAWSServiceAccess API operation with the controltower.amazonaws.com service name and the details of the new AWS account.

> **Suggested Answer:** CD
> **Community Vote:** CD (78%), CE (22%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**teo2157** (Fri 17 Jan 2025 09:59) - *Upvotes: 1*
Agrees with CE based on the blog that CHRIS1272222 provided

---

**youonebe** (Fri 27 Dec 2024 18:17) - *Upvotes: 1*
C - AWS Control Tower requires the AWSControlTowerExecution role to be created in each managed account. This role allows AWS Control Tower to manage the account and enforce governance and compliance rules. When a new account is created, AWS Control Tower will need this role to carry out management tasks.

E - The EnableAWSServiceAccess API operation is used to enable AWS Control Tower service access in AWS Organizations. This action ensures that AWS Control Tower can operate across the organization and manage new accounts that are created within the organization. By enabling service access for Control Tower, new accounts can be automatically enrolled in the governance and management processes of Control Tower.

---

**CHRIS12722222** (Fri 27 Dec 2024 16:19) - *Upvotes: 3*
Read blog
https://aws.amazon.com/blogs/architecture/field-notes-enroll-existing-aws-accounts-into-aws-control-tower/

Download the python code and you will see it calls the ProvisionProduct API in method provision_sc_product

---

**Ky_24** (Sun 15 Dec 2024 21:24) - *Upvotes: 4*
Option Details:

1. C. Create an AWSControlTowerExecution role:
• AWS Control Tower requires an AWSControlTowerExecution role in new accounts.
• This role allows AWS Control Tower to assume control of the account and apply the necessary guardrails, policies, and configurations.
• Without this role, AWS Control Tower cannot manage the account.
2. D. Call the AWS Service Catalog ProvisionProduct API operation:
• Account Factory uses AWS Service Catalog to create and enroll new accounts into AWS Control Tower.
• The ProvisionProduct API operation allows programmatic provisioning of new accounts through Account Factory, ensuring enrollment into Control Tower governance.

---

**f4b18ba** (Sun 17 Nov 2024 22:33) - *Upvotes: 2*
Answer: CD (had a typo)

---

**f4b18ba** (Sun 17 Nov 2024 22:32) - *Upvotes: 3*
Answer: CE
WSControlTowerExecution Role (Option C): For AWS Control Tower to manage accounts, each account must have the AWSControlTowerExecution role, which allows the AWS Control Tower administrator account to assume the role and apply required policies and controls. Creating this role in the new account enables Control Tower to perform management operations as needed.

Service Catalog ProvisionProduct API (Option D): AWS Control Tower uses AWS Service Catalog products to provision and manage accounts. Calling the ProvisionProduct API operation as part of the Step Functions workflow allows the new account to be enrolled in Control Tower by provisioning it through the appropriate Service Catalog product. This step ensures that the new account is enrolled in the AWS Control Tower landing zone.

---


<br/>

## Question 304

*Date: Nov. 20, 2024, 5:14 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company's web application uses an Application Load Balancer (ALB) to direct traffic to Amazon EC2 instances across three Availability Zones.

The company has deployed a newer version of the application to one Availability Zone for testing. If a problem is detected with the application, the company wants to direct traffic away from the affected Availability Zone until the deployment has been rolled back. The application must remain available and maintain static stability during the rollback.

Which solution will meet these requirements with the MOST operational efficiency?

**Options:**
- A. Disable cross-zone load balancing on the ALB's target group. Initiate a zonal shift on the ALB to direct traffic away from the affected Availability Zone.
- B. Disable cross-zone load balancing on the ALB's target group. Manually remove instances in the target group that belong to the affected Availability Zone.
- C. Configure cross-zone load balancing on the ALB's target group to inherit settings from the ALB. Initiate a zonal shift on the ALB to direct traffic away from the affected Availability Zone.
- D. Configure cross-zone load balancing on the ALB's target group to inherit settings from the ALB. Remove the subnet that is associated with the affected Availability Zone.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**awspro_winjin** (Sat 24 May 2025 10:39) - *Upvotes: 1*
ARC + Route53

---

**Srikantha** (Tue 01 Apr 2025 02:37) - *Upvotes: 1*
Why Option A?
Zonal Shifts Provide Quick and Controlled Traffic Redirection
AWS Route 53 Application Recovery Controller allows you to initiate a zonal shift to temporarily remove an entire Availability Zone (AZ) from service.
This is operationally efficient as it does not require manually removing instances or modifying target groups.
Disabling Cross-Zone Load Balancing Ensures Proper Traffic Steering
If cross-zone load balancing is enabled, the ALB will continue distributing traffic across all AZs even if one AZ is experiencing issues.
Disabling it ensures that a zonal shift properly removes traffic from the affected AZ.
Maintains Availability and Stability
Other AZs will continue handling traffic without disruption.
The rollback process can proceed without affecting users.

---

**matt200** (Wed 25 Dec 2024 06:31) - *Upvotes: 2*
should be A

---

**f4b18ba** (Wed 20 Nov 2024 17:14) - *Upvotes: 4*
Disable Cross-Zone Load Balancing:

Disabling cross-zone load balancing ensures that each Availability Zone handles traffic only for its own targets. This enables the ability to isolate traffic away from a specific zone when needed.
Initiate a Zonal Shift:

AWS offers Zonal Shift (through AWS Elastic Disaster Recovery or AWS Global Accelerator) to temporarily shift traffic away from a specific Availability Zone. This is highly operationally efficient because it allows automated rerouting without manual changes to target groups or instance configurations.
A zonal shift can quickly redirect traffic while maintaining high availability and stability for the application during rollback or testing.
Ref -https://docs.aws.amazon.com/r53recovery/latest/dg/arc-zonal-shift.html

---


<br/>

## Question 305

*Date: Nov. 20, 2024, 5:21 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has several AWS accounts. An Amazon Connect instance runs in each account. The company uses an Amazon EventBridge default event bus in each account for event handling.

A DevOps team needs to receive all the Amazon Connect events in a single DevOps account.

Which solution meets these requirements?

**Options:**
- A. Update the resource-based policy of the default event bus in each account to allow the DevOps account to replay events. Configure an EventBridge rule in the DevOps account that matches Amazon Connect events and has a target of the default event bus in the other accounts.
- B. Update the resource-based policy of the default event bus in each account to allow the DevOps account to receive events. Configure an EventBridge rule in the DevOps account that matches Amazon Connect events and has a target of the default event bus in the other accounts.
- C. Update the resource-based policy of the default event bus in the DevOps account. Update the policy to allow events to be received from the accounts. Configure an EventBridge rule in each account that matches Amazon Connect events and has a target of the DevOps account's default event bus.
- D. Update the resource-based policy of the default event bus in the DevOps account. Update the policy to allow events to be replayed by the accounts. Configure an EventBridge rule in each account that matches Amazon Connect events and has a target of the DevOps account's default event bus.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Tue 01 Apr 2025 02:46) - *Upvotes: 1*
Explanation:
1. Event Flow Across AWS Accounts

The goal is to consolidate Amazon Connect events from multiple AWS accounts into a single DevOps account.
Amazon EventBridge allows cross-account event ingestion using a resource-based policy on the receiving event bus (DevOps account).
2. Correct Configuration Steps

✅ Step 1: Update the resource-based policy of the default event bus in the DevOps account

This allows it to receive events from other accounts.
The policy must specify the source accounts and the events.amazonaws.com principal.
✅ Step 2: Create EventBridge rules in each Amazon Connect account

These rules match Amazon Connect events and forward them to the DevOps account's default event bus.

---

**teo2157** (Fri 17 Jan 2025 11:28) - *Upvotes: 2*
Agree with f4b18ba comments

---

**Ky_24** (Sun 15 Dec 2024 21:50) - *Upvotes: 3*
Explanation:

To centralize Amazon Connect events from multiple AWS accounts into a single account’s EventBridge event bus, the following steps are required:
1. Update the resource-based policy of the EventBridge event bus in the DevOps account:
• This policy allows the DevOps account’s event bus to accept events from the other accounts.
• The policy must specify the sending account IDs in the Principal field and grant permissions for actions like events:PutEvents.
2. Create EventBridge rules in each Amazon Connect account:
• These rules match the specific Amazon Connect events (e.g., contact events, agent status updates) and forward them to the default event bus in the DevOps account.

---

**f4b18ba** (Wed 20 Nov 2024 17:21) - *Upvotes: 3*
Resource-Based Policy on the DevOps Account's Event Bus:

To allow cross-account event routing, the DevOps account's EventBridge event bus must have a resource-based policy that grants permissions to other accounts to send events to it.

EventBridge Rule in Each Account:

Each account needs an EventBridge rule that matches the desired Amazon Connect events and sends them to the DevOps account's event bus as the target. This ensures all relevant events are aggregated in the DevOps account.

Cross-Account Event Routing:
EventBridge supports cross-account event routing with a combination of resource-based policies and properly configured rules in the source accounts.

https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cross-account.html
https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-event-bus-permissions.html
https://docs.aws.amazon.com/connect/latest/adminguide/eventbridge.html

---


<br/>

## Question 306

*Date: Nov. 28, 2024, 10:01 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has deployed an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Amazon EC2 node groups. The company's DevOps team uses the Kubernetes Horizontal Pod Autoscaler and recently installed a supported EKS cluster Autoscaler.

The DevOps team needs to implement a solution to collect metrics and logs of the EKS cluster to establish a baseline for performance. The DevOps team will create an initial set of thresholds for specific metrics and will update the thresholds over time as the cluster is used. The DevOps team must receive an Amazon Simple Notification Service (Amazon SNS) email notification if the initial set of thresholds is exceeded or if the EKS cluster Autoscaler is not functioning properly.

The solution must collect cluster, node, and pod metrics. The solution also must capture logs in Amazon CloudWatch.

Which combination of steps should the DevOps team take to meet these requirements? (Choose three.)

**Options:**
- A. Deploy the CloudWatch agent and Fluent Bit to the cluster. Ensure that the EKS cluster has appropriate permissions to send metrics and logs to CloudWatch.
- B. Deploy AWS Distro for OpenTelemetry to the cluster. Ensure that the EKS cluster has appropriate permissions to send metrics and logs to CloudWatch.
- C. Create CloudWatch alarms to monitor the CPU, memory, and node failure metrics of the cluster. Configure the alarms to send an SNS email notification to the DevOps team if thresholds are exceeded.
- D. Create a CloudWatch composite alarm to monitor a metric log filter of the CPU, memory, and node metrics of the cluster. Configure the alarm to send an SNS email notification to the DevOps team when anomalies are detected.
- E. Create a CloudWatch alarm to monitor the logs of the Autoscaler deployments for errors. Configure the alarm to send an SNS email notification to the DevOps team if thresholds are exceeded.
- F. Create a CloudWatch alarm to monitor a metric log filter of the Autoscaler deployments for errors. Configure the alarm to send an SNS email notification to the DevOps team if thresholds are exceeded.

> **Suggested Answer:** ACF
> **Community Vote:** ACF (69%), ADF (15%), Other, Other, A (35%), C (25%), B (20%), Other

### Discussions

**robotgeek** (Wed 21 May 2025 11:01) - *Upvotes: 2*
You need the logs, then you go A instead of B.
And if you are going A, you cannot go C, you have to go D, then of course F

---

**Srikantha** (Wed 02 Apr 2025 00:49) - *Upvotes: 1*
Explanation
To collect logs and metrics from the EKS cluster, nodes, and pods, and to ensure notifications are sent when thresholds are exceeded, we need:

A mechanism to collect logs and metrics (Option A).
Alarms for key cluster performance metrics (Option C).
Alarms to detect Autoscaler failures (Option F).

---

**matt200** (Mon 30 Dec 2024 09:40) - *Upvotes: 1*
Correct:

B. Deploy AWS Distro for OpenTelemetry (ADOT):
ADOT is the recommended solution for collecting metrics and logs from EKS clusters


E. Create CloudWatch alarm for Autoscaler logs:
Monitors Autoscaler functionality through log analysis

Wrong:
A. Deploy CloudWatch agent and Fluent Bit:
While this would work, it's not the recommended approach for EKS
F. Create CloudWatch alarm with metric log filter for Autoscaler:
Direct log monitoring (Option E) is more appropriate

---

**matt200** (Mon 30 Dec 2024 09:50) - *Upvotes: 2*
change my mind to ACF

---

**luisfsm_111** (Tue 10 Dec 2024 14:36) - *Upvotes: 4*
A collects metrics and logs, C create the alarms and F monitors the auto scaler

---

**tinyshare** (Wed 04 Dec 2024 10:14) - *Upvotes: 1*
The question asks to "collect metrics and logs".
You need to install the CloudWatch Agent which is A.
You need to collect metrics which is C.
You need to collect logs which is E.

---

**ArunRav** (Thu 28 Nov 2024 10:01) - *Upvotes: 4*
A- To collect the cloudwatch logs and send to cloudwatch service.
C - To setup Alarms
F - To monitor and alert

---


<br/>

## Question 307

*Date: Nov. 20, 2024, 5:54 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company discovers that its production environment and disaster recovery (DR) environment are deployed to the same AWS Region. All the production applications run on Amazon EC2 instances and are deployed by AWS CloudFormation. The applications use an Amazon FSx for NetApp ONTAP volume for application storage. No application data resides on the EC2 instances.

A DevOps engineer copies the required AMIs to a new DR Region. The DevOps engineer also updates the CloudFormation code to accept a Region as a parameter. The storage needs to have an RPO of 10 minutes in the DR Region.

Which solution will meet these requirements?

**Options:**
- A. Create an Amazon S3 bucket in both Regions. Configure S3 Cross-Region Replication (CRR) for the S3 buckets. Create a scheduled AWS Lambda function to copy any new content from the FSx for ONTAP volume to the S3 bucket in the production Region.
- B. Use AWS Backup to create a backup vault and a custom backup plan that has a 10-minute frequency. Specify the DR Region as the target Region. Assign the EC2 instances in the production Region to the backup plan.
- C. Create an AWS Lambda function to create snapshots of the instance store volumes that are attached to the EC2 instances. Configure the Lambda function to copy the snapshots to the DR Region and to remove the previous copies. Create an Amazon EventBridge scheduled rule that invokes the Lambda function every 10 minutes.
- D. Create an FSx for ONTAP instance in the DR Region. Configure a 5-minute schedule for a volume-level NetApp SnapMirror to replicate the volume from the production Region to the DR Region.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Wed 02 Apr 2025 01:04) - *Upvotes: 1*
Option D: Use FSx for ONTAP SnapMirror with a 5-minute replication schedule.
This ensures continuous, low-latency replication and meets the 10-minute RPO requirement efficiently.

---

**matt200** (Wed 25 Dec 2024 06:32) - *Upvotes: 2*
should be D

---

**f4b18ba** (Wed 20 Nov 2024 17:54) - *Upvotes: 2*
Implementation Steps :
Create an FSx for ONTAP Instance in the DR Region:

Deploy an FSx for NetApp ONTAP instance in the new DR Region.
Set Up SnapMirror Replication:

Configure SnapMirror from the FSx volume in the production Region to the FSx instance in the DR Region.
Set the replication schedule to 5 minutes to meet the 10-minute RPO requirement.
Test the DR Setup:

Verify that the replicated volume in the DR Region is consistent and accessible.
Ensure that the DR environment can failover to the replicated FSx volume.
Update CloudFormation:

Ensure the updated CloudFormation templates can deploy EC2 instances in the DR Region and mount the replicated FSx volume.

---


<br/>

## Question 308

*Date: Nov. 20, 2024, 2:47 a.m.
Disclaimers:
- ExamTopics website is not rel*

During a security audit, a company discovered that some security groups allow SSH traffic from 0.0.0.0/0. A security team must implement a solution to detect and remediate this issue as soon as possible. The company uses one organization in AWS Organizations to manage all the company's AWS accounts.

Which solution will meet these requirements?

**Options:**
- A. Enable AWS Config for all AWS accounts. Use a periodic trigger to activate the vpe-sg-port-restriction-check AWS Config rule. Create an AWS Lambda function to remediate any noncompliant rules.
- B. Create an AWS Lambda function in each AWS account to delete all the security group rules. Create an Amazon EventBridge rule to match security group update events or creation events. Set the Lambda function in each account as a target for the rule.
- C. Enable AWS Config for all AWS accounts. Create a custom AWS Config rule to run on the restricted-ssh configuration change trigger. Configure the rule to invoke an AWS Lambda function to remediate any noncompliant resources.
- D. Create an AWS Systems Manager Automation document in each account to inspect all security groups and to delete noncompliant rules. Use an Amazon EventBridge rule to run the Automation document every hour.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**phu0298** (Wed 20 Nov 2024 02:47) - *Upvotes: 5*
C
why not A:

The vpe-sg-port-restriction-check AWS Config rule is not specific to this use case.
The periodic trigger does not provide real-time detection, potentially delaying remediation.

---

**Srikantha** (Wed 02 Apr 2025 01:14) - *Upvotes: 1*
Option C: AWS Config + real-time rule evaluation + Lambda remediation.
This provides real-time security enforcement across all accounts.

---

**Slays** (Sun 15 Dec 2024 13:16) - *Upvotes: 4*
This option involves enabling AWS Config across all accounts, deploying the restricted-ssh rule, and setting up automatic remediation to address non-compliant security groups, thereby meeting the requirements efficiently.

---


<br/>

## Question 309

*Date: Dec. 1, 2024, 3:05 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company's DevOps engineer must install a software package on 30 on-premises VMs and 15 Amazon EC2 instances.

The DevOps engineer needs to ensure that all VMs receive the package in a process that is auditable and that any configuration drift on the VMs is automatically identified and alerted on. The company uses AWS Direct Connect to connect its on-premises data center to AWS.

Which solution will meet these requirements with the MOST operational efficiency?

**Options:**
- A. Write a script that iterates through the list of VMs once a week. Configure the script to check for the package and install the package if the package is not found. Configure the script to send an email message notification to the system administrator if the package is not found.
- B. Install the AWS Systems Manager Agent (SSM Agent) on all VMs. Use the SSM Agent to install the package. Use AWS Config to monitor for configuration drift. Use Amazon Simple Notification Service (Amazon SNS) to notify the system administrator if any drift is found.
- C. Write a script that checks if the package is installed across the environment. Configure the script to create a list of all VMs that are noncompliant. Configure the script to send the list to the system administrator, who will install the package on the noncompliant VMs.
- D. Log in to each VM. Use a local package manager to install the package. Use AWS Config to monitor the AWS resources for configuration changes. Write a script to monitor the on-premises resources.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Wed 02 Apr 2025 01:25) - *Upvotes: 1*
ption B: Install the AWS Systems Manager Agent (SSM Agent) on all VMs. Use the SSM Agent to install the package. Use AWS Config to monitor for configuration drift. Use Amazon SNS to notify the system administrator if any drift is found.

---

**gunjan229** (Sun 01 Dec 2024 15:05) - *Upvotes: 3*
Coorect Ans B

---


<br/>

## Question 310

*Date: Dec. 1, 2024, 3:05 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an AWS CodePipeline pipeline in the eu-west-1 Region. The pipeline stores the build artifacts in an Amazon S3 bucket. The pipeline builds and deploys an AWS Lambda function by using an AWS CloudFormation deploy action.

A DevOps engineer needs to update the existing pipeline to also deploy the Lambda function to the us-east-1 Region. The pipeline has already been updated to create an additional artifact to deploy to us-east-1.

Which combination of steps should the DevOps engineer take to meet these requirements? (Choose two.)

**Options:**
- A. Modify the CloudFormation template to include a parameter for the Lambda function code's .zip file location. Create a new CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to pass in the us-east-1 artifact location as a parameter override.
- B. Create a new CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to use the CloudFormation template from the additional artifact that was created for us-east-1.
- C. Create an S3 bucket in us-east-1. Configure the S3 bucket policy to allow CodePipeline to have read and write access.
- D. Create an S3 bucket in us-east-1. Configure S3 Cross-Region Replication (CRR) from the S3 bucket in eu-west-1 to the S3 bucket in us-east-1.
- E. Modify the pipeline to include the S3 bucket for us-east-1 as an artifact store. Create a new CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to use the CloudFormation template from the us-east-1 artifact.

> **Suggested Answer:** CE
> **Community Vote:** CE (93%), 7%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**eugene2owl** (Mon 09 Dec 2024 15:07) - *Upvotes: 5*
Correct ansers "C" and "E".
"A" is wrong for sure, because it's not enough to simply bypass a new Lambda code location as a parameter of existing CloudFormation Template: you need NOT only another Lambda code location, but also another region, another VPC id, etc.
So you need to have a second dedicated separate template.

---

**Srikantha** (Sun 06 Apr 2025 15:08) - *Upvotes: 3*
Question says pipeline already modified to create the artifact in us-east-1

---

**asimohat** (Thu 13 Mar 2025 15:42) - *Upvotes: 1*
The question states “The pipeline has already been updated to create additional deliverables to deploy to us-east-1.
Option B simply adds a deploy action in a way that leverages this existing update.

---

**tubtab** (Wed 25 Dec 2024 15:40) - *Upvotes: 3*
Correct

---

**gunjan229** (Sun 01 Dec 2024 15:05) - *Upvotes: 3*
Correct Answer C, E

---


<br/>

## Question 311

*Date: Nov. 27, 2024, 4:05 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses an AWS Cloud Development Kit (AWS CDK) application for its infrastructure. The AWS CDK application creates AWS Lambda functions and the IAM roles that are attached to the functions. The company also uses AWS Organizations. The company's developers can assume the AWS CDK application deployment role.

The company's security team discovered that the developers and the role used to deploy the AWS CDK application have more permissions than necessary. The security team also discovered that the roles attached to the Lambda functions that the CDK application creates have more permissions than necessary. The developers must not have the ability to grant additional permissions.

Which solution will meet these requirements with the LEAST operational overhead?

**Options:**
- A. Create an SCP that denies the iam:CreateRole action and the iam:UpdateRole action for the developer role and the AWS CDK application deployment role. Centrally create new IAM roles to attach to the Lambda functions for the developers to use to provision Lambda functions.
- B. Create an IAM permission boundary policy. Define the maximum actions that the AWS CDK application requires in the policy. Update the account's AWS CDK bootstrapping to use the permission boundary. Update the configuration in the AWS CDK application for the default permissions boundary to use the policy.
- C. Create an IAM permission boundary policy. Define the maximum actions that the AWS CDK application requires in the policy. Instruct the developers to use the permission boundary policy name when they create a role in the AWS CDK application code.
- D. Create an SCP that denies the iam:CreateRole action and the iam:UpdateRole action for the developer role. Give the AWS CDK deployment role access to create roles associated with Lambda functions. Run AWS Identity and Access Management Access Analyzer to verify that the Lambda functions role does not have permissions.

> **Suggested Answer:** B
> **Community Vote:** B (67%), A (33%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**phu0298** (Wed 27 Nov 2024 04:05) - *Upvotes: 5*
In Option B, the use of a permission boundary directly in the CDK workflow ensures that the created roles are inherently compliant, removing the need for IAM Access Analyzer’s reactive validation step. This approach reduces operational complexity and aligns with best practices for proactive security.

---

**Srikantha** (Wed 02 Apr 2025 02:10) - *Upvotes: 1*
Option B → Use a permission boundary at the AWS CDK bootstrap level to restrict both developer permissions and Lambda function roles.

This solution enforces least privilege, requires no manual IAM role creation, and keeps the AWS CDK workflow seamless.

---

**teo2157** (Fri 17 Jan 2025 13:22) - *Upvotes: 3*
Going for A as B doesn´t restrict any permission to the deverlopers, just to the CDK role...

---


<br/>

## Question 312

*Date: Nov. 27, 2024, 4:10 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses Amazon Elastic Container Registry (Amazon ECR) private registries to store container images.

A DevOps team needs to ensure that the container images are regularly scanned for software package vulnerabilities.

Which solution will meet this requirement?

**Options:**
- A. Enable enhanced scanning for private registries in Amazon ECR.
- B. Enable basic continuous scanning for private registries in Amazon ECR.
- C. Create an AWS System Manager Automation document to scan images by using the AWS SDK. Configure the Automation document to run when a new image is pushed to an ECR registry.
- D. Create an AWS Lambda function that scans all images in Amazon ECR by using the AWS SDK. Create an Amazon EventBridge rule to invoke the Lambda function each day.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Wed 02 Apr 2025 02:11) - *Upvotes: 1*
Why is Option A the Best Choice?
Enhanced scanning provides the most comprehensive security coverage.
It uses AWS-provided and third-party scanning tools (e.g., Amazon Inspector).
It scans continuously and provides detailed vulnerability reports.
Supports automatic scanning on image push and periodic rescanning.
This ensures new images and existing images are continuously monitored.
No need for custom scripts or Lambda functions.
Fully managed by AWS → low operational overhead.

---

**phu0298** (Wed 27 Nov 2024 04:10) - *Upvotes: 4*
Amazon ECR offers two levels of image scanning:
1. Basic Scanning: Detects vulnerabilities when images are pushed to the repository.
2. Enhanced Scanning: Provides continuous scanning of images, including detailed reports and automatic rescans when vulnerabilities are published.

---


<br/>

## Question 313

*Date: Nov. 18, 2024, 10:05 a.m.
Disclaimers:
- ExamTopics website is not rel*

A security team sets up a workflow that invokes an AWS Step Functions workflow when Amazon EventBridge matches specific events. The events can be generated by several AWS services. AWS CloudTrail records user activities.

The security team notices that some important events do not invoke the workflow as expected. The CloudTrail logs do not indicate any direct errors related to the missing events.

Which combination of steps will identify the root cause of the missing event invocations? (Choose three.)

**Options:**
- A. Enable EventBridge schema discovery on the event bus to determine whether the event patterns match the expected schema.
- B. Configure Amazon CloudWatch to monitor EventBridge metrics and Step Functions metrics. Set up alerts for anomalies in event patterns and workflow invocations.
- C. Configure an AWS Lambda logging function to monitor and log events from EventBridge to provide more details about the processed events.
- D. Review the Step Functions execution history for patterns of failures or timeouts that could correlate to the missing event invocations.
- E. Review metrics for the EventBridge failed invocations to ensure that the IAM execution role that is attached to the rule has sufficient permissions.
- F. Verify that the Step Functions workflow has the correct permissions to be invoked by EventBridge.

> **Suggested Answer:** ABE
> **Community Vote:** ABE (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Wed 02 Apr 2025 02:20) - *Upvotes: 1*
✔ A. Enable EventBridge schema discovery to validate event structure.
✔ B. Monitor CloudWatch metrics for EventBridge and Step Functions.
✔ E. Check EventBridge failed invocations metrics for IAM permission issues.

This ensures you're covering schema validation, event monitoring, and IAM permission checks—all critical for debugging missing event invocations.

---

**luisfsm_111** (Tue 10 Dec 2024 17:53) - *Upvotes: 2*
Agree with the execution role approach

---

**Impromptu** (Fri 22 Nov 2024 15:59) - *Upvotes: 4*
I'd say E instead of F. The EventBridge rule contains the IAM execution role, that needs the permissions to invoke the step function. The permissions is not given at the step function side (no resource-based policy, see https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-iam.html)

---

**uncledana** (Mon 18 Nov 2024 10:05) - *Upvotes: 2*
A,B,F
The best steps to identify the root cause of the missing event invocations are enabling schema discovery to ensure the event structure is correct, monitoring EventBridge and Step Functions metrics to detect anomalies, and verifying that the Step Functions workflow has the correct permissions to be invoked by EventBridge. These actions will help to narrow down the issue effectively and efficiently.

---


<br/>

## Question 314

*Date: Nov. 22, 2024, 4:04 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company's DevOps engineer uses AWS Systems Manager to perform maintenance tasks. The company has a few Amazon EC2 instances that require a restart after notifications from AWS Health.

The DevOps engineer must implement an automated solution that uses Amazon EventBridge to remediate the notifications during the company's scheduled maintenance windows.

How should the DevOps engineer configure an EventBridge rule to meet these requirements?

**Options:**
- A. Configure an event source of AWS Health. Configure event types that indicate scheduled instance termination and retirement. Target the AWS-RestartEC2Instance Systems Manager Automation runbook to restart the EC2 instances.
- B. Configure an event source of Systems Manager. Configure an event type that indicates a maintenance window. Target the AWS-RestartEC2Instance Systems Manager Automation runbook to restart the EC2 instances.
- C. Configure an event source of AWS Health. Configure event types that indicate scheduled instance termination and retirement. Target a newly created AWS Lambda function that registers a Systems Manager maintenance window task to restart the EC2 instances.
- D. Configure an event source of EC2. Configure an event type that indicates instance state notification. Target a newly created AWS Lambda function that registers a Systems Manager maintenance window task to restart the EC2 instances.

> **Suggested Answer:** A
> **Community Vote:** A (70%), C (30%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**tinyshare** (Fri 06 Dec 2024 03:52) - *Upvotes: 7*
Same as the question 20, A is correct.

---

**Srikantha** (Wed 02 Apr 2025 02:28) - *Upvotes: 1*
Event Source of AWS Health:
The DevOps engineer wants to act on AWS Health notifications. AWS Health provides event types for scheduled instance terminations and instance retirements, which are common reasons for needing to restart EC2 instances during a maintenance window.
Event Types:
EventBridge can be configured to listen for event types like EC2 Instance Termination or EC2 Instance Retirement to automate instance restarts when these events occur.
Target Systems Manager Automation Runbook:
AWS Systems Manager Automation runbooks such as AWS-RestartEC2Instance are pre-defined automation workflows that can restart EC2 instances. When triggered by EventBridge, this runbook can automatically restart the EC2 instances that need remediation.

---

**devops_1** (Wed 19 Feb 2025 02:28) - *Upvotes: 2*
Option A does not satisfy the maintenance window requirement

---

**c87b433** (Thu 06 Feb 2025 17:33) - *Upvotes: 1*
C is the right answer. A will do action immediately but using lambda we can create a maintenance window according to organization downtime activity. A could have been the right answer, but it takes immediate action on an event that is not required. We have to do this in a particular time.

---

**gildzeee** (Mon 23 Dec 2024 21:50) - *Upvotes: 1*
question states during the maintenance window not immediately

---

**phu0298** (Wed 27 Nov 2024 04:36) - *Upvotes: 3*
C. Lambda Function to Register a Maintenance Window Task
• While this approach could work, it unnecessarily complicates the solution. AWS Systems Manager and EventBridge provide native integrations that eliminate the need for a custom Lambda function.
Therefore the answer should be A

---

**Changwha** (Sat 23 Nov 2024 11:57) - *Upvotes: 3*
AWS Health provides notifications for events such as scheduled instance termination and decommissioning.

---

**Impromptu** (Fri 22 Nov 2024 16:04) - *Upvotes: 2*
Although a bit more complex than A, I think C is best. Option A would just restart the instance when the event was received, and not withing the company's maintenance windows.

---


<br/>

## Question 315

*Date: t200*

A DevOps engineer manages an AWS CodePipeline pipeline that builds and deploys a web application on AWS. The pipeline has a source stage, a build stage, and a deploy stage. When deployed properly, the web application responds with a 200 OK HTTP response code when the URL of the home page is requested.

The home page recently returned a 503 HTTP response code after CodePipeline deployed the application. The DevOps engineer needs to add an automated test into the pipeline. The automated test must ensure that the application returns a 200 OK HTTP response code after the application is deployed. The pipeline must fail if the response code is not present during the test. The DevOps engineer has added a CheckURL stage after the deploy stage in the pipeline.

What should the DevOps engineer do next to implement the automated test?

**Options:**
- A. Configure the CheckURL stage to use an Amazon CloudWatch action. Configure the action to use a canary synthetic monitoring check on the application URL and to report a success or failure to CodePipeline.
- B. Create an AWS Lambda function to check the response code status of the URL and to report a success or failure to CodePipeline. Configure an action in the CheckURL stage to invoke the Lambda function.
- C. Configure the CheckURL stage to use an AWS CodeDeploy action. Configure the action with an input artifact that is the URL of the application and to report a success or failure to CodePipeline.
- D. Deploy an Amazon API Gateway HTTP API that checks the response code status of the URL and that reports success or failure to CodePipeline. Configure the CheckURL stage to use the AWS Device Farm test action and to provide the API Gateway HTTP API as an input artifact.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Wed 02 Apr 2025 02:37) - *Upvotes: 2*
This is the correct approach because:

AWS Lambda functions are ideal for running custom code that can check the HTTP response status of the application.
The Lambda function can make an HTTP request to the application’s home page URL, check the response code, and then return a success or failure depending on whether the code is 200 OK.
CodePipeline can then be configured to run this Lambda function as part of the CheckURL stage, and the pipeline can fail if the response code is not 200 OK.

---

**matt200** (Wed 25 Dec 2024 06:33) - *Upvotes: 3*
should be B

---


<br/>

## Question 316

*Date: t200*

A company has an application that uploads access logs to an Amazon CloudWatch Logs log group. The fields in the log lines include the response code and the application name.

The company wants to create a CloudWatch metric to track the number of requests by response code in a specific range and with a specific application name.

Which solution will meet these requirements?

**Options:**
- A. Create a CloudWatch Logs log event filter on the CloudWatch Logs log stream to match the response code range. Configure the log event filter to increment a metric. Set the response code and application name as dimensions.
- B. Create a CloudWatch Logs metric filter on the CloudWatch Logs log group to match the response code range. Configure the metric filter to increment a metric. Set the response code and application name as dimensions.
- C. Create a CloudWatch Contributor Insights rule on the CloudWatch Logs log stream with a filter to match the response code range. Configure the Contributor Insights rule to increment a CloudWatch metric with the response code and application name as dimensions.
- D. Create a CloudWatch Logs Insights query on the CloudWatch Logs log group to match the response code range. Configure the Logs Insights query to increment a CloudWatch metric with the response code and application name as dimensions.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Wed 02 Apr 2025 02:50) - *Upvotes: 3*
Why this is the best choice:

CloudWatch Logs Metric Filters allow you to extract specific patterns from log data, such as response codes and application names, and create metrics based on those patterns.
You can set conditions to match the desired range of response codes in the logs.
You can also specify dimensions (e.g., response code and application name) for the metric, which will allow you to track the counts of requests with specific response codes and application names.
Once created, this metric filter will automatically create CloudWatch metrics for the logs that match the criteria.

---

**matt200** (Wed 25 Dec 2024 06:34) - *Upvotes: 3*
should be B

---


<br/>

## Question 317

*Date: Nov. 18, 2024, 10:46 a.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer provisioned an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with managed node groups. The DevOps engineer associated an OpenID Connect (OIDC) issuer with the cluster.

The DevOps engineer is configuring Amazon Elastic Block Store (Amazon EBS) General Purpose SSD (gp3) volumes for the cluster. The DevOps engineer attempts to initiate a PersistentVolumeClaim (PVC) request but is unable to provision a volume. To troubleshoot the issue, the DevOps engineer runs the kubectl describe pyc command. The DevOps engineer receives a failed to provision volume with StorageClass error and a could not create volume in EC2:UnauthorizedOperation error.

Which solution will resolve these errors?

**Options:**
- A. Create a Kubernetes cluster role that allows the persistent volumes to perform get, list, watch, create, and delete operations. Configure the cluster role to allow get, list, and watch operations for storage in the cluster.
- B. Create an Amazon EBS Container Storage Interface (CSI) driver IAM role that has the required permissions and trust relationships. Attach the IAM role to the Amazon EBS CSI driver add-on in the cluster.
- C. Add the ebs.csi.aws.com/volumeType:gp3 annotation to the PersistentVolumeClaim object in the cluster.
- D. Create a Kubernetes storage class object. Set the provisioner value to ebs.csi.aws.com. Set the volumeBindingMode value to WaitForFirstConsumer in the luster.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Sat 05 Apr 2025 15:26) - *Upvotes: 3*
The key error is:

"could not create volume in EC2: UnauthorizedOperation"
This indicates that the EBS CSI driver does not have the required IAM permissions to provision EBS volumes via the EC2 API. Since the cluster is using IAM roles for service accounts (IRSA) with OIDC, the EBS CSI driver must assume an IAM role with the right permissions.

Here's what's likely missing:
The IAM role for the EBS CSI driver.
Proper trust relationship with the OIDC provider.
Necessary permissions like ec2:CreateVolume, ec2:AttachVolume, etc.
🛠 What to do:
Create an IAM policy with the required permissions.
Create an IAM role for the EBS CSI driver.
Update the trust relationship to allow assumption via the cluster’s OIDC provider.
Patch the EBS CSI driver deployment to use this IAM role (via Kubernetes service account).

---

**jojewi8143** (Sun 02 Feb 2025 17:17) - *Upvotes: 1*
B seems correct.

---

**teo2157** (Tue 17 Dec 2024 14:34) - *Upvotes: 3*
It's B based on this:
https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html

---

**uncledana** (Mon 18 Nov 2024 10:46) - *Upvotes: 3*
The root cause of the error is that the EBS CSI driver does not have the necessary IAM permissions to create EBS volumes in EC2. Solution B resolves the issue by creating an appropriate IAM role and attaching it to the EBS CSI driver, giving it the required permissions.

---


<br/>

## Question 318

*Date: Nov. 18, 2024, 10:48 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company runs a fleet of Amazon EC2 instances in a VPC. The company's employees remotely access the EC2 instances by using the Remote Desktop Protocol (RDP).

The company wants to collect metrics about how many RDP sessions the employees initiate every day.

Which combination of steps will meet this requirement? (Choose three.)

**Options:**
- A. Create an Amazon EventBridge rule that reacts to EC2 Instance State-change Notification events.
- B. Create an Amazon CloudWatch Logs log group. Specify the log group as a target for the EventBridge rule.
- C. Create a flow log in VPC Flow Logs.
- D. Create an Amazon CloudWatch Logs log group. Specify the log group as a destination for the flow log.
- E. Create a log group metric filter.
- F. Create a log group subscription filter. Use EventBridge as the destination.

> **Suggested Answer:** CDE
> **Community Vote:** CDE (90%), 10%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Sat 05 Apr 2025 15:30) - *Upvotes: 1*
C. VPC Flow Logs

Flow logs capture network traffic in your VPC, including RDP traffic (TCP port 3389).
This is the only way to detect RDP session attempts from a network perspective without needing to install agents on the instances.
D. CloudWatch Logs destination for flow logs

To analyze flow logs, you must send them somewhere — CloudWatch Logs is a common destination.
Once in CloudWatch Logs, you can search and filter for RDP traffic patterns.
E. Metric filter on the log group

You can create a CloudWatch metric filter to count log events that match RDP connections.
Filter pattern would look for destination port 3389 and action "ACCEPT".

---

**Ky_24** (Sun 15 Dec 2024 22:57) - *Upvotes: 3*
C. Create a flow log in VPC Flow Logs.
• Why? VPC Flow Logs capture information about the traffic to and from network interfaces in your VPC. This is crucial for identifying and analyzing RDP sessions, which use TCP port 3389 by default.

D. Create an Amazon CloudWatch Logs log group. Specify the log group as a destination for the flow log.
• Why? The captured VPC Flow Logs must be stored in a destination to enable analysis. Specifying a CloudWatch Logs log group allows for centralized storage and querying of logs.

E. Create a log group metric filter.
• Why? A metric filter enables you to extract specific metrics from the flow logs. You can filter for traffic using port 3389 (RDP) and create a metric to count the sessions.

---

**luisfsm_111** (Wed 11 Dec 2024 15:05) - *Upvotes: 3*
I see CDE, no need for EventBridge

---

**nqg54118** (Fri 29 Nov 2024 07:30) - *Upvotes: 2*
You can use a subscription filter with Amazon Kinesis Data Streams, AWS Lambda, or Amazon Data Firehos
https://docs.aws.amazon.com/ja_jp/AmazonCloudWatch/latest/logs/SubscriptionFilters.html

---

**f4b18ba** (Fri 22 Nov 2024 20:58) - *Upvotes: 1*
By using an Amazon ECR pull through cache rule (Option C) and setting up the necessary VPC endpoints for private ECR (Option E) and S3 (Option F), the company can:

Eliminate Internet Access:
Remove NAT gateways and internet gateways from the VPC.
Maintain Image Access:
Allow ECS tasks to pull images from both private and public ECR repositories without internet access.
Ensure Image Updates:
Automatically receive updates to public images within 24 hours via the pull through cache.
Minimize Operational Overhead:
Avoid complex setups with additional services like CodeBuild, Lambda, or custom scripts.

---

**uncledana** (Mon 18 Nov 2024 10:48) - *Upvotes: 1*
The best approach for collecting metrics about RDP sessions is to use VPC Flow Logs, send them to CloudWatch Logs, and then create a metric filter to extract the relevant information (such as RDP traffic on port 3389). Option B, D, and E cover the necessary steps for implementing this solution.

---


<br/>

## Question 319

*Date: Nov. 18, 2024, 10:51 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company is using Amazon Elastic Kubernetes Service (Amazon EKS) to run its applications. The EKS cluster is successfully running multiple pods. The company stores the pod images in Amazon Elastic Container Registry (Amazon ECR).

The company needs to configure Pod Identity access for the EKS cluster. The company has already updated the node IAM role by using the permissions for Pod Identity access.

Which solution will meet these requirements?

**Options:**
- A. Create an IAM OpenID Connect (OIDC) provider for the EKS cluster.
- B. Ensure that the nodes can reach the EKS Auth API. Add and configure the EKS Pod Identity Agent add-on for the EKS cluster.
- C. Create an EKS access entry that uses the API_AND-CONFIG_MAP cluster authentication mode.
- D. Configure the AWS Security Token Service (AWS STS) endpoint for the Kubernetes service account that the pods in the EKS cluster use.

> **Suggested Answer:** B
> **Community Vote:** B (85%), A (15%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Sat 05 Apr 2025 15:36) - *Upvotes: 1*
The company wants to configure EKS Pod Identity (a newer and simpler alternative to IAM Roles for Service Accounts / IRSA).

Since they already updated the node IAM role to allow Pod Identity access, the next essential step is to:

Install the EKS Pod Identity Agent add-on to the cluster.
Ensure that the nodes can reach the EKS Auth API, which the agent uses to request temporary credentials.
This is exactly what Option B outlines.

---

**CHRIS12722222** (Sat 28 Dec 2024 15:01) - *Upvotes: 3*
Question is not talking about IRSA
Pod identities do not need OIDC

---

**tubtab** (Wed 25 Dec 2024 15:52) - *Upvotes: 2*
IT BBBB

---

**gildzeee** (Mon 23 Dec 2024 21:58) - *Upvotes: 2*
question doesnt state the pods are using irsa so the eks addon should work just fine with pod identity

---

**teo2157** (Tue 17 Dec 2024 15:36) - *Upvotes: 1*
It's A based on this:
https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html

---

**pma17** (Wed 04 Dec 2024 11:25) - *Upvotes: 3*
Pod Identity is a "new" way to provide Pod access to AWS services and does not rely on OIDC. Instead you have to setup the EKS Pod Identity Agent and must ensure kubernetes nodes can reach the EKS Auth API endpoint.
https://docs.aws.amazon.com/eks/latest/userguide/pod-identities.html

---

**f4b18ba** (Fri 22 Nov 2024 20:13) - *Upvotes: 1*
This is the necessary first step to set up IRSA. Without the IAM OIDC provider, IAM cannot trust tokens from the EKS cluster, and service accounts cannot assume IAM roles.
Enables the establishment of trust between Kubernetes service accounts and IAM roles, allowing pods to securely access AWS resources.

---

**CHRIS12722222** (Sat 28 Dec 2024 14:57) - *Upvotes: 1*
Question is not talking about IRSA
Pod identities do not need OIDC

---

**uncledana** (Mon 18 Nov 2024 10:51) - *Upvotes: 1*
The best and most accurate solution is A. Create an IAM OpenID Connect (OIDC) provider for the EKS cluster.

---


<br/>

## Question 320

*Date: Nov. 21, 2024, 8:36 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has multiple AWS accounts in an organization in AWS Organizations that has all features enabled. The company’s DevOps administrator needs to improve security across all the company's AWS accounts. The administrator needs to identify the top users and roles in use across all accounts.

Which solution will meet these requirements with the MOST operational efficiency?

**Options:**
- A. Create a new organization trail in AWS CloudTrail. Configure the trail to send log events to Amazon CloudWatch Logs. Create a CloudWatch Contributor Insights rule for the userIdentity.arn log field. View the results in CloudWatch Contributor Insights.
- B. Create an unused access analysis for the organization by using AWS Identity and Access Management Access Analyzer. Review the analyzer results and determine if each finding has the intended level of permissions required for the workload.
- C. Create a new organization trail in AWS CloudTrail. Create a table in Amazon Athena that uses partition projection. Load the Athena table with CloudTrail data. Query the Athena table to find the top users and roles.
- D. Generate a Service access report for each account by using Organizations. From the results, pull the last accessed date and last accessed by account fields to find the top users and roles.

> **Suggested Answer:** C
> **Community Vote:** C (58%), A (42%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Ky_24** (Sun 15 Dec 2024 23:04) - *Upvotes: 5*
You can use partition projection in Athena to optimize your queries by specifying how the logs are structured in S3. This makes the process of querying CloudTrail logs across multiple AWS accounts much more efficient.

---

**0ac7838** (Tue 25 Nov 2025 09:51) - *Upvotes: 1*
C (Athena) = technically works but heavy operational burden

A (Contributor Insights) = real-time, zero-query, zero-maintenance dashboards

Therefore A is the MOST operationally efficient solution.

---

**AWSLoverLoverLoverLoverLover** (Thu 22 May 2025 19:21) - *Upvotes: 3*
Which solution will meet these requirements with the MOST operational efficiency?

Answer is A

---

**robotgeek** (Fri 16 May 2025 11:51) - *Upvotes: 1*
What do you think "partition projection" has to do with the required scenario guys?

---

**Srikantha** (Sat 05 Apr 2025 15:46) - *Upvotes: 1*
This option provides the MOST operational efficiency because it:

Aggregates CloudTrail logs from all AWS accounts using a single organization trail.
Leverages Amazon Athena to analyze logs at scale with SQL-like queries.
Allows for automated and repeatable querying to identify top users and roles across the entire organization.
Partition projection reduces the need for manual partition management, improving performance and automation.

---

**teo2157** (Fri 31 Jan 2025 10:00) - *Upvotes: 4*
Voting for C, agree with Ky_24

---

**Erso** (Tue 21 Jan 2025 10:23) - *Upvotes: 1*
MOST operation efficency is the key point here

---

**teo2157** (Tue 17 Dec 2024 15:42) - *Upvotes: 4*
Athena is much more efficient that CloudWatch Contributor Insights in this case

---

**f4b18ba** (Fri 22 Nov 2024 20:17) - *Upvotes: 4*
Option A provides a solution that is operationally efficient, scalable, and directly addresses the requirement to identify the top users and roles in use across all AWS accounts. By leveraging AWS services like CloudTrail and CloudWatch Contributor Insights, the DevOps administrator can gain real-time insights with minimal setup and maintenance effort.

---

**phu0298** (Thu 21 Nov 2024 08:36) - *Upvotes: 4*
C
A: While Contributor Insights can identify the top contributors (e.g., users and roles), it is limited to specific log patterns and is more suited for real-time analysis.
This option is not as operationally efficient for long-term, detailed analysis across all accounts.

---


<br/>

## Question 321

*Date: Nov. 22, 2024, 8:48 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an organization in AWS Organizations with many Oils that contain many AWS accounts. The organization has a dedicated delegated administrator AWS account.

The company needs the accounts in one OU to have server-side encryption enforced for all Amazon Elastic Block Store (Amazon EBS) volumes and Amazon Simple Queue Service (Amazon SQS) queues that are created or updated on an AWS CloudFormation stack.

Which solution will enforce this policy before a CloudFormation stack operation in the accounts of this OU?

**Options:**
- A. Activate trusted access to CloudFormation StackSets. Create a CloudFormation Hook that enforces server-side encryption on EBS volumes and SQS queues. Deploy the Hook across the accounts in the OU by using StackSets.
- B. Set up AWS Config in all the accounts in the OU. Use AWS Systems Manager to deploy AWS Config rules that enforce server-side encryption for EBS volumes and SQS queues across the accounts in the OU.
- C. Write an SCP to deny the creation of EBS volumes and SQS queues unless the EBS volumes and SQS queues have server-side encryption. Attach the SCP to the OU.
- D. Create an AWS Lambda function in the delegated administrator account that checks whether server-side encryption is enforced for EBS volumes and SQS queues. Create an IAM role to provide the Lambda function access to the accounts in the OU.

> **Suggested Answer:** A
> **Community Vote:** A (90%), 10%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**tubtab** (Wed 25 Dec 2024 15:59) - *Upvotes: 5*
KEYWORD enforce this policy before a CloudFormation stack operation

---

**rrodriguezmarulanda** (Fri 08 Aug 2025 10:12) - *Upvotes: 2*
SCP will deny the creation of those resources if are not encrypted

---

**Srikantha** (Sat 05 Apr 2025 15:53) - *Upvotes: 1*
The company wants to enforce encryption at resource creation time before a CloudFormation stack operation is allowed. The best way to do that is by using CloudFormation Hooks, which can validate or block stack operations pre-deployment based on custom logic.

CloudFormation Hooks allow you to enforce pre-provisioning checks.
Hooks can block stack creations or updates that don’t meet compliance (e.g., unencrypted EBS volumes or SQS queues).
You can deploy the Hook organization-wide using CloudFormation StackSets with trusted access enabled.
This provides automated, consistent enforcement across accounts in an OU.

---

**Ky_24** (Sun 15 Dec 2024 23:07) - *Upvotes: 4*
• CloudFormation StackSets allows you to deploy a CloudFormation template across multiple AWS accounts and regions in your organization. By enabling trusted access to CloudFormation StackSets, you can manage resources and apply policies uniformly across multiple accounts within the OU.
• A CloudFormation Hook is a way to enforce specific policies or checks during stack operations. In this case, you can create a Hook to ensure that all EBS volumes and SQS queues created or updated in the CloudFormation stack have server-side encryption enabled.
• The StackSet and Hook can be deployed across all accounts in the specified OU, ensuring that server-side encryption is automatically enforced before any stack operation proceeds, thus satisfying the company’s policy.

---

**Changwha** (Sat 23 Nov 2024 11:42) - *Upvotes: 4*
The answer is A

---

**f4b18ba** (Fri 22 Nov 2024 20:48) - *Upvotes: 4*
CloudFormation Hooks allow you to intercept stack operations and perform validations or enforce policies before resources are created or updated.
Develop a CloudFormation Hook that checks whether EBS volumes and SQS queues in the CloudFormation templates have SSE enabled.
Use CloudFormation StackSets with trusted access to deploy the Hook across all accounts in the OU.
The Hook will validate templates and prevent non-compliant resources from being created or updated during stack operations.
Applies only to resources managed via CloudFormation, aligning with the company's requirement.
Centralized Deployment: StackSets allow you to deploy the Hook across multiple accounts and regions efficiently.
Hooks do not interfere with non-CloudFormation operations, limiting the scope to what's required.

---


<br/>

## Question 322

*Date: Nov. 18, 2024, 11:40 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is running an internal application in an Amazon Elastic Container Service (Amazon ECS) cluster on Amazon EC2. The ECS cluster instances can connect to the public internet. The ECS tasks that run on the cluster instances are configured to use images from both private Amazon Elastic Container Registry (Amazon ECR) repositories and a public ECR registry repository.

A new security policy requires the company to remove the ECS cluster's direct access to the internet. The company must remove any NAT gateways and internet gateways from the VPC that hosts the cluster. A DevOps engineer needs to ensure the ECS cluster can still download images from both the public ECR registry and the private ECR repositories. Images from the public ECR registry must remain up-to-date. New versions of the images must be available to the ECS cluster within 24 hours of publication.

Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)

**Options:**
- A. Create an AWS CodeBuild project and a new private ECR repository for each image that is downloaded from the public ECR registry. Configure each project to pull the image from the public ECR repository and push the image to the new private ECR repository. Create an Amazon EventBridge rule that invokes the CodeBuild project once every 24 hours. Update each task definition in the ECS cluster to refer to the new private ECR repository.
- B. Create a new Amazon ECR pull through cache rule for each image that is downloaded from the public ECR registry. Create an AWS Lambda function that invokes each pull through cache rule. Create an Amazon EventBridge rule that invokes the Lambda function once every 24 hours. Update each task definition in the ECS cluster to refer to the image from the pull through cache.
- C. Create a new Amazon ECR pull through cache rule for the public ECR registry. Update each task definition in the ECS cluster to refer to the image from the pull through cache. Ensure each public image has been downloaded through the pull through cache at least once before removing internet access from the VPC.
- D. Create an Amazon ECR interface VPC endpoint for the public ECR repositories that are in the VPC.
- E. Create an Amazon ECR interface VPC endpoint for the private ECR repositories that are in the VPC.
- F. Create an Amazon S3 gateway endpoint in the VPC.

> **Suggested Answer:** CEF
> **Community Vote:** CEF (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Sat 05 Apr 2025 15:55) - *Upvotes: 2*
C. ECR Pull Through Cache for Public Images

This allows public ECR images to be cached in your private ECR.
You avoid internet access by referencing the cached copy.
Once an image is pulled through the cache, it's stored in private ECR and available for future use even without internet access.
Ensures that new versions can still be fetched (within 24h of publication), assuming it's referenced and updated in the cache.
Least operational overhead: No need for custom pipelines or daily sync jobs.
E. Interface VPC Endpoint for Private ECR

Needed to let ECS pull private images without internet or NAT gateway.
Interface VPC endpoints connect your VPC to ECR APIs via AWS PrivateLink.
F. S3 Gateway Endpoint

ECR stores image layers in Amazon S3.
When ECS pulls an image, it downloads layers from S3 — so the cluster needs S3 access even if ECR is private.
A gateway endpoint provides private S3 access from the VPC.

---

**teo2157** (Tue 17 Dec 2024 16:27) - *Upvotes: 3*
Going with CEF as well

---

**f4b18ba** (Fri 22 Nov 2024 20:59) - *Upvotes: 4*
By using an Amazon ECR pull through cache rule (Option C) and setting up the necessary VPC endpoints for private ECR (Option E) and S3 (Option F), the company can:

Eliminate Internet Access:
Remove NAT gateways and internet gateways from the VPC.
Maintain Image Access:
Allow ECS tasks to pull images from both private and public ECR repositories without internet access.
Ensure Image Updates:
Automatically receive updates to public images within 24 hours via the pull through cache.
Minimize Operational Overhead:
Avoid complex setups with additional services like CodeBuild, Lambda, or custom scripts.

---

**rainwalker** (Wed 20 Nov 2024 08:28) - *Upvotes: 3*
C, E, F
https://docs.aws.amazon.com/AmazonECR/latest/userguide/vpc-endpoints.html

VPC endpoints currently don't support Amazon ECR Public repositories. Consider using a pull through cache rule to host the public image in a private repository in the same Region as the VPC endpoint. For more information

The image metadata and layers in the ECR are stored in Amazon S3.
Creating an S3 Gateway endpoint enables the ECS cluster to exchange data between ECR and S3 without the internet.

---

**uncledana** (Mon 18 Nov 2024 23:40) - *Upvotes: 2*
By implementing the pull through cache rule and setting up VPC endpoints for both public and private ECR repositories, the ECS cluster can securely access required container images without direct internet access. This approach ensures compliance with the security policy while maintaining operational efficiency and timely updates to images.

---


<br/>

## Question 323

*Date: Dec. 17, 2024, 4:32 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has a continuous integration pipeline where the company creates container images by using AWS CodeBuild. The created images are stored in Amazon Elastic Container Registry (Amazon ECR).

Checking for and fixing the vulnerabilities in the images takes the company too much time. The company wants to identify the image vulnerabilities quickly and notify the security team of the vulnerabilities.

Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)

**Options:**
- A. Activate Amazon Inspector enhanced scanning for Amazon ECR. Configure the enhanced scanning to use continuous scanning. Set up a topic in Amazon Simple Notification Service (Amazon SNS).
- B. Create an Amazon EventBridge rule for Amazon Inspector findings. Set an Amazon Simple Notification Service (Amazon SNS) topic as the rule target.
- C. Activate AWS Lambda enhanced scanning for Amazon ECR. Configure the enhanced scanning to use continuous scanning. Set up a topic in Amazon Simple Email Service (Amazon SES).
- D. Create a new AWS Lambda function. Invoke the new Lambda function when scan findings are detected.
- E. Activate default basic scanning for Amazon ECR for all container images. Configure the default basic scanning to use continuous scanning. Set up a topic in Amazon Simple Notification Service (Amazon SNS).

> **Suggested Answer:** AB
> **Community Vote:** AB (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Sat 05 Apr 2025 15:58) - *Upvotes: 1*
✅ A. Amazon Inspector Enhanced Scanning + SNS
Amazon Inspector now supports enhanced container image scanning.
It can be configured to use continuous scanning, which means images are scanned as soon as they are pushed or updated in ECR.
SNS is used to send notifications (email, SMS, or to other systems).
This setup requires minimal configuration and provides automated security insights.
✅ B. EventBridge rule for Inspector findings
Inspector findings are emitted as EventBridge events.
Creating an EventBridge rule to trigger SNS when findings occur allows you to immediately alert the security team.
No need for custom logic — EventBridge to SNS is low-maintenance and scalable.

---

**teo2157** (Tue 17 Dec 2024 16:32) - *Upvotes: 4*
It's AB based on this https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning-enhanced.html

---


<br/>

## Question 324

*Date: Nov. 18, 2024, 11:51 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps administrator is configuring a repository to store a company's container images. The administrator needs to configure a lifecycle rule that automatically deletes container images that have a specific tag and that are older than 15 days.

Which solution will meet these requirements with the MOST operational efficiency?

**Options:**
- A. Create a repository in Amazon Elastic Container Registry (Amazon ECR). Add a lifecycle policy to the repository to expire images that have the matching tag after 15 days.
- B. Create a repository in AWS CodeArtifact. Add a repository policy to the CodeArtifact repository to expire old assets that have the matching tag after 15 days.
- C. Create a bucket in Amazon S3. Add a bucket lifecycle policy to expire old objects that have the matching tag after 15 days
- D. Create an EC2 Image Builder container recipe. Add a build component to expire the container that has the matching tag after 15 days.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Sat 05 Apr 2025 15:59) - *Upvotes: 2*
Amazon ECR supports lifecycle policies that automatically manage the retention of container images. You can:

Match images by tags, including specific values or untagged.
Specify age conditions like deleting images older than 15 days.
Set up the policy once for automatic cleanup, requiring minimal ongoing maintenance.
This is the most operationally efficient solution, purpose-built for container image lifecycle management.

---

**matt200** (Wed 25 Dec 2024 06:36) - *Upvotes: 4*
A. Create a repository in Amazon Elastic Container Registry (Amazon ECR).

---

**uncledana** (Mon 18 Nov 2024 23:51) - *Upvotes: 3*
The requirement is to automatically manage container images based on a specific tag and age. Amazon Elastic Container Registry (Amazon ECR) supports lifecycle policies, which are specifically designed to automate the management of image lifecycle events like expiration. Lifecycle policies in ECR allow you to define rules for expiring or retaining images based on criteria like image tags and age, providing an efficient solution with minimal operational overhead.

---


<br/>

## Question 325

*Date: Nov. 6, 2024, 6:10 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses Amazon Redshift as its data warehouse solution. The company wants to create a dashboard to view changes to the Redshift users and the queries the users perform.

Which combination of steps will meet this requirement? (Choose two.)

**Options:**
- A. Create an Amazon CloudWatch log group. Create an AWS CloudTrail trail that writes to the CloudWatch log group.
- B. Create a new Amazon S3 bucket. Configure default audit logging on the Redshift cluster. Configure the S3 bucket as the target.
- C. Configure the Redshift cluster database audit logging to include user activity logs. Configure Amazon CloudWatch as the target.
- D. Create an Amazon CloudWatch dashboard that has a log widget. Configure the widget to display user details from the Redshift logs.
- E. Create an AWS Lambda function that uses Amazon Athena to query the Redshift logs. Create an Amazon CloudWatch dashboard that has a custom widget type that uses the Lambda function.

> **Suggested Answer:** CD
> **Community Vote:** CD (76%), BE (18%), 6%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**pma17** (Wed 04 Dec 2024 11:41) - *Upvotes: 6*
To log users queries you need to include user activity logs which require additional setup on the Redshift cluster. So C, because it is not mentioned on B.
Then to display this logs the only option is D, because E requires the logs to be on S3.

---

**Srikantha** (Sat 05 Apr 2025 16:04) - *Upvotes: 2*
To track Redshift user activity and query execution, you need to enable Redshift audit logging. This allows you to analyze:

User connections and disconnections
SQL queries run
Changes to database users and privileges
B. Enable Redshift Audit Logging to S3

Redshift supports audit logging which includes user activity, connection logs, and user changes.
Logs are delivered to an Amazon S3 bucket, which you can then query using tools like Athena.
This is the most efficient and supported way to persist and analyze historical Redshift activity.
E. Query logs via Athena + Display on CloudWatch Dashboard

Once Redshift logs are in S3, you can set up an Athena table to query them.
You can use an AWS Lambda function to run these queries.
A custom widget in an Amazon CloudWatch Dashboard can call the Lambda and display the results, such as recent user actions or frequent queries.
This enables a near-real-time, visual dashboard with low operational overhead.

---

**tdlAws** (Wed 22 Jan 2025 17:11) - *Upvotes: 1*
Setting up the default audit log on your Redshift cluster will allow you to record database activity, including user changes and queries performed.
Specifying an Amazon S3 bucket as the destination will store the logs for later analysis.
Using Amazon Athena to query the logs stored in S3 allows you to create custom queries and extract the relevant data.
The Amazon CloudWatch dashboard with a custom AWS Lambda-based widget can display the information directly on the dashboard.

---

**Impromptu** (Sun 24 Nov 2024 12:38) - *Upvotes: 4*
audit logging can be sent to cloudwatch logs. So easier to just have them in cloudwatch and make a dashboard

---

**siheom** (Wed 20 Nov 2024 06:21) - *Upvotes: 1*
vote BC

---

**uncledana** (Mon 18 Nov 2024 23:55) - *Upvotes: 1*
B. Create a new Amazon S3 bucket. Configure default audit logging on the Redshift cluster. Configure the S3 bucket as the target.
E. Create an AWS Lambda function that uses Amazon Athena to query the Redshift logs. Create an Amazon CloudWatch dashboard that has a custom widget type that uses the Lambda function.

Explanation:

To create a dashboard for viewing changes to Amazon Redshift users and the queries they perform, you need to capture the necessary audit logs and process them into a dashboard-friendly format.

---

**tinyshare** (Fri 15 Nov 2024 08:02) - *Upvotes: 3*
You need to use CloudWatch for dashboard, so the target must be CloudWatch. B is wrong.
CloudTrail does not record user queries. A is wrong.
Use Lambda to create your own solution is not recommended, E is wrong.

---


<br/>

## Question 326

*Date: Nov. 22, 2024, 9:18 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses an organization in AWS Organizations to manage its 500 AWS accounts. The organization has all features enabled. The AWS accounts are in a single OU. The developers need to use the CostCenter tag key for all resources in the organization's member accounts. Some teams do not use the CostCenter tag key to tag their Amazon EC2 instances.

The cloud team wrote a script that scans all EC2 instances in the organization's member accounts. If the EC2 instances do not have a CostCenter tag key, the script will notify AWS account administrators. To avoid this notification, some developers use the CostCenter tag key with an arbitrary string in the tag value.

The cloud team needs to ensure that all EC2 instances in the organization use a CostCenter tag key with the appropriate cost center value.

Which solution will meet these requirements?

**Options:**
- A. Create an SCP that prevents the creation of EC2 instances without the CostCenter tag key. Create a tag policy that requires the CostCenter tag to be values from a known list of cost centers for all EC2 instances. Attach the policy to the OU. Update the script to scan the tag keys and tag values. Modify the script to update noncompliant resources with a default approved tag value for the CostCenter tag key.
- B. Create an SCP that prevents the creation of EC2 instances without the CostCenter tag key. Attach the policy to the OU. Update the script to scan the tag keys and tag values and notify the administrators when the tag values are not valid.
- C. Create an SCP that prevents the creation of EC2 instances without the CostCenter tag key. Attach the policy to the OU. Create an IAM permission boundary in the organization's member accounts that restricts the CostCenter tag values to a list of valid cost centers.
- D. Create a tag policy that requires the CostCenter tag to be values from a known list of cost centers for all EC2 instances. Attach the policy to the OU. Configure an AWS Lambda function that adds an empty CostCenter tag key to an EC2 instance. Create an Amazon EventBridge rule that matches events to the RunInstances API action with the Lambda function as the target.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**f4b18ba** (Fri 22 Nov 2024 21:18) - *Upvotes: 6*
Service Control Policy (SCP): Creating an SCP ensures that any attempt to create EC2 instances without the CostCenter tag key is denied right from the start. This enforces the requirement at the organizational level.

Tag Policy: By creating a tag policy that enforces the CostCenter tag values to be from a known list, you can ensure that only valid cost center values are used across all EC2 instances.

Script Update: Updating the script to not only scan for tag keys and values but also to update noncompliant resources with a default approved tag value ensures compliance and mitigates the issue of arbitrary string values.

Comprehensive Solution: This approach addresses both the presence of the CostCenter tag and the correctness of its value, providing a comprehensive solution to the problem.

---

**Srikantha** (Sat 05 Apr 2025 16:10) - *Upvotes: 2*
SCP to require the CostCenter tag key:
Prevents users from launching EC2 instances without the required tag.
Tag policy to validate values:
AWS tag policies can enforce allowed values for tag keys like CostCenter, across all accounts in an OU.
Script enhancement for compliance:
Script detects noncompliant resources and applies a default value, maintaining tag integrity and reducing manual intervention.

---

**Changwha** (Sat 23 Nov 2024 11:48) - *Upvotes: 4*
The answer is A

---


<br/>

## Question 327

*Date: Nov. 19, 2024, 12:15 a.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer uses a pipeline in AWS CodePipeline. The pipeline has a build action and a deploy action for a single-page web application that is delivered to an Amazon S3 bucket. Amazon CloudFront serves the web application. The build action creates an artifact for the web application.

The DevOps engineer has created an AWS CloudFormation template that defines the S3 bucket and configures the S3 bucket to host the application. The DevOps engineer has configured a CloudFormation deploy action before the S3 action. The CloudFormation deploy action creates the S3 bucket. The DevOps engineer needs to configure the S3 deploy action to use the S3 bucket from the CloudFormation template.

Which combination of steps will meet these requirements? (Choose two.)

**Options:**
- A. Add an output named BucketName to the CloudFormation template. Set the output's value to refer to the S3 bucket from the CloudFormation template. Configure the output value to export to an AWS::SSM::Parameter resource named Stackvariables.
- B. Add an output named BucketName to the CloudFormation template. Set the output's value to refer to the S3 bucket from the CloudFormation template. Set the CloudFormation action's namespace to StackVariables in the pipeline.
- C. Configure the output artifacts of the CloudFormation action in the pipeline to be an AWS Systems Manager Parameter Store parameter named StackVariables. Name the artifact BucketName.
- D. Configure the build artifact from the build action as the input to the CodePipeline S3 deploy action. Configure the deploy action to deploy to the S3 bucket by using the StackVariables.BucketName variable.
- E. Configure the build artifact from the build action and the AWS Systems Manager parameter as the inputs to the deploy action. Configure the deploy action to deploy to the S3 bucket by using the StackVariables.BucketName variable.

> **Suggested Answer:** BD
> **Community Vote:** BD (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Changwha** (Sat 23 Nov 2024 11:54) - *Upvotes: 3*
1. Define a BucketName output in the CloudFormation template and set the CloudFormation action’s namespace in the pipeline to StackVariables.
2. Use the StackVariables.BucketName variable in the S3 deploy action to specify the S3 bucket dynamically, while using the build artifact as input.

---

**f4b18ba** (Fri 22 Nov 2024 21:32) - *Upvotes: 4*
Option B:
Adding an output to the CloudFormation template allows the pipeline to extract the S3 bucket name created during the stack deployment.
Setting the namespace (StackVariables) in the CloudFormation action enables other actions in the pipeline to reference the output values using that namespace.
Option D:

The build artifact needs to be input to the S3 deploy action to provide the files for deployment.
By using StackVariables.BucketName, the deploy action dynamically references the bucket created by the CloudFormation stack, ensuring that the files are deployed to the correct location.

---

**uncledana** (Tue 19 Nov 2024 00:15) - *Upvotes: 3*
To dynamically deploy a single-page web application to an S3 bucket created by a CloudFormation deploy action in AWS CodePipeline, you should:

1. Define a BucketName output in the CloudFormation template and set the CloudFormation action’s namespace in the pipeline to StackVariables.
2. Use the StackVariables.BucketName variable in the S3 deploy action to specify the S3 bucket dynamically, while using the build artifact as input.

This approach ensures the pipeline is correctly configured to handle dynamic resource creation with minimal complexity.

---


<br/>

## Question 328

*Date: t200*

A company used a lift and shift strategy to migrate a workload to AWS. The company has an Auto Scaling group of Amazon EC2 instances. Each EC2 instance runs a web application, a database, and a Redis cache.

Users are experiencing large variations in the web application's response times. Requests to the web application go to a single EC2 instance that is under significant load. The company wants to separate the application components to improve availability and performance.

Which solution will meet these requirements?

**Options:**
- A. Create a Network Load Balancer and an Auto Scaling group for the web application. Migrate the database to an Amazon Aurora Serverless database. Create an Application Load Balancer and an Auto Scaling group for the Redis cache.
- B. Create an Application Load Balancer and an Auto Scaling group for the web application. Migrate the database to an Amazon Aurora database that has a Multi-AZ deployment. Create a Network Load Balancer and an Auto Scaling group in a single Availability Zone for the Redis cache.
- C. Create a Network Load Balancer and an Auto Scaling group for the web application. Migrate the database to an Amazon Aurora Serverless database. Create an Amazon ElastiCache (Redis OSS) cluster for the cache. Create a target group that has a DNS target type that contains the ElastiCache (Redis OSS) cluster hostname.
- D. Create an Application Load Balancer and an Auto Scaling group for the web application. Migrate the database to an Amazon Aurora database that has a Multi-AZ deployment. Create an Amazon ElastiCache (Redis OSS) cluster for the cache.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**matt200** (Wed 25 Dec 2024 06:37) - *Upvotes: 5*
should be D

---

**Srikantha** (Sat 05 Apr 2025 16:45) - *Upvotes: 2*
This solution separates the components of the application to improve availability and performance while addressing the specific issues the company is facing, such as variations in response times and heavy load on a single EC2 instance.

Key steps in this solution:

Application Load Balancer (ALB):
Distributes incoming web traffic across multiple EC2 instances in an Auto Scaling group.
Ensures high availability and horizontal scaling of the web application.
Amazon Aurora (Multi-AZ):
Aurora is a fully managed relational database service, and using Multi-AZ deployment ensures high availability and automatic failover.
Helps offload the database work from the EC2 instances.
Amazon ElastiCache (Redis):
Offloads caching from the EC2 instances to ElastiCache (Redis OSS), which is purpose-built for high-performance caching and can handle load more effectively than having Redis run on each EC2 instance.
Improves response times by reducing the database load.

---


<br/>

## Question 329

*Date: t200*

A company is using AWS Organizations and wants to implement a governance strategy with the following requirements:

• AWS resource access is restricted to the same two Regions for all accounts.
• AWS services are limited to a specific group of authorized services for all accounts.
• Authentication is provided by Active Directory.
• Access permissions are organized by job function and are identical in each account.

Which solution will meet these requirements?

**Options:**
- A. Establish an organizational unit (OU) with group policies in the management account to restrict Regions and authorized services. Use AWS CloudFormation StackSets to provision roles with permissions for each job function, including an IAM trust policy for IAM identity provider authentication in each account.
- B. Establish a permission boundary in the management account to restrict Regions and authorized services. Use AWS CloudFormation StackSets to provision roles with permissions for each job function, including an IAM trust policy for IAM identity provider authentication in each account.
- C. Establish a service control policy in the management account to restrict Regions and authorized services. Use AWS Resource Access Manager (AWS RAM) to share management account roles with permissions for each job function, including AWS IAM Identity Center for authentication in each account.
- D. Establish a service control policy in the management account to restrict Regions and authorized services. Use AWS CloudFormation StackSets to provision roles with permissions for each job function, including an IAM trust policy for IAM identity provider authentication in each account.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Sat 05 Apr 2025 16:49) - *Upvotes: 2*
Service Control Policies (SCPs):
SCPs are used in AWS Organizations to apply restrictions at the organization level to control which AWS Regions and services can be used.
SCPs will enforce the policy across all accounts in the organization, ensuring that resource access is restricted to only the allowed Regions and services.
CloudFormation StackSets:
AWS CloudFormation StackSets are used to automatically create and maintain roles and permissions across all accounts.
This allows for standardized job-function-based roles to be created consistently in each account, with the exact same structure, regardless of account.
IAM Trust Policy with Active Directory:
The solution includes an IAM trust policy that allows Active Directory to authenticate users. This ensures that access is controlled by user identity and role, according to the organizational job functions.

---

**matt200** (Wed 25 Dec 2024 06:37) - *Upvotes: 4*
should be D

---


<br/>

## Question 330

*Date: Dec. 3, 2024, 8:52 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company detects unusual login attempts in many of its AWS accounts. A DevOps engineer must implement a solution that sends a notification to the company's security team when multiple failed login attempts occur. The DevOps engineer has already created an Amazon Simple Notification Service (Amazon SNS) topic and has subscribed the security team to the SNS topic.

Which solution will provide the notification with the LEAST operational effort?

**Options:**
- A. Configure AWS CloudTrail to send management events to an Amazon CloudWatch Logs log group. Create a CloudWatch Logs metric filter to match failed ConsoleLogin events. Create a CloudWatch alarm that is based on the metric filter. Configure an alarm action to send messages to the SNS topic.
- B. Configure AWS CloudTrail to send management events to an Amazon S3 bucket. Create an Amazon Athena query that returns a failure if the query finds failed logins in the logs in the S3 bucket. Create an Amazon EventBridge rule to periodically run the query. Create a second EventBridge rule to detect when the query fails and to send a message to the SNS topic.
- C. Configure AWS CloudTrail to send data events to an Amazon CloudWatch Logs log group. Create a CloudWatch logs metric filter to match failed ConsoleLogin events. Create a CloudWatch alarm that is based on the metric filter. Configure an alarm action to send messages to the SNS topic.
- D. Configure AWS CloudTrail to send data events to an Amazon S3 bucket. Configure an Amazon S3 event notification for the s3:ObjectCreated event type. Filter the event type by ConsoleLogin failed events. Configure the event notification to forward to the SNS topic.

> **Suggested Answer:** A
> **Community Vote:** A (93%), 7%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**eugene2owl** (Tue 10 Dec 2024 17:13) - *Upvotes: 6*
"A" is indeed the most elegant and obvious solution.
"B" might work but seems way more overcomplicated

---

**teo2157** (Wed 18 Dec 2024 11:08) - *Upvotes: 5*
A as you can choose to send cloudtrail events to CloudWatch log groups.

---

**Srikantha** (Sat 05 Apr 2025 16:51) - *Upvotes: 2*
This solution leverages AWS CloudTrail for logging, CloudWatch Logs for capturing the log data, and CloudWatch Alarms for monitoring the failed login attempts, with SNS used for notifications. It provides the least operational effort for the following reasons:

AWS CloudTrail captures management events, including failed login attempts (ConsoleLogin failures).
These events are sent to Amazon CloudWatch Logs, which is a straightforward way to centralize the log data for analysis.
A CloudWatch Logs metric filter is created to match the ConsoleLogin failure events. This metric filter scans the CloudWatch logs for specific failed login attempts.
CloudWatch Alarm is created based on the metric filter to trigger when there are multiple failed login attempts.
The alarm is configured to send a message to the SNS topic, notifying the security team.
This solution automates the detection of failed login attempts and provides a simple, efficient way to send notifications with minimal ongoing management.

---

**On9son** (Tue 03 Dec 2024 08:52) - *Upvotes: 1*
CloudTrail publishes log to S3.
And management event contains login information
https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-events.html#cloudtrail-management-events

---


<br/>

## Question 331

*Date: Dec. 16, 2024, 2:06 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has deployed a new REST API by using Amazon API Gateway. The company uses the API to access confidential data. The API must be accessed from only specific VPCs in the company.

Which solution will meet these requirements?

**Options:**
- A. Create and attach a resource policy to the API Gateway API. Configure the resource policy to allow only the specific VPC IDs.
- B. Add a security group to the API Gateway API. Configure the inbound rules to allow only the specific VPC IP address ranges.
- C. Create and attach an IAM role to the API Gateway API. Configure the IAM role to allow only the specific VPC IDs.
- D. Add an ACL to the API Gateway API. Configure the outbound rules to allow only the specific VPC IP address ranges.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Sat 05 Apr 2025 16:53) - *Upvotes: 1*
Amazon API Gateway supports resource policies, which allow you to control who can access your API based on the source IP address, VPC ID, or even specific IP address ranges.

In this case, to restrict access to the API from only specific VPCs, you would create and attach a resource policy to the API Gateway. The resource policy allows you to specify which VPCs (via their VPC IDs) can access the API, ensuring that the API can only be accessed from the designated VPCs.

The resource policy is the most efficient and appropriate method for achieving this in API Gateway.

---

**CHRIS12722222** (Sat 28 Dec 2024 18:04) - *Upvotes: 3*
https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-private-api-create.html

---

**Ky_24** (Mon 16 Dec 2024 14:06) - *Upvotes: 4*
Explanation:
API Gateway supports resource policies, which can restrict access based on specific conditions, such as VPC IDs or IP ranges. You can attach a resource policy to the API Gateway that allows access only from specific VPCs. This is the most direct and secure way to meet the requirement of allowing access only from specific VPCs.

---


<br/>

## Question 332

*Date: Oct. 31, 2024, 12:01 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company runs a website by using an Amazon Elastic Container Service (Amazon ECS) service that is connected to an Application Load Balancer (ALB). The service was in a steady state with tasks responding to requests successfully.

A DevOps engineer updated the task definition with a new container image and deployed the new task definition to the service. The DevOps engineer noticed that the service is frequently stopping and starting new tasks because the ALB healtth checks are failing.

What should the DevOps engineer do to troubleshoot the failed deployment?

**Options:**
- A. Ensure that a security group associated with the service allows traffic from the ALB.
- B. Increase the ALB health check grace period for the service.
- C. Increase the service minimum healthy percent setting.
- D. Decrease the ALB health check interval.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Sat 05 Apr 2025 17:00) - *Upvotes: 2*
When the DevOps engineer deploys a new version of the ECS service (e.g., with a new container image), the ECS tasks might take some time to become healthy and respond to ALB health checks successfully. During this period, the ALB health checks could fail if they are not given enough time to detect the new tasks' readiness.

By increasing the ALB health check grace period, the engineer ensures that the new tasks have enough time to start up and become healthy before the ALB marks them as unhealthy. This can prevent the tasks from frequently stopping and starting due to health check failures.

---

**DKM** (Tue 18 Mar 2025 20:36) - *Upvotes: 1*
aws elbv2 modify-target-group-attributes \
--target-group-arn <target-group-arn> \
--attributes Key=health_check.grace_period,Value=<desired-value>

---

**Ky_24** (Mon 16 Dec 2024 14:08) - *Upvotes: 4*
B - ALB health check is failing the new task

---

**eugene2owl** (Tue 10 Dec 2024 17:30) - *Upvotes: 4*
"A" is wrong because in the question it's said that Engineer has only updated image in the task definition. So he could not affect security group.
"B" might be correct explanation, because new Docker container (based on a new image) takes longer time to start. So increasing a grace period might help to eventually get a successful health-check result and quit restarting the container.

---

**awsarchitect5** (Thu 31 Oct 2024 12:01) - *Upvotes: 4*
B

---


<br/>

## Question 333

*Date: Nov. 22, 2024, 9:50 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company that uses electronic patient health records runs a fleet of Amazon EC2 instances with an Amazon Linux operating system. The company must continuously ensure that the EC2 instances are running operating system patches and application patches that are in compliance with current privacy regulations. The company uses a custom repository to store application patches.

A DevOps engineer needs to automate the deployment of operating system patches and application patches. The DevOps engineer wants to use both the default operating system patch repository and the custom patch repository.

Which solution will meet these requirements with the LEAST effort?

**Options:**
- A. Use AWS Systems Manager to create a new custom patch baseline that includes the default operating system repository and the custom repository. Run the AWS-RunPatchBaseline document by using the Run command to verify and install patches. Use the BaselineOverride API to configure the new custom patch baseline.
- B. Use AWS Direct Connect to integrate the custom repository with the EC2 instances. Use Amazon EventBridge events to deploy the patches.
- C. Use the yum-config-manager command to add the custom repository to the /etc/yum.repos.d configuration. Run the yum-config-manager-enable command to activate the new repository.
- D. Use AWS Systems Manager to create a patch baseline for the default operating system repository and a second patch baseline for the custom repository. Run the AWS-RunPatchBaseline document by using the Run command to verify and install patches. Use the BaselineOverride API to configure the default patch baseline and the custom patch baseline.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**phu0298** (Fri 22 Nov 2024 09:50) - *Upvotes: 6*
A
Why Not Option D?
Two Separate Patch Baselines:
Creating and managing two separate patch baselines (one for OS patches and one for the custom repository) increases complexity.
Running AWS-RunPatchBaseline twice (once for each baseline) adds unnecessary operational overhead.

---

**Srikantha** (Sun 06 Apr 2025 16:19) - *Upvotes: 2*
D. Systems Manager does not support using two separate patch baselines at once. You must define a single patch baseline that includes all patch sources.

---

**Srikantha** (Sun 06 Apr 2025 16:18) - *Upvotes: 2*
To automate patching while using both the default OS patch repo and a custom app patch repo, the most efficient and scalable solution is to:

Use AWS Systems Manager Patch Manager, which supports:
Custom patch baselines
Combining default repositories with custom ones
Centralized, automated patch management
Create a custom patch baseline that:
Includes the Amazon Linux OS patching rules
Defines custom sources (your custom app patch repository)
Use AWS-RunPatchBaseline SSM document to apply patches.
Optionally, use the BaselineOverride parameter if you want to temporarily apply a different baseline (e.g., for testing).

---

**f4b18ba** (Fri 22 Nov 2024 23:19) - *Upvotes: 4*
AWS Systems Manager allows you to create a custom patch baseline that includes both the default operating system repository and the custom repository. This centralizes the management of patch baselines.
The AWS-RunPatchBaseline document can be run using the Systems Manager Run Command to automate the verification and installation of patches, ensuring compliance with current privacy regulations.
Using the BaselineOverride API provides flexibility to override the default settings with a custom patch baseline, streamlining the patching process across all EC2 instances.

---


<br/>

## Question 334

*Date: Nov. 22, 2024, 10:23 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company use an organization in AWS Organizations to manage multiple AWS accounts. The company has enabled all features enabled for the organization. The company configured the organization as a hierarchy of OUs under the root OU. The company recently registered all its OUs and enrolled all its AWS accounts in AWS Control Tower.

The company needs to customize the AWS Control Tower managed AWS Config configuration recorder in each of the company's AWS accounts. The company needs to apply the customizations to both the existing AWS accounts and to any new AWS accounts that the company enrolls in AWS Control Tower in the future.

Which combination of steps will meet these requirements? (Choose three.)

**Options:**
- A. Create a new AWS account. Create an AWS Lambda function in the new account to apply the customizations to the AWS Config configuration recorder in each AWS account in the organization.
- B. Create a new AWS account as an AWS Config delegated administrator. Create an AWS Lambda function in the delegated administrator account to apply the customizations to the AWS Config configuration recorder in the delegated administrator account.
- C. Configure an Amazon EventBridge rule in the AWS Control Tower management account to invoke an AWS Lambda function when the Organizations OU is registered or reregistered. Re-register the root Organizations OU.
- D. Configure the AWSControlTowerExecution IAM role in each AWS account in the organization to be assumable by an AWS Lambda function. Configure the Lambda function to assume the AWSControlTowerExecution IAM role.
- E. Create an IAM role in the AWS Control Tower management account that an AWS Lambda function can assume. Grant the IAM role permission to assume the AWSControlTowerExecution IAM role in any account in the organization. Configure the Lambda function to use the new IAM role.
- F. Configure an Amazon EventBridge rule in the AWS Control Tower management account to invoke an AWS Lambda function when an AWS account is updated or enrolled in AWS Control Tower or when the landing zone is updated. Re-register each Organizations OU in the organization.

> **Suggested Answer:** AEF
> **Community Vote:** AEF (56%), CEF (22%), 11%, 11%, A (35%), C (25%), B (20%), Other

### Discussions

**0ac7838** (Sun 16 Nov 2025 09:51) - *Upvotes: 1*
✔ C – EventBridge rule for OU registration (existing accounts)
✔ F – EventBridge rule for account enrollment/update (new accounts)
✔ E – IAM role in management account that allows Lambda to assume AWSControlTowerExecution

---

**Srikantha** (Sun 06 Apr 2025 16:31) - *Upvotes: 1*
❌ Why not the others?
A. While a Lambda in a new account can work, it’s better to use a delegated admin for AWS Config management.
C. EventBridge doesn’t emit events when an OU is registered or re-registered. This is not a valid trigger.
D. Modifying the AWSControlTowerExecution role in each account breaks the Control Tower managed roles and is not recommended.

---

**DKM** (Wed 26 Mar 2025 14:40) - *Upvotes: 1*
These steps ensure that the customizations are applied consistently across all existing and new AWS accounts, leveraging the delegated administrator account for centralized management and automation123.

---

**fcbflo** (Mon 06 Jan 2025 08:56) - *Upvotes: 1*
C covers OU registration/re-registration events⁠
F handles account enrollment, updates, and landing zone changes⁠
⁠​E's role in providing the necessary IAM permissions structure:
Creates proper IAM role in Control Tower management account
Enables assumption of AWSControlTowerExecution IAM role across accounts

---

**CHRIS12722222** (Sun 29 Dec 2024 11:06) - *Upvotes: 3*
https://aws.amazon.com/solutions/guidance/customizing-aws-config-resources-in-aws-control-tower/

- Need eventbridge in CT management acct to react to CT lifecycle events
- need CT management acct lambda function to assume AWSControlTowerExecution role and customise config.
- If lambda is not in CT management acct then it will need to assume a role in CT management acct which has trust with AWSControlTowerExecution role in member accts

---

**teo2157** (Wed 18 Dec 2024 11:59) - *Upvotes: 2*
I think there is a misspelling in the A option as it's said just "Create a new AWS account" when it should said " Create a new AWS account as an AWS Config delegated administrator.", said that, I go for AEF.

---

**phu0298** (Fri 22 Nov 2024 10:23) - *Upvotes: 3*
B, E, and F.
B: AWS Config supports delegated administrators, allowing a central account to manage configurations across the organization.
By creating a Lambda function in the delegated administrator account, you can apply the customizations to the AWS Config configuration recorder in all member accounts centrally.

E: The AWSControlTowerExecution IAM role exists in each enrolled account and allows centralized operations.
The IAM role in the management account needs permissions to assume the AWSControlTowerExecution role in member accounts.

F: AWS Control Tower emits events when an account is enrolled or updated, or when the landing zone is updated.
An EventBridge rule can trigger the Lambda function to ensure that any new or updated accounts automatically receive the customizations.
Re-registering each OU ensures that Control Tower applies its governance to all accounts and OUs consistently.

---


<br/>

## Question 335

*Date: Dec. 10, 2024, 5:42 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company runs an application in an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2 instances run Docker containers that make requests to a MySQL database that runs on separate EC2 instances.

A DevOps engineer needs to update the application to use a serverless architecture.

Which solution will meet this requirement with the FEWEST changes?

**Options:**
- A. Replace the containers that run on EC2 instances and the ALB with AWS Lambda functions. Replace the MySQL database with an Amazon Aurora Serverless v2 database that is compatible with MySQL.
- B. Replace the containers that run on EC2 instances with AWS Fargate. Replace the MySQL database with an Amazon Aurora Serverless v2 database that is compatible with MySQL.
- C. Replace the containers that run on EC2 instances and the ALB with AWS Lambda functions. Replace the MySQL database with Amazon DynamoDB tables.
- D. Replace the containers that run on EC2 instances with AWS Fargate. Replace the MySQL database with Amazon DynamoDB tables.

> **Suggested Answer:** B
> **Community Vote:** B (80%), A (20%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Sat 05 Apr 2025 22:11) - *Upvotes: 2*
Replace EC2 + ALB with AWS Lambda → Moves compute to serverless.
Replace MySQL with Aurora Serverless v2 (MySQL-compatible) → Keeps compatibility with MySQL, meaning minimal code/database query changes.
✅ Best choice because it fully transitions to serverless (compute + database) with the least disruption to existing code.


It offers a fully serverless solution with minimum code changes, since:

Lambda can take over the logic in containers.
Aurora Serverless v2 provides MySQL compatibility, so the data layer is minimally impacted.

---

**CHRIS12722222** (Sun 29 Dec 2024 11:08) - *Upvotes: 4*
fargate and aurora serverless v2

---

**eugene2owl** (Tue 10 Dec 2024 17:42) - *Upvotes: 4*
"B" is the most easy-to-implement option - as the question asks.
The rest options require moving from SQL to NoSQL (which requires multiple code changes) and/or moving from Docker containers to Lambda (which requires multiple code changes).

---


<br/>

## Question 336

*Date: Nov. 22, 2024, 11:31 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses an organization in AWS Organizations to manage 10 AWS accounts. All features are enabled, and trusted access for AWS CloudFormation is enabled.

A DevOps engineer needs to use CloudFormation to deploy an IAM role to the Organizations management account and all member accounts in the organization.

Which solution will meet these requirements with the LEAST operational overhead?

**Options:**
- A. Create a CloudFormation StackSet that has service-managed permissions. Set the root OU as a deployment target.
- B. Create a CloudFormation StackSet that has service-managed permissions. Set the root OU as a deployment target. Deploy a separate CloudFormation stack in the Organizations management account.
- C. Create a CloudFormation StackSet that has self-managed permissions. Set the root OU as a deployment target.
- D. Create a CloudFormation StackSet that has self-managed permissions. Set the root OU as a deployment target. Deploy a separate CloudFormation stack in the Organizations management account.

> **Suggested Answer:** B
> **Community Vote:** B (61%), A (39%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**CHRIS12722222** (Sun 29 Dec 2024 13:00) - *Upvotes: 6*
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-orgs-associate-stackset-with-org.html#:~:text=StackSets%20doesn%27t%20deploy%20stack%20instances%20to%20the%20organization%27s%20management%20account%2C%20even%20if%20the%20management%20account%20is%20in%20your%20organization%20or%20in%20an%20OU%20in%20your%20organization

Stackset cant deploy to management acct

---

**Srikantha** (Sat 05 Apr 2025 22:27) - *Upvotes: 1*
CloudFormation StackSets with service-managed permissions use AWS Organizations integration to automatically assume roles in target accounts — no manual role setup required.
Setting the root OU as the deployment target means that the stack set will deploy to all existing and future accounts in the organization.
You don’t need to separately deploy anything to the management account unless it must be treated differently (which is not mentioned here).
This is the most hands-off, scalable approach — ideal for centrally managing IAM roles across all accounts in an organization.

---

**jojewi8143** (Sun 02 Feb 2025 19:38) - *Upvotes: 2*
Stackset cant deploy to management account.

---

**spring21** (Sat 21 Dec 2024 01:08) - *Upvotes: 2*
StackSets automatically generates the IAM roles required to deploy stack instances. You can create the IAM roles required by the AWS CloudFormation StackSets feature in the management account of AWS Organizations.

---

**spring21** (Sat 21 Dec 2024 01:08) - *Upvotes: 1*
Service-managed permissions
StackSets automatically generate the IAM roles required to deploy stack instances. You can create the IAM roles required by the AWS CloudFormation StackSets feature in the management account of AWS Organizations. You can also delegate the management of StackSets to member accounts.

---

**Impromptu** (Sun 24 Nov 2024 13:41) - *Upvotes: 3*
Should be B I think. A stackset with service-managed permissions does not deploy to the management account.

"StackSets doesn't deploy stack instances to the organization's management account, even if the management account is in your organization or in an OU in your organization."
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-getting-started-create.html#stacksets-orgs-associate-stackset-with-org

---

**Changwha** (Sat 23 Nov 2024 11:46) - *Upvotes: 1*
A. Create a CloudFormation StackSet that has service-managed permissions. Set the root OU as a deployment target.

---

**f4b18ba** (Fri 22 Nov 2024 23:31) - *Upvotes: 3*
Using service-managed permissions simplifies the deployment process because AWS manages the permissions required for deploying the StackSet. This reduces the complexity and effort involved in setting up and managing permissions manually.
By setting the root Organizational Unit (OU) as the deployment target, the StackSet will automatically deploy the IAM role to all AWS accounts under the root OU, including both existing and future accounts. This ensures comprehensive and automatic coverage.
Service-managed StackSets provide a streamlined and scalable solution, requiring minimal manual intervention and oversight, thus reducing operational overhead.

---


<br/>

## Question 337

*Date: Nov. 19, 2024, 9:56 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company runs an application that stores artifacts in an Amazon S3 bucket. The application has a large user base. The application writes a high volume of objects to the S3 bucket. The company has enabled event notifications for the S3 bucket.

When the application writes an object to the S3 bucket, several processing tasks need to be performed simultaneously. The company's DevOps team needs to create an AWS Step Functions workflow to orchestrate the processing tasks.

Which combination of steps should the DevOps team take to meet these requirements with the LEAST operational overhead? (Choose two.)

**Options:**
- A. Create a Standard workflow that contains a parallel state that defines the processing tasks. Create an Asynchronous Express workflow that contains a parallel state that defines the processing tasks.
- B. Create a Synchronous Express workflow that contains a map state that defines the processing tasks.
- C. Create an Amazon EventBridge rule to match when a new S3 object is created. Configure the EventBridge rule to invoke an AWS Lambda function. Configure the Lambda function to start the processing workflow.
- D. Create an Amazon EventBridge rule to match when a new S3 object is created. Configure the EventBridge rule to start the processing workflow.

> **Suggested Answer:** AD
> **Community Vote:** AD (83%), Other, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Sun 06 Apr 2025 16:42) - *Upvotes: 2*
✅ A. Create a Standard workflow that contains a parallel state that defines the processing tasks.
Standard workflows in AWS Step Functions are best suited for long-running, durable processes and support parallel states, which are perfect for triggering multiple tasks simultaneously.
Since the processing involves several tasks, this structure provides reliability, retries, and observability out of the box.
✅ D. Create an Amazon EventBridge rule to match when a new S3 object is created. Configure the EventBridge rule to start the processing workflow.
EventBridge can directly invoke a Step Functions workflow (without needing an intermediate Lambda function), which reduces operational overhead.
It simplifies the event-driven architecture by removing custom code and improves maintainability.

---

**teo2157** (Mon 20 Jan 2025 15:40) - *Upvotes: 1*
Changing my mind, as I said previously, there's a misspelling but the problem is that " Create an Asynchronous Express workflow that contains a parallel state that defines the processing tasks. " should be a separate option as it's the correct first part. D is pretty evident.

---

**teo2157** (Wed 18 Dec 2024 13:32) - *Upvotes: 3*
There's a misspelling and the option B is included in the option A, said that, going for standard workflow as it's recommended for long-running (up to one year), durable, and auditable workflows instead of Express Workflows that are ideal for high-volume, event-processing workloads such as IoT data ingestion, streaming data processing and transformation, and mobile application backends and can run for up to five minutes. .

---

**luisfsm_111** (Wed 11 Dec 2024 13:01) - *Upvotes: 3*
Because of the requirement for parallel task processing, I vote for A and D.

---

**tinyshare** (Sun 08 Dec 2024 07:34) - *Upvotes: 1*
Standard workflow is for long-running. Express workflow is for high-volume.
So B is better than A.
https://docs.aws.amazon.com/step-functions/latest/dg/choosing-workflow-type.html

---

**uncledana** (Tue 19 Nov 2024 09:56) - *Upvotes: 2*
Use a Standard Step Functions workflow with parallel states to handle the processing tasks (Option A).
Use an EventBridge rule to directly trigger the Step Functions workflow (Option D).

---


<br/>

## Question 338

*Date: Nov. 19, 2024, 10:04 a.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps team supports an application that runs in an Amazon Elastic Container Service (Amazon ECS) cluster behind an Application Load Balancer (ALB). Currently, the DevOps team uses AWS CodeDeploy to deploy the application by using a blue/green all-at-once strategy. Recently, the DevOps team had to roll back a deployment when a new version of the application dramatically increased response times for requests.

The DevOps team needs use to a deployment strategy that will allow the team to monitor a new version of the application before the team shifts all traffic to the new version. If a new version of the application increases response times, the deployment should be rolled back as quickly as possible.

Which combination of steps will meet these requirements? (Choose two.)

**Options:**
- A. Modify the CodeDeploy deployment to use the CodeDeployDefault.ECSCanary10Percent5Minutes configuration.
- B. Modify the CodeDeploy deployment to use the CodeDeployDefault.ECSLinear10PercentEvery3Minutes configuration.
- C. Create an Amazon CloudWatch alarm to monitor the UnHealthyHostCount metric for the ALB. Set the alarm to activate if the metric is higher than the desired value. Associate the alarm with the CodeDeploy deployment group. Modify the deployment group to roll back when a deployment fails.
- D. Create an Amazon CloudWatch alarm to monitor the TargetResponseTime metric for the ALB. Set the alarm to activate if the metric is higher than the desired value. Associate the alarm with the CodeDeploy deployment group. Modify the deployment group to roll back when alarm thresholds are met.
- E. Create an Amazon CloudWatch alarm to monitor the TargetConnectionErrorCount metric for the ALB. Set the alarm to activate if the metric is higher than the desired value. Associate the alarm with the CodeDeploy deployment group. Modify the deployment group to roll back when alarm thresholds are met.

> **Suggested Answer:** AD
> **Community Vote:** AD (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**f4b18ba** (Fri 22 Nov 2024 23:43) - *Upvotes: 5*
Using a canary deployment strategy with CodeDeployDefault.ECSCanary10Percent5Minutes allows for controlled, gradual deployment, providing the ability to monitor the new version's performance. Simultaneously, monitoring the TargetResponseTime with CloudWatch alarms ensures that any issues with response times are detected early, allowing for quick rollbacks to the previous stable version.

---

**uncledana** (Tue 19 Nov 2024 10:04) - *Upvotes: 3*
• Use the CodeDeployDefault.ECSCanary10Percent5Minutes configuration (Option A) for canary deployment to test with minimal traffic initially.
• Monitor the ALB’s TargetResponseTime metric (Option D) with an alarm to detect performance issues and trigger a rollback if needed.

---


<br/>

## Question 339

*Date: Nov. 19, 2024, 10:11 a.m.
Disclaimers:
- ExamTopics website is not rel*

A security team must record the configuration of AWS resources, detect issues, and send notifications for findings. The main workload in the AWS account consists of an Amazon EC2 Auto Scaling group that scales in and out several times during the day.

The team wants to be notified within 2 days if any Amazon EC2 security group allows traffic on port 22 for 0.0.0.0/0. The team also needs a snapshot of the configuration of the AWS resources to be taken routinely.

The security team has already created and subscribed to an Amazon Simple Notification Service (Amazon SNS) topic.

Which solution meets these requirements?

**Options:**
- A. Configure AWS Config to use periodic recording for the AWS account. Deploy the vpc-sg-port-restriction-check AWS Config managed rule. Configure AWS Config to use the SNS topic as the target for notifications.
- B. Configure AWS Config to use configuration change recording for the AWS account. Deploy the vpc-sg-open-only-to-authorized-ports AWS Config managed rule. Configure AWS Config to use the SNS topic as the target for notifications.
- C. Configure AWS Config to use configuration change recording for the AWS account. Deploy the ssh-restricted AWS Config managed rule. Configure AWS Config to use the SNS topic as the target for notifications.
- D. Create an AWS Lambda function to evaluate security groups and publish a message to the SNS topic. Use an Amazon EventBridge rule to schedule the Lambda function to run once a day.

> **Suggested Answer:** C
> **Community Vote:** C (79%), A (16%), 5%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**f4b18ba** (Fri 22 Nov 2024 23:47) - *Upvotes: 5*
Configuration Change Recording: By configuring AWS Config to use configuration change recording, the system will continuously monitor and record configurations of your AWS resources whenever there are changes. This ensures real-time compliance monitoring and reduces the delay in detection.

Appropriate Managed Rule: The ssh-restricted AWS Config managed rule specifically checks for security groups that allow unrestricted SSH (port 22) access. This rule directly addresses the requirement to be notified if any EC2 security group allows traffic on port 22 for 0.0.0.0/0.

Notification Setup: Configuring AWS Config to use the SNS topic ensures that the security team will be notified within the specified time frame if the rule is violated. AWS Config can send notifications to the SNS topic as soon as a non-compliant resource is detected.

---

**Srikantha** (Sun 06 Apr 2025 16:50) - *Upvotes: 1*
AWS Config with configuration change recording ensures that every time a resource configuration changes, it's recorded—ideal for dynamic environments like Auto Scaling groups.
The ssh-restricted managed rule specifically checks whether port 22 (SSH) is open to 0.0.0.0/0, which directly satisfies the requirement.
Configuring AWS Config to publish to an existing SNS topic ensures the security team is notified of any findings within 2 days or sooner, depending on when the change occurs.

---

**jojewi8143** (Sun 02 Feb 2025 19:45) - *Upvotes: 2*
ssh-restricted

---

**matt200** (Mon 30 Dec 2024 14:43) - *Upvotes: 1*
Why not B?

https://docs.aws.amazon.com/config/latest/developerguide/vpc-sg-open-only-to-authorized-ports.html

---

**matt200** (Mon 06 Jan 2025 08:00) - *Upvotes: 2*
change my mind to C

---

**CHRIS12722222** (Sun 29 Dec 2024 16:25) - *Upvotes: 1*
i think we need to use periodic or daily recording instead of continuous one

---

**spring21** (Sat 21 Dec 2024 20:48) - *Upvotes: 4*
For monitoring if any EC2 security group allows traffic on port 22 (SSH) from 0.0.0.0/0:
Use the managed AWS Config rule:

restricted-ssh
This rule checks that security groups do not allow unrestricted incoming SSH traffic (port 22) from 0.0.0.0/0.

---

**eugene2owl** (Wed 11 Dec 2024 11:56) - *Upvotes: 2*
"A" because "vpc-sg-port-restriction-check" fits requested check well, and the condition says "... to be taken routinely", which means "periodically", "regularly".
"B" and "C" propose running "on-change" instead, which does not fit condition "routinely"

---

**uncledana** (Tue 19 Nov 2024 10:11) - *Upvotes: 3*
The correct answer is C

---


<br/>

## Question 340

*Date: Nov. 19, 2024, 10:20 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has proprietary data available by using an Amazon CloudFront distribution. The company needs to ensure that the distribution is accessible by only users from the corporate office that have a known set of IP address ranges. An AWS WAF web ACL is associated with the distribution and has a default action set to Count.

Which solution will meet these requirements with the LEAST operational overhead?

**Options:**
- A. Create a new regex pattern set. Add the regex pattern set to a new rule group. Create a new web ACL that has a default action set to Block. Associate the web ACL with the CloudFront distribution. Add a rule that allows traffic based on the new rule group.
- B. Create an AWS WAF IP address set that matches the corporate office IP address range. Create a new web ACL that has a default action set to Allow. Associate the web ACL with the CloudFront distribution. Add a rule that allows traffic from the IP address set.
- C. Create a new regex pattern set. Add the regex pattern set to a new rule group. Set the default action on the existing web ACL to Allow. Add a rule that has priority 0 that allows traffic based on the regex pattern set.
- D. Create a WAF IP address set that matches the corporate office IP address range. Set the default action on the existing web ACL to Block. Add a rule that has priority 0 that allows traffic from the IP address set.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Sun 06 Apr 2025 16:51) - *Upvotes: 2*
Goal: Allow access only from specific IP ranges (corporate office), and block everything else.
WAF IP set: This is the right tool for matching specific source IP addresses.
Default action = Block: Ensures all traffic is blocked unless explicitly allowed.
Rule priority 0 (highest priority): Ensures that corporate IPs are evaluated first and allowed.
Uses existing web ACL: Minimizes overhead by not needing to create a new ACL.

---

**teo2157** (Wed 18 Dec 2024 16:04) - *Upvotes: 4*
Agreee with D as prioty 0 is the highest priority rule

---

**f4b18ba** (Fri 22 Nov 2024 23:52) - *Upvotes: 4*
Using Existing Web ACL: This approach leverages the existing web ACL, minimizing the need to create a new one, which reduces operational overhead.

IP Address Set: By creating a WAF IP address set that matches the corporate office IP address range, you precisely define which IP addresses are allowed access.

Blocking by Default: Setting the default action to Block ensures that only traffic from the defined IP addresses is allowed, meeting the security requirement.

High Priority Rule: Adding a high-priority rule (priority 0) to allow traffic from the IP address set ensures that legitimate traffic from the corporate office is not blocked.

---

**uncledana** (Tue 19 Nov 2024 10:20) - *Upvotes: 4*
The requirements are:

1. Restrict access to the CloudFront distribution to users from a known set of IP address ranges (the corporate office).
2. Minimize operational overhead.
3. Use the existing AWS WAF web ACL, which has the default action set to Count.

Option D: Create a WAF IP address set that matches the corporate office IP address range. Set the default action on the existing web ACL to Block. Add a rule that has priority 0 that allows traffic from the IP address set.

---


<br/>

## Question 341

*Date: Nov. 19, 2024, 10:45 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company runs several applications in the same AWS account. The applications send logs to Amazon CloudWatch.

A data analytics team needs to collect performance metrics and custom metrics from the applications. The analytics team needs to transform the metrics data before storing the data in an Amazon S3 bucket. The analytics team must automatically collect any new metrics that are added to the CloudWatch namespace.

Which solution will meet these requirements with the LEAST operational overhead?

**Options:**
- A. Configure a CloudWatch metric stream to include metrics from the application and the CloudWatch namespace. Configure the metric stream to deliver the metrics to an Amazon Data Firehose delivery stream. Configure the Firehose delivery stream to invoke an AWS Lambda function to transform the data. Configure the delivery stream to send the transformed data to the S3 bucket.
- B. Configure a CloudWatch metrics stream to include all the metrics and to deliver the metrics to an Amazon Data Firehose delivery stream. Configure the Firehose delivery stream to invoke an AWS Lambda function to transform the data. Configure the delivery stream to send the transformed data to the S3 bucket.
- C. Configure metric filters for the CloudWatch logs to create custom metrics. Configure a CloudWatch metric stream to deliver the application metrics to the S3 bucket.
- D. Configure subscription filters on the application log groups to target an Amazon Data Firehose delivery stream. Configure the Firehose delivery stream to invoke an AWS Lambda function to transform the data. Configure the delivery stream to send the transformed data to the S3 bucket.

> **Suggested Answer:** B
> **Community Vote:** B (53%), A (47%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**CHRIS12722222** (Sun 29 Dec 2024 16:46) - *Upvotes: 6*
Why not option B?

Just collect all the metrics in the namespace, if new ones are added later, it will also be collected

---

**GripZA** (Tue 22 Apr 2025 18:50) - *Upvotes: 1*
I think his Selected Answer supposed to be A.

---

**uncledana** (Tue 19 Nov 2024 10:45) - *Upvotes: 5*
Explanation:

The requirements are:

1. Collect performance metrics and custom metrics from CloudWatch.
2. Automatically include new metrics added to the namespace.
3. Transform the metrics data.
4. Store the transformed data in an S3 bucket.
5. Minimize operational overhead.

Option A: Configure a CloudWatch metric stream to include metrics from the application and the CloudWatch namespace. Configure the metric stream to deliver the metrics to an Amazon Data Firehose delivery stream. Configure the Firehose delivery stream to invoke an AWS Lambda function to transform the data. Configure the delivery stream to send the transformed data to the S3 bucket.

---

**BietTuot** (Fri 22 Aug 2025 15:11) - *Upvotes: 1*
B is the correct answer.

Imagine have an application that currently publishes these metrics: app/metrics (RequestCount, ErrorCount).

You configure a CloudWatch metric stream (Option A) with a filter: Include only metrics from namespace App/Metrics

Day 1: Metrics RequestCount and ErrorCount are captured in the metric stream.
...
Day 7: Your development team deploys a new feature that emits metrics in a new namespace: Namespace: App/Cache (CacheHitRate, CacheMissRate)

Because your metric stream was explicitly configured to include only App/Metrics, the new App/Cache namespace is not included.

With Option B (all metrics): On Day 7, the new App/Cache metrics automatically appear in S3 without you touching anything. ✅

---

**Srikantha** (Sun 06 Apr 2025 16:53) - *Upvotes: 2*
CloudWatch metric streams are designed specifically to stream metric data in near real-time to destinations like Firehose.
Selecting "all metrics" ensures that any new metrics added in the future are automatically included, satisfying the requirement to "automatically collect any new metrics."
Amazon Kinesis Data Firehose supports Lambda transforms, so the analytics team can modify data before it's delivered.
Output to S3 is natively supported by Firehose, which satisfies the storage requirement.

---

**4eaa7a4** (Sat 15 Nov 2025 11:00) - *Upvotes: 1*
It does say collect any new metrics from the Cloudwatch namespace that is already added (as a whole) in answer A

---

**rinip86277** (Mon 17 Feb 2025 07:21) - *Upvotes: 1*
B is better than A because new metrics must be automatically collected. With A they have to be manually selected. With B they are automatically collected.

---

**Ky_24** (Mon 16 Dec 2024 21:25) - *Upvotes: 4*
A is correct

---


<br/>

## Question 342

*Date: Nov. 19, 2024, 10:58 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses an HPC platform to run analysis jobs for data. The company uses AWS CodeBuild to create container images and store the images on Amazon Elastic Container Registry (Amazon ECR). The images are then deployed on Amazon Elastic Kubernetes Service (Amazon EKS).

To maintain compliance, the company needs to ensure that the images are signed before the images are deployed on Amazon EKS. The signing keys must be rotated periodically and must be managed automatically. The company needs to track who generates the signatures.

Which solution will meet these requirements with the LEAST operational effort?

**Options:**
- A. Use CodeBuild to retrieve the image that was previously pushed to Amazon ECR. Use AWS Signer to sign the image. Use AWS CloudTrail to track who generates the signatures.
- B. Use AWS Lambda to retrieve the image that was previously pushed to Amazon ECR. Use a Lambda function to sign the image. Use Amazon CloudWatch to track who generates the signatures.
- C. Use AWS Lambda to retrieve the image that was previously pushed to Amazon ECR. Use AWS Signer to sign the image. Use Amazon CloudWatch to track who generates the signatures.
- D. Use CodeBuild to build the image. Sign the image by using AWS Signer before pushing the image to Amazon ECR. Use AWS CloudTrail to track who generates the signatures.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Sun 06 Apr 2025 16:55) - *Upvotes: 1*
CodeBuild is already in use and integrates well with image build workflows.
AWS Signer supports container image signing, especially with Amazon ECR.
Signing before pushing ensures that only signed images are stored and used — a best practice for compliance.
Key rotation is managed automatically by AWS Key Management Service (KMS) when used with AWS Signer.
AWS CloudTrail tracks who signs the image (i.e., tracks the AWS identity that invokes Signer).
This setup has the least operational overhead because it stays within managed AWS services and integrates smoothly with the existing build pipeline.

---

**uncledana** (Tue 19 Nov 2024 10:58) - *Upvotes: 4*
Explanation:

This solution meets all of the requirements with the least operational effort because:

1. Image Signing with AWS Signer:
2. Automated Key Rotation:
3. Tracking Who Signs the Images:
4. Using CodeBuild for Image Creation:
5. Least Operational Effort:

---


<br/>

## Question 343

*Date: Nov. 19, 2024, 11 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses an AWS CodeArtifact repository to store Python packages that the company developed internally. A DevOps engineer needs to use AWS CodeDeploy to deploy an application to an Amazon EC2 instance. The application uses a Python package that is stored in the CodeArtifact repository. A BeforeInstall lifecycle event hook will install the package.

The DevOps engineer needs to grant the EC2 instance access to the CodeArtifact repository.

Which solution will meet this requirement?

**Options:**
- A. Create a service-linked role for CodeArtifact. Associate the role with the EC2 instance. Use the aws codeartifact get-authorization-token CLI command on the instance.
- B. Configure a resource-based policy for the CodeArtifact repository that allows the ReadFromRepository action for the EC2 instance principal.
- C. Configure ACLs on the CodeArtifact repository to allow the EC2 instance to access the Python package.
- D. Create an instance profile that contains an IAM role that has access to CodeArtifact. Associate the instance profile with the EC2 instance. Use the aws codeartifact login CLI command on the instance.

> **Suggested Answer:** D
> **Community Vote:** D (71%), B (29%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**nickp84** (Sat 24 May 2025 19:00) - *Upvotes: 1*
B. Configure a resource-based policy for the CodeArtifact repository:
Issue: CodeArtifact repositories do not support resource-based policies (unlike S3 buckets or SNS topics). Access to CodeArtifact is controlled entirely through IAM policies attached to roles or users, making this option invalid.

---

**GripZA** (Tue 22 Apr 2025 18:59) - *Upvotes: 1*
just like other resource policies, CodeArtifact uses resource-based permissions to control access. Resource-based permissions let you specify who has access to a repository and what actions they can perform on it. By default, only the repository owner has access to a repository so implicit deny unless you apply a policy document that allows other IAM principals to access your repository.

https://docs.aws.amazon.com/codeartifact/latest/ug/repo-policies.html

---

**robotgeek** (Thu 15 May 2025 09:12) - *Upvotes: 1*
That is not saying that "specifically" you can not use IBPs, IBPs are prefered when you want to give individual access

---

**Srikantha** (Sun 06 Apr 2025 16:56) - *Upvotes: 1*
IAM role with CodeArtifact permissions: You need an IAM role attached to the EC2 instance (via instance profile) that grants permission to read from CodeArtifact.
aws codeartifact login sets up your Python environment (e.g., pip) to authenticate to the CodeArtifact repository using temporary credentials tied to the instance’s IAM role.
This is the recommended approach to grant secure and scalable access to CodeArtifact from EC2 instances.

---

**teo2157** (Wed 18 Dec 2024 16:35) - *Upvotes: 4*
Vote for D based on https://docs.aws.amazon.com/codeartifact/latest/ug/security-iam.html

---

**tinyshare** (Sun 08 Dec 2024 10:08) - *Upvotes: 3*
It is the resource allows who can use it, in this case, CodeArtifact.
https://docs.aws.amazon.com/codeartifact/latest/ug/repo-policies.html

---

**uncledana** (Tue 19 Nov 2024 11:00) - *Upvotes: 4*
D. Create an instance profile that contains an IAM role that has access to CodeArtifact. Associate the instance profile with the EC2 instance. Use the aws codeartifact login CLI command on the instance.

Explanation:

To allow the EC2 instance to access the CodeArtifact repository, the EC2 instance must have the necessary IAM permissions to interact with AWS CodeArtifact. Here’s why option D is the best solution:

---


<br/>

## Question 344

*Date: Nov. 19, 2024, 11:04 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has a file-reading application that saves files to a database that runs on Amazon EC2 instances. Regulations require the company to delete files from EC2 instances every day at a specific time. The company must delete database records that are older than 60 days.

The database record deletion must occur after the file deletions. The company has created scripts to delete files and database records. The company needs to receive an email notification for any failure of the deletion scripts.

Which solution will meet these requirements with the LEAST development effort?

**Options:**
- A. Use AWS Systems Manager State Manager to automatically invoke a Systems Manager Automation document at the specified time each day. Configure the Automation document to use a run command to run the deletion scripts in sequential order. Create an Amazon EventBridge rule to use Amazon Simple Notification Service (Amazon SNS) to send failure notifications to the company.
- B. Use AWS Systems Manager State Manager to automatically invoke a Systems Manager Automation document at the specified time each day. Configure the Automation document to use a run command to run the deletion scripts in sequential order. Create a conditional statement inside the Automation document as the last step to check for errors. Use Amazon Simple Email Service (Amazon SES) to send failure notifications as email messages to the company.
- C. Create an Amazon EventBridge rule that invokes an AWS Lambda function at the specified time. Add the necessary permissions for the invocation to the Lambda function's resource-based policy. Configure the Lambda function to run the deletion scripts in sequential order. Configure the Lambda function to use Amazon Simple Notification Service (Amazon SNS) to send failure notifications to the company.
- D. Create an Amazon EventBridge rule that invokes an AWS Lambda function at the specified time. Add the necessary permissions for the invocation to the Lambda function's resource-based policy. Configure the Lambda function to run the deletion scripts in sequential order. Configure the Lambda function to use Amazon Simple Email Service (Amazon SES) to send failure notifications as email messages to the company.

> **Suggested Answer:** A
> **Community Vote:** A (83%), B (17%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**uncledana** (Tue 19 Nov 2024 11:04) - *Upvotes: 7*
Option A provides the most efficient solution with the least development effort. It uses AWS Systems Manager Automation for orchestrating the execution of deletion scripts and uses Amazon SNS to handle failure notifications, which is simpler and more streamlined than using SES.

---

**Jonalb** (Thu 24 Jul 2025 10:38) - *Upvotes: 1*
Less operational efficiency
State Manager and Automation Documents already manage scheduling and execution without the need for custom code.

RunCommand executes commands directly on EC2 instances without an external agent.

EventBridge and SNS are already natively integrated with Automation Documents execution failures.

Reduced development effort: Uses managed and integrated services without writing code (unlike Lambda).

---

**Srikantha** (Sun 06 Apr 2025 16:58) - *Upvotes: 2*
Systems Manager State Manager: Can schedule tasks like running scripts on EC2 without having to manage cron jobs or Lambda triggers.
Automation Document with Run Command: Easily runs existing scripts on EC2 instances in sequence, fulfilling the requirement to delete files before database records.
Amazon SNS + EventBridge: Out-of-the-box integration to notify on Automation execution failures — no need to build error-handling logic yourself.

---

**tinyshare** (Sun 24 Nov 2024 13:13) - *Upvotes: 2*
A and B are about the same, but B has specifics to check errors by using the return values.

---

**GripZA** (Tue 22 Apr 2025 19:02) - *Upvotes: 1*
No, it uses SES not SNS.

---


<br/>

## Question 345

*Date: Nov. 19, 2024, 11:07 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses an organization in AWS Organizations that has all features enabled to manage its AWS accounts. Amazon EQ instances run in the AWS accounts.

The company requires that all current EC2 instances must use Instance Metadata Service Version 2 (IMDSv2). The company needs to block AWS API calls that originate from EC2 instances that do not use IMDSv2.

Which solution will meet these requirements?

**Options:**
- A. Create a new SCP statement that denies the ec2:RunInstances action when the ec2:MetadataHttpTokens condition key is not equal to the value of required. Attach the SCP to the root of the organization.
- B. Create a new SCP statement that denies the ec2:RunInstances action when the ec2:MetadataHttpPutResponseHopLimit condition key value is greater than two. Attach the SCP to the root of the organization.
- C. Create a new SCP statement that denies "*" when the ec2:RoleDelivery condition key value is less than two. Attach the SCP to the root of the organization.
- D. Create a new SCP statement that denies when the ec2:MetadataHttpTokens condition key value is not equal to required. Attach the SCP to the root of the organization.

> **Suggested Answer:** D
> **Community Vote:** D (56%), A (31%), 13%, Other, A (35%), C (25%), B (20%), Other

### Discussions

**Impromptu** (Sun 24 Nov 2024 14:08) - *Upvotes: 8*
I think it's D.
It must indeed use the ec2:MetadataHttpTokens condition key, but if we only deny the ec2:RunInstances, then the already running EC2 instances can still do AWS API calls. Even if they are not using IMDSv2.

---

**teo2157** (Thu 19 Dec 2024 10:08) - *Upvotes: 6*
Going for D, as A just enforce that the new EC2 instances to use IMDSv2 but there can be old instances not running IDMSv2 that can still do API calls...

---

**robotgeek** (Thu 15 May 2025 09:06) - *Upvotes: 4*
It is C. From the official docs, not from ChatGPT:
"Furthermore, you can choose an additional layer of protection to enforce the change from IMDSv1 to IMDSv2. At the access management layer with respect to the APIs called via EC2 Role credentials, you can use a new condition key in either IAM policies or AWS Organizations service control policies (SCPs). Specifically, by using the condition key ec2:RoleDelivery with a value of 2.0 in your IAM policies, API calls made with EC2 Role credentials obtained from IMDSv1 will receive an UnauthorizedOperation response."
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-metadata-transition-to-version-2.html

---

**Srikantha** (Sun 06 Apr 2025 17:01) - *Upvotes: 2*
Service Control Policies (SCPs) allow you to control which actions are allowed or denied across an entire organization or specific organizational units (OUs) in AWS Organizations.
The ec2:MetadataHttpTokens condition key is used to enforce IMDSv2. Setting the value of required ensures that the EC2 instances launched must use IMDSv2, as IMDSv1 would be denied.
By denying ec2:RunInstances when the IMDSv2 condition is not met, you are enforcing the policy for all EC2 instances launched, preventing the creation of instances without IMDSv2.

---

**DKM** (Wed 26 Mar 2025 14:54) - *Upvotes: 2*
This Service Control Policy (SCP) ensures that any attempt to launch EC2 instances without using IMDSv2 will be denied. By attaching this SCP to the root of the organization, it will apply to all accounts within the organization, ensuring compliance across the board.

Here is an example of the SCP statement:

{
"Version": "2012-10-17",
"Statement": [
{
"Effect": "Deny",
"Action": "ec2:RunInstances",
"Resource": "*",
"Condition": {
"StringNotEquals": {
"ec2:MetadataHttpTokens": "required"
}
}
}
]
}

---

**DKM** (Tue 25 Mar 2025 22:33) - *Upvotes: 2*
Here's why:

Service Control Policies (SCPs): SCPs allow you to set permission guardrails for all accounts in your organization. By creating an SCP that denies the ec2:RunInstances action when the ec2:MetadataHttpTokens condition key is not set to required, you ensure that only instances configured to use IMDSv2 can be launched1.
Condition Key: The ec2:MetadataHttpTokens condition key ensures that the instance metadata service requires the use of IMDSv21.
This approach enforces the use of IMDSv2 across all EC2 instances in the organization, enhancing security by preventing the use of the less secure IMDSv1.

---

**CHRIS12722222** (Sun 29 Dec 2024 17:31) - *Upvotes: 4*
Option A will prevent creating ec2 instances, allowing existing ones to violate policy

---

**uncledana** (Tue 19 Nov 2024 11:07) - *Upvotes: 4*
Option A provides the correct solution by using the ec2:MetadataHttpTokens condition key in an SCP to deny the ec2:RunInstances action for instances that do not have IMDSv2 enabled. This is the most effective way to ensure compliance with the company’s requirement.

---


<br/>

## Question 346

*Date: Nov. 19, 2024, 11:13 a.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps team supports an application that runs on a large number of Amazon EC2 instances in an Auto Scaling group. The DevOps team uses AWS CloudFormation to deploy the EC2 instances. The application recently experienced an issue. A single instance returned errors to a large percentage of requests. The EC2 instance responded as healthy to both Amazon EC2 and Elastic Load Balancing health checks.

The DevOps team collects application logs in Amazon CloudWatch by using the embedded metric format. The DevOps team needs to receive an alert if any EC2 instance is responsible for more than half of all errors.

Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)

**Options:**
- A. Create a CloudWatch Contributor Insights rule that groups logs from the CloudWatch application logs based on instance ID and errors.
- B. Create a resource group in AWS Resource Groups. Use the CloudFormation stack to group the resources for the application. Add the application to CloudWatch Application Insights. Use the resource group to identify the application.
- C. Create a metric filter for the application logs to count the occurrence of the term "Error.'' Create a CloudWatch alarm that uses the METRIC_COUNT function to determine whether errors have occurred. Configure the CloudWatch alarm to send a notification to an Amazon Simple Notification Service (Amazon SNS) topic to notify the DevOps team.
- D. Create a CloudWatch alarm that uses the INSIGHT_RULE_METRIC function to determine whether a specific instance is responsible for more than half of all errors reported by EC2 instances. Configure the CloudWatch alarm to send a notification to an Amazon Simple Notification Service (Amazon SNS) topic to notify the DevOps team.
- E. Create a CloudWatch subscription filter for the application logs that filters for errors and invokes an AWS Lambda function. Configure the Lambda function to send the instance ID and error and in a notification to an Amazon Simple Notification Service (Amazon SNS) topic to notify the DevOps team.

> **Suggested Answer:** AD
> **Community Vote:** AD (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**CHRIS12722222** (Sun 29 Dec 2024 17:47) - *Upvotes: 6*
contributor insight for top N

---

**uncledana** (Tue 19 Nov 2024 11:13) - *Upvotes: 5*
A and D provide a streamlined and automated way to track which EC2 instance is contributing to a high percentage of errors and send an alert when this threshold is crossed, with minimal manual intervention and operational overhead.

---

**Srikantha** (Sun 06 Apr 2025 17:03) - *Upvotes: 3*
CloudWatch Contributor Insights (A):
CloudWatch Contributor Insights can be used to analyze CloudWatch logs and aggregate them by custom dimensions, such as the instance ID in this case. This will allow you to identify which EC2 instance is contributing the most to errors in the application logs, which directly addresses the requirement of identifying if any EC2 instance is responsible for more than half of the errors.
CloudWatch Alarm with INSIGHT_RULE_METRIC (D):
After creating the Contributor Insights rule to track error patterns and group logs by instance, you can use the INSIGHT_RULE_METRIC function to create a CloudWatch alarm. This metric can monitor the Contributor Insights rule for instances that contribute disproportionately to errors (e.g., more than 50% of all errors).
You can configure the CloudWatch alarm to trigger notifications via Amazon SNS, alerting the DevOps team when the threshold is breached.

---


<br/>

## Question 347

*Date: Nov. 19, 2024, 11:17 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company is using AWS CloudFormation to perform deployments of its application environment. A deployment failed during a recent update to the existing CloudFormation stack. A DevOps engineer discovered that some resources in the stack were manually modified.

The DevOps engineer needs a solution that detects manual modification of resources and sends an alert to the DevOps lead.

Which solution will meet these requirements with the LEAST operational effort?

**Options:**
- A. Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the DevOps lead to the topic by using an email address. Create an AWS Config managed rule that has the CLOUDFORMATION_STACK_DRIFT_DETECTION_CHECK identifier. Create an Amazon EventBridge rule that is invoked on the NON_COMPLIANT resources status. Set the SNS topic as the rule target.
- B. Tag all CloudFormation resources with a specific tag. Create an AWS Config custom rule by using the AWS Config Rules Development Kit Library (RDKlib) that checks all resource changes that have the specific tag. Configure the custom rule to mark all the tagged resource changes as NON_COMPLIANT when the change is not performed by CloudFormation. Create an Amazon EventBridge rule that is invoked on the NON_COMPUANT resources status. Create an AWS Lambda function that sends an email message to the DevOps lead. Set the Lambda function as the rule target.
- C. Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the DevOps lead to the topic by using an email address. Create an AWS Config managed rule that has the CLOUDFORMATION_STACK_DRIFT_DETECTION_CHECK identifier. Create an Amazon EventBridge rule that is invoked on the COMPLIANT resources status. Set the SNS topic as the rule target.
- D. Create an AWS Config managed rule that has the CLOUDFORMATION_STACK_DRIFT_DETECTION_CHECK identifier. Create an Amazon EventBridge rule that is invoked on the NON_COMPLIANT resources status. Create an AWS Lambda function that sends an email message to the DevOps lead. Set the Lambda function as the rule target.

> **Suggested Answer:** A
> **Community Vote:** A (93%), 7%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Sun 06 Apr 2025 17:07) - *Upvotes: 2*
AWS Config Managed Rule (CLOUDFORMATION_STACK_DRIFT_DETECTION_CHECK):
The CLOUDFORMATION_STACK_DRIFT_DETECTION_CHECK is a built-in AWS Config rule that automatically detects drift on resources managed by CloudFormation. Drift refers to manual changes made to CloudFormation-managed resources, and this rule identifies such changes.
EventBridge Rule:
You can create an EventBridge rule that listens for NON_COMPLIANT events triggered by the AWS Config rule when drift is detected. This will ensure that whenever there are manual modifications on CloudFormation-managed resources, the event will be captured.
SNS Notification:
Using Amazon SNS, you can set up an email notification for the DevOps lead whenever the event is triggered. Subscribing the DevOps lead to the SNS topic ensures that they are immediately notified without requiring manual intervention.

---

**Ky_24** (Mon 16 Dec 2024 17:24) - *Upvotes: 4*
Key Requirements:

1. Detect manual modification of CloudFormation-managed resources.
2. Send an alert to the DevOps lead when such changes are detected.
3. Achieve this with minimal operational effort.

---

**luisfsm_111** (Wed 11 Dec 2024 13:33) - *Upvotes: 4*
Least operational overhead always will involve using AWS-Managed services instead of developing code, for example. So, A in my opinion.

---

**Impromptu** (Sun 24 Nov 2024 14:19) - *Upvotes: 4*
A is less complex by just using SNS for notifying, instead of creating a lambda function just to do that.

---

**uncledana** (Tue 19 Nov 2024 11:17) - *Upvotes: 1*
Option D is the most efficient and least operationally complex solution because it uses AWS Config’s drift detection rule, integrates with EventBridge for event handling, and leverages a Lambda function to send notifications. This approach directly addresses the need to detect manual changes in CloudFormation-managed resources and alert the DevOps lead.

---


<br/>

## Question 348

*Date: Nov. 19, 2024, 11:22 a.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer deployed multiple AWS accounts by using AWS Control Tower to support different business, technical, and administrative units in a company. A security team needs the DevOps engineer to automate AWS Control Tower guardrails for the company. The guardrails must be applied to all accounts in an OU of the company's organization in AWS Organizations.

The security team needs a solution that has version control and can be reviewed and rolled back if necessary. The security team will maintain the management of the solution in its OU. The security team wants to limit the type of guardrails that are allowed and allow only new guardrails that are approved by the security team.

Which solution will meet these requirements with the MOST operational efficiency?

**Options:**
- A. Create individual AWS CloudFormation templates that align to a guardrail. Store the templates in an AWS CodeCommit repository. Create an AWS::ControlTower::EnableControl logical resource in the template for each OU in the organization. Configure an AWS Code Build project that an Amazon EventBridge rule will invoke for the security team's AWS CodeCommit changes.
- B. Create individual AWS CloudFormation templates that align to a guardrail. Store the templates in an AWS CodeCommit repository. Create an AWS::ControlTower::EnableControl logical resource in the template for each account in the organization. Configure an AWS CodePipeline pipeline in the security team's account. Advise the security team to invoke the pipeline and provide these parameters when starting the pipeline.
- C. Create individual AWS CloudFormation templates that align to a guardrail. Store the templates in an AWS CodeCommit repository. Create an AWS::ControlTower::EnableControl logical resource in the template for each OU in the organization. Configure an AWS CodePipeline pipeline in the security team's account that an Amazon EventBridge rule will invoke for the security team's CodeCommit changes.
- D. Configure an AWS CodePipeline pipeline in the security team's account that an Amazon EventBridge rule will invoke for PutObject events to an Amazon S3 bucket. Create individual AWS CloudFormation templates that align to a guardrail. Store the templates in the S3 bucket. Create an AWS::ControlTower::EnableControl logical resource in the template for each OU in the organization.

> **Suggested Answer:** C
> **Community Vote:** C (82%), D (18%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**uncledana** (Tue 19 Nov 2024 11:22) - *Upvotes: 6*
Option C is the most efficient and scalable solution for automating AWS Control Tower guardrails while meeting the security team’s requirements for version control, approval, and rollback, with minimal operational overhead. It uses CodeCommit, CodePipeline, and EventBridge, leveraging the best AWS services for this purpose.

---

**Srikantha** (Sun 06 Apr 2025 17:08) - *Upvotes: 3*
Version control is managed easily with CodeCommit, and the changes to the guardrails can be reviewed and rolled back if necessary.
Approval and governance are built into the process, with the security team controlling the changes and ensuring that only approved guardrails are applied.
Automation through CodePipeline and EventBridge ensures that the guardrails are applied to the correct OUs automatically, without the need for manual processes or additional operational overhead.
The solution is scalable as it can be applied to multiple OUs and accounts.

---

**c87b433** (Fri 07 Feb 2025 12:17) - *Upvotes: 2*
D is not right because solution should be like AWS CodePipeline pipeline must be invooked by security team commits. But in D, PutObject events to an Amazon S3 bucket is used to invoke CodePipeline.
A is using AWS Code Build unnecesaarily on Amazon EventBridge rule. It does not say anything automated and involve manual efforts.
B is completely manual steps mentioned in the line so can't be efficient.

C is completely automated so its a right answer.

---


<br/>

## Question 349

*Date: Nov. 19, 2024, 11:24 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company runs a web application on Amazon Elastic Kubernetes Service (Amazon EKS). The company uses Amazon CloudFront to distribute the application. The company recently enabled AWS WAF. The company set up Amazon CloudWatch Logs to send logs to an aws-waf-logs log group.

The company wants a DevOps engineer to receive alerts if there are sudden changes in blocked traffic. The company does not want to receive alerts for other changes in AWS WAF log behavior. The company will tune AWS WAF rules over time.

The DevOps engineer is currently subscribed to an Amazon Simple Notification Service (Amazon SNS) topic in the environment.

Which solution will meet these requirements?

**Options:**
- A. Create a CloudWatch Logs metrics filter for blocked requests on the AWS WAF log group to create a custom metric. Create a CloudWatch alarm by using CloudWatch anomaly detection and the published custom metric. Configure the alarm to notify the SNS topic to alert the DevOps engineer.
- B. Create a CloudWatch anomaly detector for the log group. Create a CloudWatch alarm by using metrics that the CloudWatch anomaly detector publishes. Use the high setting for the LogAnomalyPriority metric. Configure the alarm to go into alarm state if a static threshold of one anomaly is detected. Configure the alarm to notify the SNS topic to alert the DevOps engineer.
- C. Create a CloudWatch metrics filter for counted requests on the AWS WAF log group to create a custom metric. Create a CloudWatch alarm that activates when the sum of blocked requests in the custom metric during a period of 1 hour is greater than a static estimate for the acceptable number of blocked requests in 1 hour. Configure the alarm to notify the SNS topic to alert the DevOps engineer.
- D. Create a CloudWatch anomaly detector for the log group. Create a CloudWatch alarm by using metrics that the CloudWatch anomaly detector publishes. Use the medium setting for the LogAnomalyPriority metric. Configure the alarm to go into alarm state if a sum of anomalies over 1 hour is greater than an expected value. Configure the alarm to notify the SNS topic to alert the DevOps engineer.

> **Suggested Answer:** A
> **Community Vote:** A (91%), 9%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Srikantha** (Sun 06 Apr 2025 17:10) - *Upvotes: 2*
Option A is the most efficient solution because it directly monitors blocked requests through a custom CloudWatch metric and uses CloudWatch anomaly detection to identify significant deviations, which is precisely what the company needs to monitor.

---

**CHRIS12722222** (Sun 29 Dec 2024 18:30) - *Upvotes: 4*
I go with option A
if we want to detect SUDDEN change in blocked request, we cant do so in 1hr period as that would be too long a time and what if the blocked request normalised quickly within that 1hr. I think using anomaly detection will provide some upper and lower limit and free us from defining and tuning a static threshold

---

**Slays** (Sat 28 Dec 2024 19:53) - *Upvotes: 1*
Unc the question said: "The company does not want to receive alerts for other changes in AWS WAF log behavior"

They only want notifications when blocked traffic increase, so anomaly detection doesn't fit the requirements.

Gotta be C

---

**uncledana** (Tue 19 Nov 2024 11:24) - *Upvotes: 4*
Option A provides the most precise and scalable solution to meet the company’s requirements. It focuses on blocked requests, uses anomaly detection for adaptive monitoring, and provides alerting through SNS when a sudden change in blocked traffic occurs.

---

**Slays** (Sat 28 Dec 2024 19:53) - *Upvotes: 1*
Unc the question said: "The company does not want to receive alerts for other changes in AWS WAF log behavior"

They only want notifications when blocked traffic increase, so anomaly detection doesn't fit the requirements.

Gotta be C

---


<br/>

## Question 350

*Date: Nov. 19, 2024, 11:35 a.m.
Disclaimers:
- ExamTopics website is not rel*

A video platform company is migrating its video catalog to AWS. The company will host MP4 videos files in an Amazon S3 bucket. The company will use Amazon CloudFront and Amazon EC2 instances to serve the video files.

Users first connect to a frontend application that redirects to a video URL. The video URL contains an authorization token in CloudFront. The cache is activated on the CloudFront distribution. Authorization token check activity needs to be logged in Amazon CloudWatch.

The company wants to prevent direct access to video files on CloudFront and Amazon S3 and wants to implement checks of the authorization token that the frontend application provides. The company also wants to perform regular rolling updates of the code that checks the authorization token signature.

Which solution will meet these requirements with the LEAST operational effort?

**Options:**
- A. Implement an authorization token check in Lambda@Edge as a trigger on the CloudFront distribution. Enable CloudWatch logging for the Lambda@Edge function. Attach the Lambda@Edge function to the CloudFront distribution. Implement CloudFront continuous deployment to perform updates.
- B. Implement an authorization token check in CloudFront Functions. Enable CloudWatch logging for the CloudFront function. Attach the CloudFront function to the CloudFront distribution. Implement CloudFront continuous deployment to perform updates.
- C. Implement an authorization token check in the application code that is installed on the EC2 instances. Install the CloudWatch agent on the EC2 instances. Configure the application to log to the CloudWatch agent. Implement a second CloudFront distribution. Migrate the traffic from the first CloudFront distribution by using Amazon Route 53 weighted routing.
- D. Implement an authorization token check in CloudFront Functions. Enable CloudWatch logging for the CloudFront function. Attach the CloudFront function to the CloudFront distribution. Implement a second CloudFront distribution. Migrate the traffic from the first CloudFront distribution by using Amazon Route 53 weighted routing.

> **Suggested Answer:** B
> **Community Vote:** B (62%), A (38%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**ryuhei** (Wed 20 Aug 2025 14:40) - *Upvotes: 1*
A is the correct choice:Option A is correct because:Lambda@Edge enables flexible authentication token checks at CloudFront's edge, preventing direct access to S3 and CloudFront.
CloudWatch logging is supported, allowing token check activities to be recorded.
Continuous deployment ensures smooth rolling updates with minimal operational effort.
Serverless architecture reduces infrastructure management, minimizing operational overhead.

Other options (B, C, D) fall short due to limitations in logging (B, D), increased operational complexity (C, D), or insufficient access control (C).

---

**nickp84** (Sat 24 May 2025 19:22) - *Upvotes: 2*
B. Use CloudFront Functions:
CloudFront Functions are lightweight and designed for simple request/response modifications (e.g., URL rewriting or header manipulation) but have limitations in runtime (Node.js or Python), memory, and execution time (less than 1 second). They are not well-suited for complex token signature validation (e.g., cryptographic operations for JWTs). Additionally, CloudFront Functions do not support direct CloudWatch logging; logging requires additional setup (e.g., via CloudFront access logs), increasing complexity. While continuous deployment is supported, the functional limitations make this less suitable.

---

**GripZA** (Tue 22 Apr 2025 19:40) - *Upvotes: 1*
CloudFront Functions over Lambda@Edge here since its integrated directly with CloudFront and CloudFront continuous deployment.

---

**Srikantha** (Sun 06 Apr 2025 17:13) - *Upvotes: 1*
Option B is the most efficient and operationally effective solution. It uses CloudFront Functions to check the authorization token directly at the edge, minimizing latency, and integrates seamlessly with CloudWatch for logging. Additionally, CloudFront continuous deployment simplifies updates, making it the optimal solution for the company’s requirements.

---

**matt200** (Mon 30 Dec 2024 15:27) - *Upvotes: 2*
Option A is correct. Here's why:
Lambda@Edge:

Perfect for token authorization checks
Supports CloudWatch logging
Can handle complex validation logic
Built for CloudFront integration
Rolling updates via continuous deployment

---

**f4b18ba** (Sat 23 Nov 2024 05:10) - *Upvotes: 3*
CloudFront Functions is a lightweight JavaScript-based environment that runs at the edge and is designed for high performance with low latency. It's ideal for simple tasks like authorization checks.
Enabling CloudWatch logging for CloudFront Functions ensures that the authorization token check activities are logged, providing visibility into the process.
Implementing CloudFront continuous deployment simplifies the process of rolling updates for the function, ensuring that new code can be deployed quickly and seamlessly.

---

**uncledana** (Tue 19 Nov 2024 11:35) - *Upvotes: 3*
Option B provides the most efficient solution with the least operational overhead. It uses CloudFront Functions for token validation, enables CloudWatch logging, and supports continuous deployment for easy updates, meeting the company’s requirements in a scalable and cost-effective manner.

---


<br/>

## Question 351

*Date: Nov. 19, 2024, 11:39 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses an organization in AWS Organizations to manage multiple AWS accounts in a hierarchical structure. An SCP that is associated with the organization root allows IAM users to be created.

A DevOps team must be able to create IAM users with any level of permissions. Developers must also be able to create IAM users. However, developers must not be able to grant new IAM users excessive permissions. The developers have the CreateAndManageUsers role in each account. The DevOps team must be able to prevent other users from creating IAM users.

Which combination of steps will meet these requirements? (Choose two.)

**Options:**
- A. Create an SCP in the organization to deny users the ability to create and modify IAM users. Attach the SCP to the root of the organization. Attach the CreateAndManageUsers role to developers.
- B. Create an SCP in the organization to grant users that have the DeveloperBoundary policy attached the ability to create new IAM users and to modify IAM users. Configure the SCP to require users to attach the PermissionBoundaries policy to any new IAM user. Attach the SCP to the root of the organization.
- C. Create an IAM permissions policy named PermissionBoundaries within each account. Configure the PermissionBoundaries policy to specify the maximum permissions that a developer can grant to a new IAM user.
- D. Create an IAM permissions policy named PermissionBoundaries within each account. Configure PermissionsBoundaries to allow users who have the PermissionBoundaries policy to create new IAM users.
- E. Create an IAM permissions policy named DeveloperBoundary within each account. Configure the DeveloperBoundary policy to allow developers to create IAM users and to assign policies to IAM users of only if the developer includes the PermissionBoundaries policy as the permissions boundary. Attach the DeveloperBoundary policy to the CreateAndManageUsers role within each account.

> **Suggested Answer:** CE
> **Community Vote:** CE (93%), 7%, B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Ky_24** (Mon 16 Dec 2024 17:34) - *Upvotes: 5*
1. IAM user creation:
• Both the DevOps team and developers should be able to create IAM users.
2. Permissions control:
• Developers should be restricted from granting excessive permissions to the IAM users they create.
3. Prevention of unauthorized IAM user creation:
• Only the designated roles (DevOps and developers) should create IAM users.

To achieve this, AWS Permissions Boundaries provide an effective way to enforce limits on the permissions that developers can assign

---

**Impromptu** (Sun 24 Nov 2024 14:50) - *Upvotes: 5*
A would prevent anyone to create IAM users, so both DevOps teams and Developers cannot create IAM users.
B would prevent DevOps team to create IAM users "with any level of permissions".
C would create the permission boundary that defines the maximum permissions of a user created by the Developers.
D does not work like that. The permission boundary would be used for preventing too many permissions on a user created by the Developers, and not for giving them user creation rights as well.
E would give the Developers the permissions to create users, but would force them to also attach the permission boundary (created in C) to the new user, limiting their permissions correctly (even if the Developer would give that user too many permissions)

---

**Jonalb** (Thu 24 Jul 2025 10:15) - *Upvotes: 1*
✅ C.

Create an IAM permissions policy named PermissionBoundaries within each account. Configure the PermissionBoundaries policy to specify the maximum permissions a developer can grant to a new IAM user.

✅ Reason: This clearly defines the permissions limits that developers can grant to users they create—exactly the expected behavior when using permissions boundaries.

✅ E.

Create an IAM permissions policy named DeveloperBoundary within each account. Configure the DeveloperBoundary policy to allow developers to create IAM users and assign policies to IAM users only if the developer includes the PermissionBoundaries policy as the permissions boundary. Attach the DeveloperBoundary policy to the CreateAndManageUsers role within each account.

✅ Reason: This step ensures that developers can only create users if they are using the boundary (PermissionBoundaries). This enforces the golden rule: you can only create users if you use the boundary.

---

**ArunRav** (Thu 28 Nov 2024 09:06) - *Upvotes: 3*
SCP in A denies the access to everyone but it doesnt explain the details about PermissionBoundaries policy used in C option. When you combine the C option with E option ie Creation of PermissionBoundaries policy to create the boundary and Creation of Developer boundary policy which allow developers to have access with boundaries mentioned in PermissionBoundaries make sense.
Hence CE

---

**uncledana** (Tue 19 Nov 2024 11:39) - *Upvotes: 1*
Option A provides the control at the organizational level to deny IAM user creation by non-DevOps users.
• Option E ensures that developers can create users with limited permissions by enforcing Permission Boundaries, ensuring they cannot assign excessive permissions.

This combination effectively meets the requirements with the least operational overhead.

---


<br/>

## Question 352

*Date: Nov. 19, 2024, 11:40 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has deployed a landing zone that has a well-defined AWS Organizations structure and an SCP. The company's development team can create their AWS resources only by using AWS CloudFormation and the AWS Cloud Development Kit (AWS CDK).

A DevOps engineer notices that Amazon Simple Queue Service (Amazon SQS) queues that are deployed in different CloudFormation stacks have different configurations. The DevOps engineer also notices that the application cost allocation tag is not always set.

The DevOps engineer needs a solution that will enforce tagging and promote the reuse of code. The DevOps engineer needs to avoid different configurations for the deployed SQS queues.

What should the DevOps engineer do to meet these requirements?

**Options:**
- A. Create an Organizations tag policy to enforce the cost allocation tag in CloudFormation stacks. Instruct the development team to use CloudFormation to define SQS queues. Instruct the development team to deploy the SQS queues by using CloudFormation StackSets.
- B. Update the SCP to enforce the cost allocation tag in CloudFormation stacks. Instruct the development team to use CloudFormation modules to define SQS queues. Instruct the development team to deploy the SQS queues by using CloudFormation stacks.
- C. Use AWS CDK tagging to enforce the cost allocation tag in CloudFormation StackSets. Instruct the development team to use the AWS CDK to define SQS queues. Instruct the development team to deploy the SQS queues by using CDK stacks.
- D. Use AWS CDK tagging to enforce the cost allocation tag in CloudFormation stacks. Instruct the development team to use the AWS CDK to define SQS queues. Instruct the development team to deploy the SQS queues by using CDK feature flags.

> **Suggested Answer:** B
> **Community Vote:** B (45%), C (35%), 10%, 10%, A (35%), C (25%), B (20%), Other

### Discussions

**sn61613** (Sun 08 Dec 2024 11:27) - *Upvotes: 6*
C:
Enforce tagging across all accounts via StackSets.
Use CDK Stacks to deploy same configuration SQS.

---

**ryuhei** (Wed 27 Aug 2025 04:27) - *Upvotes: 2*
I think the answer is A.

---

**BietTuot** (Sat 23 Aug 2025 08:21) - *Upvotes: 2*
B is correct: Implementing and enforcing tagging with SCP

" Where you enforce tagging with SCPs, documentation needs to be available to developers so they can ensure their resources meet the policies that have been applied to their accounts."

Source: https://docs.aws.amazon.com/whitepapers/latest/tagging-best-practices/implementing-and-enforcing-tagging.html

C is incorrect: CDK tagging alone is not enforcement (developers could still omit tags in non-CDK templates)

---

**robotgeek** (Thu 15 May 2025 08:01) - *Upvotes: 3*
Guys, the question is poorly redacted but clearly it is not C
- Option A: Within the Tag Policy itself, you can enable enforcement for specific resource types (so it would be the typical correct response if it was not for the "reuse" thing)

Source: https://docs.aws.amazon.com/whitepapers/latest/tagging-best-practices/implementing-and-enforcing-tagging.html

- Option B: "Instruct the development team to use CloudFormation modules", that improves "promote the reuse of code" while assuring enforcing

- Option C and D: BOTH options rely on programmers not forgetting to implement lines of codes to configure the tags, that is not how Devops and enforcement works guys.

---

**Srikantha** (Thu 10 Apr 2025 02:59) - *Upvotes: 1*
Tagging enforcement via CDK stack-level tagging.
Reusable constructs for consistent SQS configuration.
CDK feature flags to help with best practices and configuration enforcement.
All with the least operational overhead and maximum developer productivity.

---

**jojewi8143** (Sun 02 Feb 2025 20:09) - *Upvotes: 1*
Going with C

---

**teo2157** (Mon 20 Jan 2025 17:39) - *Upvotes: 2*
Going with B which enforce tagging using SCPs and promote the reuse of code using CF modules for the SQS.

---

**ArunRav** (Thu 28 Nov 2024 09:25) - *Upvotes: 2*
Though it is a straightforward solution...It doesn't give the level of enforcing with SCP has. Hence for restricting SCP and CFT to deploy.
Hence B

---

**uncledana** (Tue 19 Nov 2024 11:40) - *Upvotes: 1*
Option D provides a straightforward, flexible, and scalable solution by using AWS CDK for both tagging enforcement and reusing code, while also ensuring that developers can maintain standardization in their deployments with feature flags. This solution promotes best practices in terms of both infrastructure consistency and operational efficiency.

---


<br/>

## Question 353

*Date: Nov. 19, 2024, 11:42 a.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps team manages a company's AWS account. The company wants to ensure that specific AWS resource configuration changes are automatically reverted.

Which solution will meet this requirement?

**Options:**
- A. Use AWS Config rules to detect changes in resource configurations. Configure remediation action that uses AWS Systems Manager Automation documents to revert the configuration changes.
- B. Use Amazon CloudWatch alarms to monitor resource metrics. When an alarm is activated, use an Amazon Simple Notification Service (Amazon SNS) topic to notify an administrator to manually reverts the configuration changes.
- C. Use AWS CloudFormation to create a stack that deploys the necessary configuration changes. Update the stack when configuration changes need to be reverted.
- D. Use AWS Trusted Advisor to check for noncompliant configurations. Manually apply necessary changes based on Trusted Advisor recommendations.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**uncledana** (Tue 19 Nov 2024 11:42) - *Upvotes: 6*
The best solution is A, where AWS Config rules are used to monitor resource configuration changes, and remediation actions using AWS Systems Manager Automation documents are configured to automatically revert any non-compliant configuration changes. This provides an automated, scalable solution that meets the requirement to ensure that specific AWS resource configuration changes are automatically reverted.

---

**gbemimatti** (Sat 21 Jun 2025 00:52) - *Upvotes: 1*
AWS Systems Manager Automation & AWS Config

---


<br/>

## Question 354

*Date: April 22, 2025, 8:14 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company hosts an application in its AWS account. The application uses an Amazon S3 bucket to store objects that contain sensitive information.

The company needs to capture object-level S3 API calls, including calls that are rejected because the calls were made by using credentials that are not valid.

Which solution will meet these requirements?

**Options:**
- A. Create an AWS CloudTrail trail in the account. Enable S3 data events logging. Configure the trail to log to Amazon CloudWatch.
- B. Create a new S3 bucket. Configure access logging on the application's S3 bucket. Deliver the access logs to the new S3 bucket.
- C. Configure Amazon GuardDuty with S3 protection enabled for the account. Create an Amazon EventBridge rule that matches findings that are associated with the S3 bucket. Configure the rule to use an Amazon Simple Queue Service (Amazon SQS) queue as the target.
- D. Create an AWS CloudTrail trail and a new S3 bucket in the account. Configure the trail to log to the S3 new bucket.

> **Suggested Answer:** A
> **Community Vote:** A (80%), B (20%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Jonalb** (Mon 28 Jul 2025 18:31) - *Upvotes: 2*
To capture API calls at object level in the Amazon S3, including calls rejected by invalid credentials, you need to enable the "data events" in AWS CloudTrail.

Cloudtrail is the only service that records all API calls, including:

Putobject, GetObject, Deleteobject, etc.

Authorized and unauthorized calls (eg, AccessDenied error or invalidationSKeyid).

Date events are required to register object level operations in S3.

Sending logs to Cloudwatch allows for real -time alerts and queries (optional, but helps visibility).

---

**gbemimatti** (Sat 21 Jun 2025 00:58) - *Upvotes: 2*
Option A
S3 Access Logs are great for basic access auditing and billing analysis, but they do not capture denied requests or IAM-level details.

CloudTrail data events are the only service that logs both accepted and rejected S3 object-level API calls, fulfilling the security requirement.

---

**GripZA** (Tue 22 Apr 2025 20:14) - *Upvotes: 1*
Server access logging provides detailed records for the requests that are made to a bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. This information can also help you learn about your customer base and understand your Amazon S3 bill.

best practice to use a new S3 bucket: When your source bucket and destination bucket are the same bucket, additional logs are created for the logs that are written to the bucket, which creates an infinite loop of logs. We do not recommend doing this because it could result in a small increase in your storage billing.

https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerLogs.html

---


<br/>

## Question 355

*Date: April 22, 2025, 8:23 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps administrator is responsible for managing the security of a company's Amazon CloudWatch Logs log groups. The company's security policy states that employee IDs must not be visible in logs except by authorized personnel. Employee IDs follow the pattern of Emp-XXXXXX, where each X is a digit.

An audit discovered that employee IDs are found in a single log file. The log file is available to engineers, but the engineers are not authorized to view employee IDs. Engineers currently have an AWS IAM Identity Center permission that allows logs:* on all resources in the account.

The administrator must mask the employee ID so that new log entries that contain the employee ID are not visible to unauthorized personnel.

Which solution will meet these requirements with the MOST operational efficiency?

**Options:**
- A. Create a new data protection policy on the log group. Add an Emp-\d{6} custom data identifier configuration. Create an IAM policy that has a Deny action for the Action":"logs:Unmask" permission on the resource. Attach the policy to the engineering accounts.
- B. Create a new data protection policy on the log group. Add managed data identifiers for the personal data category. Create an IAM policy that has a Deny action for the "NotAction":"logs:Unmask" permission on the resource. Attach the policy to the engineering accounts.
- C. Create an AWS Lambda function to parse a log file entry, remove the employee ID, and write the results to a new log file. Create a Lambda subscription filter on the log group and select the Lambda function. Grant the lambda:InvokeFunction permission to the log group.
- D. Create an Amazon Data Firehose delivery stream that has an Amazon S3 bucket as the destination. Create a Firehose subscription filter on the log group that uses the Firehose delivery stream. Remove the "logs:*" permission on the engineering accounts. Create an Amazon Macie job on the S3 bucket that has an Emp-\d{6} custom identifier.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**GripZA** (Tue 22 Apr 2025 20:23) - *Upvotes: 2*
CloudWatch Logs uses data protection policies to select the sensitive data for which you want to scan, and the actions that you want to take to protect that data. To select the sensitive data of interest, you use data identifiers. To act upon data identifiers that are found, you can define audit and de-identify operations.
https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/cloudwatch-logs-data-protection-policies.html

Custom data identifiers (CDIs) let you define your own custom regular expressions that can be used in your data protection policy. Using custom data identifiers, you can target business-specific personally identifiable information (PII) use cases that managed data identifiers can't provide.
https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL-custom-data-identifiers.html

---


<br/>

## Question 356

*Date: April 22, 2025, 8:28 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses an organization in AWS Organizations to manage many AWS accounts. The company has enabled all features for the organization. The company uses AWS CloudFormation StackSets to deploy configurations to the accounts. The company uses AWS Config to monitor an Amazon S3 bucket.

The company needs to ensure that all object uploads to the S3 bucket use AWS Key Management Service (AWS KMS) encryption.

Which solution will meet these requirements?

**Options:**
- A. Create an AWS Config conformance pack that includes the s3-bucket-server-side-encryption-enabled rule. Deploy the conformance pack to the accounts. Configure the rule to target an Amazon Simple Notification Service (Amazon SNS) topic.
- B. Create an SCP that includes a deny statement for the s3:createBucket action and a condition statement where s3:x-amz-server-side-encryption is not aws:kms. Attach the SCP to the root of the organization.
- C. Create an AWS CloudFormation stack set to enable an AWS CloudTrail trail to capture S3 data events for the organization. In the stack set, create an Amazon EventBridge rule to match S3 PutObject events that do not use AWS KMS encryption. Configure the rule to target an Amazon Simple Notification Service (Amazon SNS) topic.
- D. Create an SCP that includes a deny statement for the s3:putObject action and a condition where s3:x-amz-server-side-encryption is not aws:kms. Attach the SCP to the root of the organization.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**GripZA** (Tue 22 Apr 2025 20:28) - *Upvotes: 2*
An SCP allows you to enforce guardrails across all accounts in an AWS Organization.
This policy:
Denies the action s3:PutObject
When the request does not specify aws:kms in s3:x-amz-server-side-encryption

This enforces encryption at upload time, and it blocks any upload attempts without KMS encryption.

---


<br/>

## Question 357

*Date: April 22, 2025, 8:34 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses an Amazon Aurora PostgreSQL DB cluster and loads transactional data into the database every 5 hours. Data analysts use the Aurora PostgreSQL database to run short-running queries, create complex aggregated queries, and create simple reports that use the data. The data analysts also manually update the data, including deleting and inserting data.

The data analysts have reported performance issues. The database team recently identified a long-running idle transaction connection that affected performance by blocking other queries and preventing VACUUM operations. The team wants to be proactively notified about these potential operational issues and about the recommended actions to fix the issues.

The company's AWS account uses Amazon DevOps Guru to monitor all the applications in the account.

Which solution will meet these requirements?

**Options:**
- A. Turn on Performance Insights and DevOps Guru in the existing Aurora PostgreSQL DB cluster. Configure DevOps Guru to send notifications to the database team by using Amazon Simple Notification Service (Amazon SNS).
- B. Turn on Performance Insights in the existing Aurora PostrgreSQL DB cluster. Configure Amazon EventBridge to receive events from the existing Aurora PostgreSQL DB cluster. Configure the Aurora PostgreSQL DB cluster to send notifications to the database team by using Amazon Simple Notification Service (Amazon SNS).
- C. Turn on Performance Insights and DevOps Guru in the existing Aurora PostgreSQL DB cluster. Configure the Aurora PostgreSQL DB cluster to send notifications to the database team by using Amazon Simple Email Service (Amazon SES).
- D. Turn on Performance Insights in the existing Aurora PostrgreSQL DB cluster. Configure Amazon EventBridge to receive events from the existing Aurora PostgreSQL DB cluster. Configure DevOps Guru to send notifications to the database team by using Amazon Simple Notification Service (Amazon SNS).

> **Suggested Answer:** A
> **Community Vote:** A (80%), D (20%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**gbemimatti** (Sat 21 Jun 2025 01:17) - *Upvotes: 2*
EventBridge not needed here

---

**AWSLoverLoverLoverLoverLover** (Mon 26 May 2025 08:41) - *Upvotes: 2*
DevOps Guru can detect anomalous behavior and operational issues and provide recommendations to fix them.

Since the company's AWS account already uses DevOps Guru, it’s best to enable it for the Aurora PostgreSQL DB cluster, so it can start monitoring it specifically.

Performance Insights provides additional visibility into database performance

Amazon SNS is a standard method for alerting or notifying teams when an operational issue is detected.

Not D, assumes DevOps Guru is already monitoring the Aurora cluster, but DevOps Guru must be explicitly enabled per resource, and D omits that step.

---

**GripZA** (Tue 22 Apr 2025 20:34) - *Upvotes: 1*
D over A, since the company's AWS account already uses Amazon DevOps Guru to monitor all the applications in the account.

---


<br/>

## Question 358

*Date: April 23, 2025, 4:15 a.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer is implementing governance controls for a company that requires its infrastructure to be housed within the United States. The company has many AWS accounts in an organization in AWS Organizations that has all features enabled.

The engineer must restrict which AWS Regions the company can use. The engineer must also ensure that an alert is sent as soon as possible if any activity outside the governance policy occurs. The controls must be automatically enabled on any new Region outside the United States.

Which combination of steps will meet these requirements? (Choose two.)

**Options:**
- A. Create an Organizations SCP deny policy that has a condition that the aws:RequestedRegion property does not match a list of all US Regions. Include an exception in the policy for global services. Attach the policy to the root of the organization.
- B. Configure AWS CloudTrail to send logs to Amazon CloudWatch Logs. Enable CloudTrail for all Regions. Use a CloudWatch Logs metric filter to create a metric in non-US Regions. Configure a CloudWatch alarm to send an alert if the metric is greater than 0.
- C. Use an AWS Lambda function that checks for AWS service activity. Deploy the Lambda function to all Regions. Write an Amazon EventBridge rule that runs the Lambda function every hour. Configure the rule to send an alert if the Lambda function finds any activity in a non-US Region.
- D. Use an AWS Lambda function to query Amazon Inspector to look for service activity in non-US Regions. Configure the Lambda function to send alerts if Amazon Inspector finds any activity.
- E. Create an Organizations SCP allow policy that has a condition that the aws:RequestedRegion property matches a list of all US Regions. Include an exception in the policy for global services. Attach the policy to the root of the organization.

> **Suggested Answer:** AB
> **Community Vote:** AB (67%), BE (33%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**dba9e3c** (Tue 02 Sep 2025 02:20) - *Upvotes: 1*
If a new region is added, wouldn’t the policy need to be updated, so wouldn’t option A be better?

---

**GripZA** (Wed 23 Apr 2025 04:15) - *Upvotes: 2*
Similar to Question #184

---


<br/>

## Question 359

*Date: April 23, 2025, 6:43 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company runs applications on Amazon EC2 instances that are in an Amazon EC2 Auto Scaling group. The EC2 instances are behind an Application Load Balancer (ALB). Users recently began to experience errors when traffic was directed to some of the EC2 instances.

A DevOps engineer discovers that the Auto Scaling group reports the problematic instances are healthy despite the application errors. User experience returns to normal after the DevOps engineer resolves the application errors on the problematic instances.

The company wants to ensure that traffic is routed only to healthy instances that are not experiencing application errors. The company also wants a support team to receive a notification if the traffic routing configuration changes.

Which solution will meet these requirements?

**Options:**
- A. Configure the Auto Scaling group to use ELB health checks. Enable AWS Config. Create an AWS Config rule to ensure that any new Auto Scaling group will use ELB health checks. Create an Amazon Simple Notification Service (Amazon SNS) topic to notify the support team if the traffic routing configuration changes. Configure the AWS Config rule to send a notification to the topic.
- B. Configure the Auto Scaling group to use EC2 health checks. Enable AWS Config. Create an AWS Config rule to ensure that any new Auto Scaling group will use EC2 health checks. Create an Amazon Simple Notification Service (Amazon SNS) topic to notify the support team if the traffic routing configuration changes. Configure the AWS Config rule to send a notification to the topic.
- C. Configure the Auto Scaling group to use EC2 health checks. Create an Amazon CloudWatch synthetic canary to monitor the application. Create a CloudWatch alarm that is triggered when the CloudWatch canary fails. Configure the alarm to notify the support team when the alarm state is in alarm.
- D. Configure the Auto Scaling group to use ELB health checks. Create an Amazon CloudWatch synthetic canary to monitor the application. Create a CloudWatch alarm that is triggered when the CloudWatch canary fails. Configure the alarm to notify the support team when the alarm state is in alarm.

> **Suggested Answer:** D
> **Community Vote:** D (55%), A (45%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**BietTuot** (Sat 23 Aug 2025 09:06) - *Upvotes: 3*
A is correct: you need to use AWS Config to provide governance controls that automatically detect if the health check configuration is changed or if new Auto Scaling groups are created without proper health check settings.

Also, use ELB Health Checks for Application-Level Health Detection

D is incorrect since it does not address the requirement for governance controls and notifications when traffic routing configurations change

---

**Adzz** (Sun 25 May 2025 14:32) - *Upvotes: 2*
option D

---

**AWSLoverLoverLoverLoverLover** (Thu 22 May 2025 16:27) - *Upvotes: 2*
Use CloudWatch Synthetics (canary) to simulate real user behavior (e.g., making HTTP requests to the app).

If the canary fails (e.g., due to application errors or routing problems), a CloudWatch alarm is triggered.

This alarm can send a notification via Amazon SNS to the support team.

D includes a canary and CloudWatch alarm with SNS notification — this provides deep visibility and alerts the support team.

---

**connorhoehn** (Thu 08 May 2025 17:04) - *Upvotes: 2*
This solution uses AWS Config to enforce ELB health checks in Auto Scaling groups, but this is not necessary.

A CloudWatch synthetic canary simulates user interactions with the application

---

**GripZA** (Wed 23 Apr 2025 06:43) - *Upvotes: 2*
use ALB as health check type in ASG settings, not EC2, for application errors.

While CloudWatch synthetic canaries can monitor application endpoints, they do not prevent traffic from being routed to unhealthy instances. So config over synthetic canary here.

---


<br/>

## Question 360

*Date: April 23, 2025, 6:59 a.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer needs to troubleshoot a pipeline that uses a GitHub code repository. The pipeline contains a source stage, a build stage, and a deploy stage. The pipeline also has an AWS CodeStar connection to the GitHub code repository.

The build stage uses an AWS CodeBuild build project. The build project needs to perform a git clone of the repository as part of the build process.

The DevOps engineer validates that the source stage is working properly. However, the build stage fails each time the pipeline runs.

What is the reason that the build stage fails in the pipeline?

**Options:**
- A. The build stage within the pipeline needs to use the AWS CodeStar connection action.
- B. The AWS CodeStar connection to GitHub contains incorrect credentials.
- C. The AWS CodePipeline service role does not have permission to use the AWS CodeStar connection.
- D. The AWS CodeBuild service role does not have permission to use the AWS CodeStar connection.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**gbemimatti** (Sat 21 Jun 2025 01:33) - *Upvotes: 2*
A. The build stage doesn't support a separate CodeStar action—it relies on CodeBuild having the correct permissions.

B. The source stage is working, so CodeStar credentials are correct.

C. The CodePipeline service role already can access CodeStar (since the Source stage works); the issue is within the CodeBuild role, not the pipeline role.

---

**GripZA** (Wed 23 Apr 2025 06:59) - *Upvotes: 2*
The initial pipeline run will fail because the CodeBuild service role must be updated with permissions to use connections. Add the codestar-connections:UseConnection IAM permission to your service role policy.

https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-github-gitclone.html

---


<br/>

## Question 361

*Date: Feb. 25, 2025, 6:30 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company's DevOps team uses Node Package Manager (NPM) open source libraries to build applications. The DevOps team runs its application build process in an AWS CodeBuild project that downloads the NPM libraries from public NPM repositories.

The company wants to host the NPM libraries in private NPM repositories. The company also needs to be able to run checks on new versions of the libraries before the DevOps team uses the libraries.

Which solution will meet these requirements with the LEAST operational effort?

**Options:**
- A. Create an AWS CodeArtifact repository with an upstream repository named npm-store. Configure the application build process to use the CodeArtifact repository as the default source for NPM. Create an AWS CodePipeline pipeline to perform the required checks on package versions in the CodeArtifact repository. Set the package status to unlisted if a failure occurs.
- B. Enable Amazon S3 caching in the CodeBuild project configuration. Add a step in the buildspec.yaml config file to perform the required checks on the package versions in the cache.
- C. Create an AWS CodeCommit repository for each library. Clone the required NPM libraries to the appropriate CodeCommit repository. Modify the CodeBuild appspec.yaml config file to use the private CodeCommit repositories. Add a step to perform the required checks on the package versions.
- D. Create an AWS CodeCommit repository for each library. Clone the required NPM libraries to the appropriate CodeCommit repository. Modify the CodeBuild buildspec.yaml config file so that NPM uses the private CodeCommit repositories. Add an AWS CodePipeline pipeline that performs the required checks on the package versions for each new commit to the repositories. Configure the pipeline to revert to the most recent commit in the event of a failure.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**GripZA** (Wed 23 Apr 2025 07:07) - *Upvotes: 2*
CodeArtifact stores software packages in repositories.
You can create a connection between your private CodeArtifact repository and an external, public repository, such as npmjs.com. CodeArtifact will then fetch and store packages on demand from the public repository when they're requested by a package manager.

https://docs.aws.amazon.com/codeartifact/latest/ug/welcome.html

---

**Bwhizzy** (Tue 25 Feb 2025 18:30) - *Upvotes: 2*
CodeArtifact with Upstream Repository

Proxy Public Packages: The npm-store upstream repository acts as a proxy for npm.org, automatically caching public packages in CodeArtifact13.

Centralized Control: The build process uses CodeArtifact as the default NPM source, ensuring all dependencies are managed privately.

CodePipeline for Version Checks

Automated Validation: The pipeline runs tests or checks on new package versions before they’re approved for use.

Unlisted Status: Failed packages are marked as unlisted, preventing their inclusion in builds until resolved16.

Why Other Options Fail:

B/C/D: Cloning libraries into CodeCommit (C/D) or using S3 caching (B) adds manual effort and complexity compared to CodeArtifact’s native proxying.

No Need for Manual Cloning: CodeArtifact’s upstream eliminates the need to clone public packages into private repositories

---


<br/>

## Question 362

*Date: April 23, 2025, 7:18 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has a search application that has a web interface. The company uses Amazon CloudFront, Application Load Balancers (ALBs), and Amazon EC2 instances in an Auto Scaling group with a desired capacity of 3. The company uses prebaked AMIs. The application starts in 1 minute. The application queries an Amazon OpenSearch Service cluster.

The application is deployed to multiple Availability Zones. Because of compliance requirements, the application needs to have a disaster recovery (DR) environment in a separate AWS Region. The company wants to minimize the ongoing cost of the DR environment and requires an RTO and an RPO of under 30 minutes. The company has created an ALB in the DR Region.

Which solution will meet these requirements?

**Options:**
- A. Add the new ALB as an origin in the CloudFront distribution. Configure origin failover functionality. Copy the AMI to the DR Region. Create a launch template and an Auto Scaling group with a desired capacity of 0 in the DR Region. Create a new OpenSearch Service cluster in the DR Region. Set up cross-cluster replication for the cluster.
- B. Create a new CloudFront distribution in the DR Region and add the new ALB as an origin. Use Amazon Route 53 DNS for Regional failover. Copy the AMI to the DR Region. Create a launch template and an Auto Scaling group with a desired capacity of 0 in the DR Region. Reconfigure the OpenSearch Service cluster as a Multi-AZ with Standby deployment. Ensure that the standby nodes are in the DR Region.
- C. Create a new CloudFront distribution in the DR Region and add the new ALB as an origin. Use Amazon Route 53 DNS for Regional failover. Copy the AMI to the DR Region. Create a launch template and an Auto Scaling group with a desired capacity of 3 in the DR Region. Reconfigure the OpenSearch Service cluster as a Multi-AZ with Standby deployment. Ensure that the standby nodes are in the DR Region.
- D. Add the new ALB as an origin in the CloudFront distribution. Configure origin failover functionality. Copy the AMI to the DR Region. Create a launch template and an Auto Scaling group with a desired capacity of 3 in the DR Region. Create a new OpenSearch Service cluster in the DR Region. Set up cross-cluster replication for the cluster.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**0ac7838** (Sun 16 Nov 2025 14:10) - *Upvotes: 1*
A Cloudfront is global so no need for regional one as mentioned in B

---

**GripZA** (Wed 23 Apr 2025 07:18) - *Upvotes: 3*
active-passive strategy with desired capacity of 0. 1min application start time gives enough time to increase desired count when you failover to DR region.

OpenSearch supports cross-cluster replication for DR scenarios. "With cross-cluster replication in Amazon OpenSearch Service, you can replicate user indexes, mappings, and metadata from one OpenSearch Service domain to another. Using cross-cluster replication helps to ensure disaster recovery if there is an outage, and allows you to replicate data across geographically distant data centers to reduce latency."

https://docs.aws.amazon.com/opensearch-service/latest/developerguide/replication.html

not B or C since DR env is in a separate region. OpenSearch multi-AZ won't help.

not D since it will have running 3 instances with associated runtime costs.

---


<br/>

## Question 363

*Date: March 14, 2025, 9:20 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer uses AWS WAF to manage web ACLs across an AWS account. The DevOps engineer must ensure that AWS WAF is enabled for all Application Load Balancers (ALBs) in the account. The DevOps engineer uses an AWS CloudFormation template to deploy an individual ALB and AWS WAF as part of each application stack's deployment process. If AWS WAF is removed from the ALB after the ALB is deployed, AWS WAF must be added to the ALB automatically.

Which solution will meet these requirements with the MOST operational efficiency?

**Options:**
- A. Enable AWS Config. Add the alb-waf-enabled managed rule. Create an AWS Systems Manager Automation document to add AWS WAF to an ALB. Edit the rule to automatically remediate. Select the Systems Manager Automation document as the remediation action.
- B. Enable AWS Config. Add the alb-waf-enabled managed rule. Create an Amazon EventBridge rule to send all AWS Config ConfigurationItemChangeNotification notification types to an AWS Lambda function. Configure the Lambda function to call the AWS Config start-resource-evaluation API in detective mode.
- C. Configure an Amazon EventBridge rule to periodically call an AWS Lambda function that calls the detect-stack-drift API on the CloudFormation template. Configure the Lambda function to modify the ALB attributes with waf.fail_open.enabled set to true if the AWS::WAFv2::WebACLAssociation resource shows a status of drifted.
- D. Configure an Amazon EventBridge rule to periodically call an AWS Lambda function that calls the detect-stack-drift API on the CloudFormation template. Configure the Lambda function to delete and redeploy the CloudFormation stack if the AWS::WAFv2::WebACLAssociation resource shows a status of drifted.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Jonalb** (Thu 24 Jul 2025 10:22) - *Upvotes: 3*
AWS Config can detect non-compliance, such as an ALB without a WAF.

The alb-waf-enabled managed rule checks for exactly that: whether an AWS WAF ACL is associated with the ALB.

By configuring an automatic remediation action with Systems Manager Automation, you can automatically reattach the Web ACL to the ALB as soon as non-compliance is detected.

This approach is 100% automated, based on managed services, with no scripting or manual detection required.

---

**GripZA** (Wed 23 Apr 2025 07:27) - *Upvotes: 3*
elb-waf-enabled managed rule checks if AWS WAF is enabled on ALBs
Systems Manager Automation provides predefined runbooks for Elastic Load Balancing.

---

**2b80c69** (Fri 14 Mar 2025 21:20) - *Upvotes: 2*
AWS Config has elb-waf-enabled managed rule which solves the problem in a most operationally efficient manner.
Not B coz, it focus more on sending notifications which is not the purpose of the question
Not C & D coz, it has drift detection, which is again not the purpose of the question

---


<br/>

## Question 364

*Date: Aug. 7, 2025, 4:56 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is developing an ecommerce application on AWS. The company wants to make sure that the application can handle sudden increases in traffic.

The company uses AWS CodePipeline for its CI/CD process. The company must implement a solution to integrate automated load testing into the CI/CD pipeline to validate the application's performance. The solution must perform production deployment only if the performance exceeds a threshold.

Which solution will meet these requirements with the LEAST operational overhead?

**Options:**
- A. Deploy the application by using AWS Elastic Beanstalk. Enable load balancing. Use Elastic Beanstalk to deploy tools for load tests. Run the tests during each deployment, and roll back the deployment if performance thresholds are unmet. Create an AWS Lambda function to monitor test metrics. Set up alarms for performance thresholds. Configure Amazon EventBridge to return an error if a test fails and to proceed with production deployment if a test passes.
- B. Implement AWS Fargate tasks to run tools for load tests. Use Amazon Elastic container Service (Amazon ECS) to manage the test containers. Create AWS Lambda functions to analyze the test results. Integrate the functions with CodePipeline by using custom actions to initiate and evaluate the tests. Program the functions to return an error if a test fails and to proceed with production deployment if a test passes.
- C. Launch Amazon EC2 instances to run tools for load tests. Store test scripts in a GitHub repository. Use AWS Step Functions to orchestrate the tests and result analysis in the CodePipeline workflow. Use Amazon EventBridge to invoke an AWS Lambda function based on the test results. Program the function to return an error if a test fails and to proceed with production deployment if a test passes.
- D. Use AWS CodeBuild to run tools for load tests, store the test artifacts in Amazon S3, and configure a CodePipeline stage to invoke the CodeBuild project. Use Amazon CloudWatch to monitor the test metrics and to set up alarms for performance thresholds. Integrate an AWS Lambda function into the pipeline by using a custom action. Program the function to return an error if a test fails and to proceed with production deployment if a test passes.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**eesa** (Fri 22 Aug 2025 19:39) - *Upvotes: 2*
Use AWS CodeBuild as a testing stage in CodePipeline to run your load tests (e.g., JMeter/Locust), produce artifacts to S3, and gate the pipeline on the results. CodeBuild is fully managed and integrates natively with CodePipeline, so you avoid managing EC2/ECS/Beanstalk fleets or extra orchestration. A simple Lambda/custom action (or even the build’s exit code) can fail the stage if thresholds aren’t met, preventing production deploys.

---

**ryuhei** (Thu 21 Aug 2025 01:34) - *Upvotes: 1*
The solution with the least operational overhead is **Option D: Using AWS CodeBuild**. It leverages a serverless, scalable build environment with seamless integration into AWS CodePipeline, minimal infrastructure management, and straightforward monitoring using Amazon CloudWatch and S3 for test

---


<br/>

## Question 365

*Date: Aug. 7, 2025, 4:56 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company needs to update its order processing application to improve resilience and availability. The application requires a stateful database and uses a single-node Amazon RDS DB instance to store customer orders and transaction history. A DevOps engineer must make the database highly available.

Which solution will meet this requirement?

**Options:**
- A. Migrate the database to Amazon DynamoDB global tables. Configure automatic failover between AWS Regions by using Amazon Route 53 health checks.
- B. Migrate the database to Amazon EC2 instances in multiple Availability Zones. Use Amazon Elastic Block Store (Amazon EBS) Multi-Attach to connect all the instances to a single EBS volume.
- C. Use the RDS DB instance as the source instance to create read replicas in multiple Availability Zones. Deploy an Application Load Balancer to distribute read traffic across the read replicas.
- D. Modify the RDS DB instance to be a Multi-AZ deployment. Verify automatic failover to the standby instance if the primary instance becomes unavailable.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**ryuhei** (Wed 27 Aug 2025 05:24) - *Upvotes: 1*
The answer is D.
Achieve high availability for an order processing application that uses a stateful database.
Improve fault tolerance and availability.
Maintain operational efficiency by leveraging your existing Amazon RDS deployment.

---


<br/>

## Question 366

*Date: Aug. 7, 2025, 4:56 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has application code in an AWS CodeConnections compatible Git repository. The company wants to configure unit tests to run when pull requests are opened. The company wants to ensure that the test status is visible in pull requests when the tests are completed. The company wants to save output data files that the tests generate to an Amazon S3 bucket after the tests are finished.

Which combination of solutions will meet these requirements? (Choose three.)

**Options:**
- A. Create an IAM service role to allow access to the resources that are required to run the tests.
B Create a pipeline in AWS CodePipeline that has a test stage. Create a trigger to run the pipeline when pull requests are created or updated. Add a source action to report test results.
- C. Create an AWS CodeBuild project to run the tests. Enable webhook triggers to run the tests when pull requests are created or updated. Enable build status reporting to report test results.
- D. Create a buildspec.yml file that has a reports section to upload output files when the tests have finished running.
- E. Create a buildspec.yml file that has an artifacts section to upload artifacts when the tests have finished running.
- F. Create an appspec.yml file that has a files section to upload output files when the tests have finished running.

> **Suggested Answer:** ACE
> **Community Vote:** ACD (50%), ACE (50%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**2ae4ed9** (Sat 20 Sep 2025 22:10) - *Upvotes: 1*
DEFINITELY

---

**ryuhei** (Thu 21 Aug 2025 01:45) - *Upvotes: 1*
Thank you! Here is the conclusion translated into English with line breaks:

**Conclusion**:
The correct choices are **A, C, E**.

These options together meet all the requirements:
- An IAM service role (A) ensures the necessary permissions.
- A CodeBuild project (C) runs tests on pull requests and displays test status.
- The artifacts section in the buildspec.yml (E) uploads test output files to S3.

---

**ryuhei** (Thu 21 Aug 2025 01:46) - *Upvotes: 1*
Grok answer

---


<br/>

## Question 367

*Date: Aug. 7, 2025, 4:56 p.m.
Disclaimers:
- ExamTopics website is not rel*

A software development team is implementing a CI/CD pipeline for its web application. The team is using AWS CodeBuild to compile Java-based source code and to run unit tests.

The team needs to store the artifacts that are created by the CodeBuild project.

Which solution will meet this requirement?

**Options:**
- A. Create an Amazon S3 bucket. Configure the S3 bucket as an artifact output location in the project. Add the artifact locations to the project's buildspec file. Configure an S3 bucket policy that allows the CodeBuild project's resource access role to access the S3 bucket.
- B. Create an Amazon S3 bucket. Configure the S3 bucket as an artifact output location in the project's buildspec file. Add the artifact locations to the project's buildspec file.
Configure an S3 bucket policy that allows the CodeBuild service role to access the S3 bucket.
- C. Configure an Amazon Elastic File System (Amazon EFS) file system as a file system location for the project. Configure the EFS file system as the artifact output location in the project's buildspec file. Configure a file system policy that allows CodeBuild to access the file system.
- D. Create an Amazon Elastic Block Store (Amazon EBS) volume. Configure the EBS volume as the artifact output location in the project's buildspec file. Configure an IAM role that allows CodeBuild to access the volume.

> **Suggested Answer:** B
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**BietTuot** (Fri 29 Aug 2025 14:49) - *Upvotes: 1*
A is correct: Artifact output location is configured in the Codebuild project level not in the Buidspec file so B is incorrect

C, D incorrect because Codebuild doesn't support EFS and EBS.

---

**ryuhei** (Thu 21 Aug 2025 01:51) - *Upvotes: 1*
answer is a

---


<br/>

## Question 368

*Date: Aug. 7, 2025, 4:56 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses AWS Organizations to manage multiple AWS accounts. The accounts are in an OU that has a policy attached to allow all actions. The company is migrating several Git repositories to a specified AWS CodeConnections supported Git provider. The Git repositories manage AWS CloudFormation stacks for application infrastructure that the company deploys across multiple AWS Regions.

The company wants a DevOps team to integrate CodeConnections into the CloudFormation stacks. The DevOps team must ensure that company staff members can integrate only with the specified Git provider. The deployment process must be highly available across Regions.

Which combination of steps will meet these requirements? (Choose three.)

**Options:**
- A. Add a new SCP statement to the OU that denies the CodeConnections CreatingConnections action where the provider type is not the specified Git provider.
- B. Add a new SCP statement to the OU that allows the CodeConnections CreatingConnections action where the provider type is the specified Git provider.
- C. Use CodeConnectlons to configure a single CodeConnections connection to each Git repository.
- D. Use CodeConnections to create a CodeConnections connection from each Region where the company operates to each Git repository.
- E. Use CodeConnections to create a CodeConnections repository link. Update each CfoudFormation stack to sync from the Git repository.
- F. For each Git repository, create a pipeline in AWS CodePipefine that has the Git repository set as the source and a CloudFormation deployment stage.

> **Suggested Answer:** ADE
> **Community Vote:** ABD (50%), ACF (50%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**ryuhei** (Thu 21 Aug 2025 02:05) - *Upvotes: 1*
Correct choices: A, C, F

If you need further clarification or examples (e.g., CodePipeline configuration or CloudFormation StackSets setup), please let me know!

---

**ryuhei** (Thu 21 Aug 2025 02:06) - *Upvotes: 1*
Grok answer

---

**GiorgioGss** (Sat 16 Aug 2025 23:58) - *Upvotes: 1*
Step 1: Restrict access to the Git provider

To ensure staff cannot create connections to any other Git provider, we need an SCP that denies connections to any provider other than the specified one. ✅

Option A ✅

Step 2: Allow connections to the specified provider

We also need to explicitly allow creating connections for the specified provider. ✅

Option B ✅

Step 3: Create the connections in a highly available way

CodeConnections connections are regional resources.

To make deployments highly available across Regions, you must create a connection in each Region where deployments occur. ✅

Option D ✅

---


<br/>

## Question 369

*Date: Aug. 7, 2025, 4:56 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer updates an AWS CloudFormation stack to add a nested stack that includes several Amazon EC2 instances. When the DevOps engineer attempts to deploy the updated stack, the nested stack fails to deploy.

What should the DevOps engineer do to determine the cause of the failure?

**Options:**
- A. Use the CloudFormation detect root cause capability for the failed stack to analyze the failure and return the event that is the most likely cause for the failure.
- B. Query failed stacks by specifying the root stack as the ParentId property. Examine the StackStatusReason property for all returned stacks to determine the reason the nested stack failed to deploy.
- C. Activate AWS Systems Manager for the AWS account where the application runs. Use the AWS Systems Manager Automation AWS-SupportTroubleshootCFNCustomResource runbook to determine the reason the nested stack failed to deploy.
- D. Configure the CloudFormation template to publish logs to Amazon CloudWatch. View the CloudFormation logs for the failed stack in the CloudWatch console to determine the reason the nested stack failed to deploy.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**ryuhei** (Thu 21 Aug 2025 02:12) - *Upvotes: 1*
Answer is B
When a nested stack deployment fails in AWS CloudFormation, the failure reason is recorded in the StackStatusReason property of the failed stack’s events. By querying the failed stack and specifying the ParentId property (which links the nested stack to its root stack), the DevOps engineer can trace the relationship between the root stack and the nested stack. Examining the StackStatusReason values allows them to determine exactly why the nested stack failed to deploy.

---

**GiorgioGss** (Sun 17 Aug 2025 00:01) - *Upvotes: 2*
When a nested CloudFormation stack fails, the failure information is recorded in the StackStatusReason of the nested stack itself. To diagnose the failure:

Go to the AWS CloudFormation console.

Locate the nested stack under the parent stack.

Examine its StackStatusReason or the events tab to see exactly why it failed (for example, missing IAM permissions, invalid parameters, or resource creation errors).

---


<br/>

## Question 370

*Date: Aug. 7, 2025, 4:56 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps team operates an integration service that runs on an Amazon EC2 instance. The DevOps team uses Amazon Route 53 to manage the integration service's domain name by using a simple routing record. The integration service is stateful and uses Amazon Elastic File System (Amazon EFS) for data storage and state storage. The integration service does not support load balancing between multiple nodes.

The DevOps team deploys the integration service on a new EC2 instance as a warm standby to reduce the mean time to recovery. The DevOps team wants the integration service to automatically fail over to the standby EC2 instance.

Which solution will meet these requirements?

**Options:**
- A. Update the existing Route 53 DNS record's routing policy to weighted. Set the existing DNS record's weighting to 100. For the same domain, add a new DNS record that points to the standby EC2 instance. Set the new DNS record's weighting to 0. Associate an application health check with each record.
- B. Update the existing Route 53 DNS record's routing policy to weighted. Set the existing DNS record's weighting to 99. For the same domain, add a new DNS record that points to the standby EC2 instance. Set the new DNS record's weighting to 1. Associate an application health check with each record.
- C. Create an Application Load Balancer (ALB). Update the existing Route 53 record to point to the ALB. Create a target group for each EC2 instance. Configure an application health check on each target group. Associate both target groups with the same ALB listener. Set the primary target group's weighting to 100. Set the standby target group's weighting to 0.
- D. Create an Application Load Balancer (ALB). Update the existing Route 53 record to point to the ALB. Create a target group for each EC2 instance. Configure an application health check on each target group. Associate both target groups with the same ALB listener. Set the primary target group's weighting to 99. Set the standby target group's weighting to 1.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**ryuhei** (Thu 21 Aug 2025 02:16) - *Upvotes: 1*
Correct Answer: A
Explanation:
The application is stateful and does not support load balancing across multiple nodes, so using an Application Load Balancer (options C and D) is not appropriate. The requirement is to have a warm standby EC2 instance that automatically takes over when the primary instance fails.
With Amazon Route 53, a weighted routing policy combined with health checks can achieve this. By assigning a weight of 100 to the primary EC2 instance record and a weight of 0 to the standby EC2 instance record, Route 53 will route all traffic to the primary instance under normal conditions. If the health check for the primary instance fails, Route 53 will automatically stop routing traffic to the primary record and will start routing traffic to the standby record.
Option B (weights 99 and 1) would result in some production traffic being sent to the standby instance, which is not the desired behavior.

---

**GiorgioGss** (Sun 17 Aug 2025 00:04) - *Upvotes: 1*
Weighted routing in Route 53 with weights 100 (primary) and 0 (standby) ensures that all traffic goes to the primary instance under normal conditions.

Health checks monitor the primary instance. If the health check fails, Route 53 automatically fails over to the standby instance.

This matches the requirement of having a warm standby that can automatically take over when the primary fails.

---


<br/>

## Question 371

*Date: Aug. 7, 2025, 4:55 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company hosts several applications in an AWS account. Each application sends logs to its own log group in Amazon CloudWatch. The company's CloudWatch costs for ingestion are increasing.

A DevOps engineer needs to identify which applications are the source of the increased logging costs.

Which solution will meet this requirement in the MOST operationally efficient way?

**Options:**
- A. Use CloudWatch metrics to create a custom expression that identifies the CloudWatch log groups that receive the most data.
- B. Use Amazon CloudWatch Logs Insights to create a query for the application log groups to identify the number of log groups that received data during a specific time period.
- C. Use AWS Cost Explorer to generate a cost report that details costs for CloudWatch usage.
- D. Use AWS CloudTrail to filter for CreateLogStream events for each application.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**ryuhei** (Thu 21 Aug 2025 02:19) - *Upvotes: 1*
Answer is A

---

**GiorgioGss** (Sun 17 Aug 2025 00:08) - *Upvotes: 2*
CloudWatch Metrics are the direct source of truth for ingestion volume: Amazon CloudWatch Logs automatically publishes a metric called IncomingBytes for each log group. This metric directly measures the uncompressed data volume ingested into the log group. Since log ingestion cost is based on the volume of data in GB, this metric is a direct indicator of cost.

---


<br/>

## Question 372

*Date: Aug. 7, 2025, 4:56 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an AWS account named PipelineAccount. The account manages a pipeline in AWS CodePipeline. The account uses an IAM role named CodePipeline_Service_Role and produces an artifact that is stored in an Amazon S3 bucket. The company uses a customer managed AWS KMS key to encrypt objects in the S3 bucket.

A DevOps engineer wants to configure the pipeline to use an AWS CodeDeploy application in an AWS account named CodeDeployAccount to deploy the produced artifact.

The DevOps engineer updates the KMS key policy to grant the CodeDeployAccount account permission to use the key. The DevOps engineer configures an IAM role named DevOps_Role in the CodeDeployAccount account that has access to the CodeDeploy resources that the pipeline requires. The DevOps engineer updates an Amazon EC2 instance role that operates within the CodeDeployAccount account to allow access to the S3 bucket and the KMS key that is in the PipelineAccount account.

Which additional steps will meet these requirements?

**Options:**
- A. Update the S3 bucket policy to grant the CodeDeployAccount account access to the S3 bucket. Configure the DevOps_Role IAM role to have an IAM trust policy that allows the PipelineAccount account to assume the role. Update the CodePipeline_Service_Role IAM role to grant permission to assume the DevOps_Role role.
- B. Update the S3 bucket policy to grant the CodeDeployAccount account access to the S3 bucket. Configure the DevOps_Role IAM role to have an IAM trust policy that allows the PipelineAccount account to assume the role. Update the DevOps_Role IAM role to grant permission to assume CodePipelfne_Service_Role role.
- C. Update the S3 bucket policy to grant the PipelineAccount account access to the S3 bucket. Configure the DevOps_Role IAM role to have an IAM trust policy that allows the PipelineAccount account to assume the role. Update the CodePipeline_Service_Role IAM to grant permission to assume the DevOps_Role role.
- D. Update the S3 bucket policy to grant the CodeDeployAccount account access to the S3 bucket. Configure the DevOps_Role IAM role to have an IAM trust policy that allows the CodeDeployAccount account to assume the role. Update the CodePipeline_Service_Role IAM role to grant permission to assume the DevOps_Role role.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**ryuhei** (Thu 21 Aug 2025 02:24) - *Upvotes: 1*
Answer is A

---

**GiorgioGss** (Sun 17 Aug 2025 00:14) - *Upvotes: 1*
Some steps are already done:

KMS key policy updated to allow CodeDeployAccount access. ✅

EC2 instance role updated to allow access to the S3 bucket and KMS key. ✅

Missing Steps:

S3 bucket policy must allow the CodeDeployAccount to read the artifact.

Cross-account role assumption:

DevOps_Role in CodeDeployAccount needs a trust policy that allows PipelineAccount to assume it.

CodePipeline_Service_Role needs permission to assume DevOps_Role in CodeDeployAccount.

This is exactly what Option A describes.

---


<br/>

## Question 373

*Date: Aug. 7, 2025, 4:57 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has multiple development teams in separate business units that work in a single shared AWS account. All Amazon EC2 resources that users create in the account must include tags that specify which user created the resources. The tagging must occur within the first hour of resource creation.

A DevOps engineer needs to add tags to new resources that include the ID of the user that created the resource and the appropriate cost center ID. The DevOps engineer configures an AWS Lambda function to use the cost center mappings to tag the resources. The DevOps engineer also sets up AWS CloudTrail in the shared AWS account. An Amazon S3 bucket stores the CloudTrail event logs.

Which solution will meet the tagging requirements?

**Options:**
- A. Create an S3 event notification on the S3 bucket to invoke the Lambda function for s3:ObjectTagging:Put events. Enable bucket versioning on the S3 bucket.
- B. Enable server access logging on the S3 bucket. Create an S3 event notification on the S3 bucket for s3:ObjectTagging:* events.
- C. Enable AWS Config in the account. Configure the required-tags AWS managed rule to check and update the required tags.
- D. Create an Amazon EventBridge rule that uses Amazon EC2 as the event source. Configure the rule to match events that CloudTrail delivers. Configure the rule to target the Lambda function.

> **Suggested Answer:** D
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**ryuhei** (Thu 21 Aug 2025 02:35) - *Upvotes: 1*
Answer is D

---

**GiorgioGss** (Sun 17 Aug 2025 00:17) - *Upvotes: 2*
EventBridge can consume CloudTrail events directly for API actions like RunInstances.
Lambda can process the event immediately and add the required tags.
This ensures tagging occurs quickly after resource creation, meeting the 1-hour requirement.

---


<br/>

## Question 374

*Date: Aug. 7, 2025, 4:56 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is developing code and wants to use semantic versioning. The company's DevOps team needs to create a pipeline for compiling the code. The team also needs to manage versions of the compiled code. If the code uses any open source libraries, the libraries must also be cached in the build process.

Which solution will meet these requirements?

**Options:**
- A. Create an AWS CodeArtifact repository and associate the upstream repositories. Create an AWS CodeBuild project that builds the semantic version of the code artifacts. Configure the project to authenticate and connect to the CodeArtifact repository and publish the artifact to the repository.
- B. Use AWS CodeDeploy to upload the generated semantic version of the artifact to an Amazon Elastic File System (Amazon EFS) file system.
- C. Use an AWS CodeBuild project to build the code and to publish the generated semantic version of the artifact to AWS Artifact. Configure build caching in the CodeBuild project.
- D. Create a new AWS CodeArtifact repository. Create an AWS Lambda function that pulls open source packages from the internet and publishes the packages to the repository. Configure AWS CodeDeploy to build semantic versions of the code and publish the versions to the repository.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**ryuhei** (Thu 21 Aug 2025 02:38) - *Upvotes: 1*
Answer is A

---

**GiorgioGss** (Sun 17 Aug 2025 00:19) - *Upvotes: 2*
AWS CodeArtifact is a fully managed artifact repository that supports semantic versioning and can host both internal and open source dependencies.
Upstream repositories allow caching of open source libraries, reducing build times.
CodeBuild can compile code, version it, and publish artifacts to CodeArtifact.
This meets all requirements: semantic versioning, build pipeline, artifact version management, and caching of dependencies.

---


<br/>

## Question 375

*Date: Aug. 7, 2025, 4:56 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses a pipeline in AWS CodePipeline to deploy an application. The company created an AWS Fault Injection Service (AWS FIS) experiment template to test the resiliency of the application. A DevOps engineer needs to integrate the experiment into the pipeline.

Which solution will meet this requirement?

**Options:**
- A. Configure a new stage in the pipeline that includes an AWS FIS action. Configure the action to reference the AWS FIS experiment template. Grant the pipeline access to start the experiment.
- B. Create an Amazon EventBridge scheduler. Grant the scheduler permission to start the AWS FIS experiment. Configure a new stage in the pipeline that includes an action to invoke the EventBridge scheduler.
- C. Create an AWS Lambda function to start the AWS FIS experiment. Grant the Lambda function permission to start the experiment. Create a new stage in the pipeline that has a Lambda action. Set the action to invoke the Lambda function.
- D. Export the AWS FIS experiment template to an Amazon S3 bucket. Create an AWS CodeBuild unit test project that has a buildspec that starts the AWS FIS experiment. Grant the CodeBuild project access to start the experiment. Configure a new stage in the pipeline that includes an action to run the CodeBuild unit test project.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**ryuhei** (Thu 21 Aug 2025 02:41) - *Upvotes: 1*
Answer is C

---

**GiorgioGss** (Sun 17 Aug 2025 00:22) - *Upvotes: 2*
AWS CodePipeline does not have a native AWS FIS action, so you cannot directly reference an FIS experiment template in a pipeline stage.
The standard approach for integrating custom operations—such as starting an FIS experiment—in CodePipeline is to use an AWS Lambda function as an action.
The Lambda function can programmatically start the FIS experiment using the FIS API, and CodePipeline's Lambda action can invoke this function as a stage in the pipeline.

---


<br/>

## Question 376

*Date: Aug. 7, 2025, 4:57 p.m.
Disclaimers:
- ExamTopics website is not rel*

A DevOps engineer is creating a CI/CD pipeline to build container images. The engineer needs to store container images in Amazon Elastic Container Registry (Amazon ECR) and scan the images for common vulnerabilities. The CI/CD pipeline must be resilient to outages in upstream source container image repositories.

Which solution will meet these requirements?

**Options:**
- A. Create an ECR private repository in the private registry to store the container images and scan images when images are pushed to the repository. Configure a replication rule in the private registry to replicate images from upstream repositories.
- B. Create an ECR public repository in the public registry to cache images from upstream source repositories. Create an ECR private repository to store images. Configure the private repository to scan images when images are pushed to the repository.
- C. Create an ECR public repository in the public registry. Configure a pull through cache rule for the repository. Create an ECR private repository to store images. Configure the ECR private registry to perform basic scanning.
- D. Create an ECR private repository in the private registry to store the container images. Enable basic scanning for the private registry, and create a pull through cache rule.

> **Suggested Answer:** D
> **Community Vote:** D (67%), C (33%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**0ac7838** (Sun 16 Nov 2025 14:50) - *Upvotes: 1*
Keyword

pull-through cache rule

---

**ryuhei** (Thu 21 Aug 2025 07:54) - *Upvotes: 1*
Grok answer is C

---

**GiorgioGss** (Sun 17 Aug 2025 00:25) - *Upvotes: 1*
Be resilient to outages in upstream source container image repositories: This is the key requirement. The pull through cache rule is the specific ECR feature designed for this purpose. When you configure a pull through cache rule in your private registry for a public registry (e.g., Docker Hub), ECR will automatically cache images from that public registry.

---


<br/>

## Question 377

*Date: Aug. 7, 2025, 4:57 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is running its ecommerce website on AWS. The website is currently hosted on a single Amazon EC2 instance in one Availability Zone. A MySQL database runs on the same EC2 instance.

The company needs to eliminate single points of failure in the architecture to improve the website's availability and resilience.

Which solution will meet these requirements with the LEAST configuration changes to the website?

**Options:**
- A. Deploy the application by using AWS Fargate containers. Migrate the database to Amazon DynamoDB. Use Amazon API Gateway to route requests.
- B. Deploy the application on EC2 instances across multiple Availability Zones. Put the EC2 instances into an Auto Scaling group behind an Application Load Balancer. Migrate the database to Amazon Aurora Multi-AZ. Use Amazon CloudFront for content delivery.
- C. Use AWS Elastic Beanstalk to deploy the application across multiple AWS Regions. Migrate the database to Amazon Redshift. Use Amazon ElastiCache for session management.
- D. Migrate the application to AWS Lambda functions. Use Amazon S3 for static content hosting. Migrate the database to Amazon DocumentDB (with MongoDB compatibility).

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**ryuhei** (Thu 21 Aug 2025 07:58) - *Upvotes: 1*
Answer is B

---

**GiorgioGss** (Sun 17 Aug 2025 00:27) - *Upvotes: 2*
Keep the existing EC2-based app but run it across multiple AZs for high availability.
Add an Auto Scaling group + ALB → eliminates single point of failure on the web tier.
Migrate MySQL to Aurora Multi-AZ → provides resilience at the DB tier without major schema changes.
CloudFront improves performance and resilience for static content.
This is the closest to current architecture with minimal changes.

---


<br/>

## Question 378

*Date: Aug. 7, 2025, 4:57 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is developing a microservices-based application on AWS. The application consists of AWS Lambda functions and Amazon Elastic Container Service (Amazon ECS) services that need to be deployed frequently.

A DevOps engineer needs to implement a consistent deployment solution across all components of the application. The solution must automate the deployments, minimize downtime during updates, and manage configuration data for the application.

Which solution will meet these requirements with the LEAST development effort?

**Options:**
- A. Use AWS CloudFormation to define and provision the Lambda functions and ECS services. Implement stack updates with resource replacement for all components. Use AWS Secrets Manager to manage the configuration data.
- B. Use AWS CodeDeploy to manage deployments for the Lambda functions and ECS services. Implement canary deployments for the Lambda functions. Implement blue/green deployments for the ECS services. Use AWS Systems Manager Parameter Store to manage the configuration data.
- C. Use AWS Step Functions to orchestrate deployments for the Lambda functions and ECS services. Use canary deployments for the Lambda functions and ECS services in a different AWS Region. Use AWS Systems Manager Parameter Store to manage the configuration data.
- D. Use AWS Systems Manager to manage deployments for the Lambda functions and ECS services. Implement all-at-once deployments for the Lambda functions. Implement rolling updates for the ECS services. Use AWS Secrets Manager to manage the configuration data.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**ryuhei** (Thu 21 Aug 2025 08:12) - *Upvotes: 1*
Answer is B

---

**GiorgioGss** (Sun 17 Aug 2025 00:29) - *Upvotes: 3*
CodeDeploy supports both Lambda and ECS deployments natively.
Provides canary deployments for Lambda and blue/green deployments for ECS = minimal downtime.
Parameter Store is well suited for app configuration.
This is the AWS-native deployment service built for exactly this use case, with minimal dev effort.

---


<br/>

## Question 379

*Date: Aug. 7, 2025, 4:57 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is implementing a CI/CD pipeline for an application by using AWS CodePipeline and AWS CodeBuild. The company needs a solution to run unit tests and automatically generate code coverage reports before any code is deployed to production. The CI/CD pipeline execution must fail if the code coverage is less than 80%.

Which solution will meet these requirements?

**Options:**
- A. Create an AWS Lambda function to run unit tests and generate code coverage reports. Add a Lambda invoke action to a stage in the CodePipeline pipeline. Create an Amazon EventBridge scheduled rule to run hourly to monitor the Lambda function's output. Configure the rule to fail the pipeline if coverage is less than 80%.
- B. Create an AWS Step Functions workflow to run unit tests and generate code coverage reports. Add a Step Functions test action to a stage in the CodePipeline pipeline to invoke the workflow. Configure the workflow to fail if the code coverage is less than 80%.
- C. Create a CodeBuild project with a buildspec.yml file that includes commands to run unit tests and generate code coverage reports. Add a CodeBuild test action to a stage in the CodePipeline pipeline. Configure the CodeBuild test action to use the source artifacts from the source action as input. Modify the buildspec.yml file to fail the build if coverage is less than 80%.
- D. Create a CodeBuild project with Jenkins installed. Configure Jenkins to run unit tests and generate code coverage reports. Add a Jenkins test action to a stage in the CodePipeline pipeline. Configure the Jenkins test action to output the coverage report as an output artifact. Configure an approval action to fail the pipeline if code coverage is less than 80%.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**ryuhei** (Thu 21 Aug 2025 08:17) - *Upvotes: 1*
Answer is C

---

**GiorgioGss** (Sun 17 Aug 2025 00:32) - *Upvotes: 4*
CodeBuild is designed for running builds and tests.
You can configure buildspec.yml to run unit tests, generate coverage reports, and enforce a coverage threshold by failing the build if coverage < 80%.
CodePipeline will then fail at the CodeBuild test action stage → exactly what’s needed.

---


<br/>

## Question 380

*Date: Aug. 7, 2025, 4:57 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has a web application that publishes logs that contain metadata for transactions, with a status of success or failure for each log. The logs are in JSON format. The application publishes the logs to an Amazon CloudWatch Logs log group.

The company wants to create a dashboard that displays the number of successful transactions.

Which solution will meet this requirement with the LEAST operational overhead?

**Options:**
- A. Create an Amazon OpenSearch Service cluster and an OpenSearch Service subscription filter to send the log group data to the cluster. Create a dashboard within the Dashboards feature in the OpenSearch Service cluster by using a search query for transactions that have a status of success.
- B. Create a CloudWatch subscription filter for the log group that uses an AWS Lambda function. Configure the Lambda function to parse the JSON logs and publish a custom metric to CloudWatch for transactions that have a status of success. Create a CloudWatch dashboard by using a metric graph that displays the custom metric.
- C. Create a CloudWatch metric filter for the log groups with a filter pattern that matches the transaction status property and a value of success. Create a CloudWatch dashboard by using a metric graph that displays the new metric.
- D. Create an Amazon Kinesis data stream that is subscribed to the log group. Configure the data stream to filter incoming log data based on a status of success and to send the filtered logs to an AWS Lambda function. Configure the Lambda function to publish a custom metric to CloudWatch. Create a CloudWatch dashboard by using a metric graph that displays the custom metric.

> **Suggested Answer:** C
> **Community Vote:** C (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**GiorgioGss** (Sun 17 Aug 2025 00:34) - *Upvotes: 2*
CloudWatch supports metric filters on log groups.
Can directly parse JSON logs and count occurrences of "status":"success".
The metric is then usable in a CloudWatch dashboard.
Minimal overhead (no extra services, no custom code).

---


<br/>

## Question 381

*Date: Aug. 7, 2025, 4:57 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses Amazon API Gateway and AWS Lambda functions to implement an API. The company uses a pipeline in AWS CodePipeline to build and deploy the API. The pipeline contains a source stage, build stage, and deployment stage.

The company deploys the API without performing smoke tests. Soon after the deployment, the company observes multiple issues with the API. A security audit finds security vulnerabilities in the production code.

The company wants to prevent these issues from happening in the future.

Which combination of steps will meet this requirement? (Choose two.)

**Options:**
- A. Create a smoke test script that returns an error code if the API code fails the test. Add an action in the deployment stage to run the smoke test script after deployment. Configure the deployment stage for automatic rollback.
- B. Create a smoke test script that returns an error code if the API code fails the test. Add an action in the deployment stage to run the smoke test script after deployment. Configure the deployment stage to fail if the smoke test script returns an error code.
- C. Add an action in the build stage that uses Amazon Inspector to scan the Lambda function code after the code is built. Configure the build stage to fail if the scan returns any security findings.
- D. Add an action in the build stage to run an Amazon CodeGuru code scan after the code is built. Configure the build stage to fail if the scan returns any security findings.
- E. Add an action in the deployment stage to run an Amazon CodeGuru code scan after deployment. Configure the deployment stage to fail if the scan returns any security findings.

> **Suggested Answer:** BD
> **Community Vote:** AD (50%), BD (50%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**ryuhei** (Thu 21 Aug 2025 08:32) - *Upvotes: 2*
**A**: Adds smoke tests post-deployment with automatic rollback, ensuring production stability by reverting if tests fail. Addresses the requirement to prevent API issues.

- **C**: Uses Amazon Inspector in the build stage to scan for security vulnerabilities, failing the pipeline before deployment if issues are found. Prevents security issues in production.

- **Why B is not selected**: While B also adds smoke tests post-deployment and fails the pipeline if tests fail, it lacks explicit automatic rollback. This makes it less effective than A for protecting production, as manual intervention may be needed.

---

**ryuhei** (Thu 21 Aug 2025 08:33) - *Upvotes: 1*
I chose the wrong option. It's a and c.

---

**GiorgioGss** (Sun 17 Aug 2025 00:37) - *Upvotes: 2*
Ensures deployment fails immediately if smoke tests detect issues.
Prevents broken APIs from staying in production.
CodeGuru Reviewer analyzes code for security and quality issues before deployment.
Failing the build if findings exist prevents vulnerable code from reaching production.

---


<br/>

## Question 382

*Date: Aug. 7, 2025, 4:57 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is implementing a standardized security baseline across its AWS accounts. The accounts are in an organization in AWS Organizations.

The company must deploy consistent IAM roles and policies across all existing and future accounts in the organization.

Which solution will meet these requirements with the MOST operational efficiency?

**Options:**
- A. Enable AWS Control Tower in the management account. Configure AWS Control Tower Account Factory customization to deploy the required IAM roles and policies to all accounts.
- B. Activate trusted access for AWS CloudFormation StackSets in Organizations. In the management account, create a stack set that has service-managed permissions to deploy the required IAM roles and policies to all accounts. Enable automatic deployment for the stack set.
- C. In each member account, create IAM roles that have permissions to create and manage resources. In the management account, create an AWS CloudFormation stack set that has self-managed permissions to deploy the required IAM roles and policies to all accounts. Enable automatic deployment for the stack set.
- D. In the management account, create an AWS CodePipeline pipeline. Configure the pipeline to use AWS CloudFormation to automate the deployment of the required IAM roles and policies. Set up cross-account IAM roles to allow CodePipeline to deploy resources in the member accounts.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**ryuhei** (Thu 21 Aug 2025 08:38) - *Upvotes: 1*
Answer is B

---

**GiorgioGss** (Sun 17 Aug 2025 00:38) - *Upvotes: 2*
StackSets with service-managed permissions integrate directly with AWS Organizations.
IAM roles and policies are deployed to all existing accounts and future accounts automatically.
No need to manage cross-account roles manually.
This is the most efficient solution.

---


<br/>

## Question 383

*Date: Sept. 21, 2025, 1:57 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company is migrating its web application to AWS. The application uses WebSocket connections for real-time updates and requires sticky sessions.

A DevOps engineer must implement a highly available architecture for the application. The application must be accessible to users worldwide with the least possible latency.

Which solution will meet these requirements with the LEAST operational overhead?

**Options:**
- A. Deploy an Application Load Balancer (ALB). Deploy another ALB in a different AWS Region. Enable cross-zone load balancing and sticky sessions on the ALBs. Integrate the ALBs with Amazon Route 53 latency-based routing.
- B. Deploy a Network Load Balancer (NLB). Deploy another NLB in a different AWS Region. Enable cross-zone load balancing and sticky sessions on the NLBs. Integrate the NLBs with Amazon Route 53 geolocation routing.
- C. Deploy a Network Load Balancer (NLB) with cross-zone load balancing enabled. Configure the NLB with IP-based targets in multiple Availability Zones. Use Amazon CloudFront for global content delivery. Implement sticky sessions by using source IP address preservation on the NLB.
- D. Deploy an Application Load Balancer (ALB) for HTTP traffic. Deploy a Network Load Balancer (NLB) in each of the company’s AWS Regions for WebSocket connections. Enable sticky sessions on the ALB. Configure the ALB to forward requests to the NLB.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**TuMMM** (Sun 21 Sep 2025 13:57) - *Upvotes: 1*
ALB natively supports WebSocket connections and sticky sessions (with cookie-based session affinity).

Cross-zone load balancing ensures even distribution across Availability Zones.

Route 53 latency-based routing directs users to the closest AWS Region for minimal latency.

---


<br/>

## Question 384

*Date: Sept. 21, 2025, 1:59 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has a workflow that generates a file for each of the company's products and stores the files in a production environment Amazon S3 bucket. The company's users can access the S3 bucket.

Each file contains a product ID. Product IDs for products that have not been publicly announced are prefixed with a specific UUID. Product IDs are 12 characters long. IDs for products that have not been publicly announces begin with the letter P.

The company does not want information about products that have not been publicly announced to be available in the production environment S3 bucket.

Which solution will meet these requirements?

**Options:**
- A. Create a new staging S3 bucket. Generate all files in the new staging bucket. Create an Amazon Macie custom data identifier to identify product IDs in the new bucket that begin with the specific UUID. Launch an Amazon Macie sensitive data discovery job with the custom data identifier. Copy all files that do not have a Macie finding to the production S3 bucket.
- B. Create an Amazon Macie custom data identifier to identify product IDs in the production bucket that begin with the specific UUID. Launch an Amazon Macie sensitive data discovery job with the custom data identifier. Remove all files that have a Macie finding from the production S3 bucket.
- C. Create a new staging S3 bucket. Generate all files in the new staging bucket. Launch an Amazon Macie sensitive data discovery job with a managed data identifier. Copy all files that do not have a Macie finding to the production S3 bucket.
- D. Create an Amazon Macie sensitive data discovery job with a managed data identifier. Remove all files that have a Macie finding from the production S3 bucket.

> **Suggested Answer:** A
> **Community Vote:** A (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**TuMMM** (Sun 21 Sep 2025 13:59) - *Upvotes: 1*
This proactively prevents sensitive files from reaching production by filtering them in the staging environment.

The custom data identifier can be tailored to match the UUID prefix and the 'P' start.

This is a preventive approach with no sensitive data ever reaching production.

---


<br/>

## Question 385

*Date: Sept. 21, 2025, 2:01 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company uses Amazon RDS for Microsoft SQL Server as its primary database for applications. The company needs to ensure high availability within and across AWS Regions.

An Amazon Route 53 CNAME record is configured for the database endpoint. The applications connect to the database endpoint. The company must redirect application traffic to a standby database during a failover event. The company must maintain an RPO of less than 1 minute and an RTO of less than 10 minutes.

Which solution will meet these requirements?

**Options:**
- A. Deploy an Amazon RDS for SQL Server Multi-AZ DB cluster deployment that uses cross-Region read replicas. Use automation to promote the read replica to a standalone instance and to update the Route 53 record.
- B. Deploy an Amazon RDS for SQL Server Multi-AZ DB cluster deployment. Set up automated snapshots to be copied to another Region every 5 minutes. Use AWS Lambda to restore the latest snapshot in the secondary Region during failover.
- C. Deploy an Amazon RDS for SQL Server Single-AZ DB instance. Use AWS Database Migration Service (AWS DMS) to replicate data continuously to an RDS DB instance in another Region. Use Amazon CloudWatch alarms to notify the company about failover events.
- D. Deploy an Amazon RDS for SQL Server Single-AZ DB instance. Configure AWS Backup to create cross-Region backups every 30 seconds. Use automation to restore the latest backup and to update the Route 53 record during failover.

> **Suggested Answer:** A
> **Community Vote:** A (67%), C (33%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**9a57a7b** (Mon 13 Oct 2025 22:40) - *Upvotes: 1*
Fact that A is the right answer

---

**zmjpor** (Tue 23 Sep 2025 08:35) - *Upvotes: 1*
Obviously A, Multi-AZ already eliminated C and D

---

**TuMMM** (Sun 21 Sep 2025 14:01) - *Upvotes: 1*
DMS provides continuous cross-Region replication (low RPO).

With automation, you can failover and update Route 53 within the RTO.

However, to achieve intra-Region high availability, you should deploy Multi-AZ instead of Single-AZ (but the option says Single-AZ).

---


<br/>

## Question 386

*Date: Sept. 21, 2025, 2:03 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company wants to build a pipeline to update the standard AMI monthly. The AMI must be updated to use the most recent patches to ensure that launched Amazon EC2 instances are up to date. Each new AMI must be available to all AWS accounts in the company's organization in AWS Organizations.

The company needs to configure an automated pipeline to build the AMI.

Which solution will meet these requirements with the MOST operational efficiency?

**Options:**
- A. Create an AWS CodePipeline pipeline that uses AWS CodeBuild. Create an AWS Lambda function to run the pipeline every month. Create an AWS CloudFormation template. Share the template with all AWS accounts in the organization.
- B. Create an AMI pipeline by using EC2 Image Builder. Configure the pipeline to distribute the AMI to the AWS accounts in the organization. Configure the pipeline to run monthly.
- C. Create an AWS CodePipeline pipeline that runs an AWS Lambda function to build the AMI. Configure the pipeline to share the AMI with the AWS accounts in the organization. Configure Amazon EventBridge Scheduler to invoke the pipeline every month.
- D. Create an AWS Systems Manager Automation runbook. Configure the automation to run in all AWS accounts in the organization. Create an AWS Lambda function to run the automation every month.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**TuMMM** (Sun 21 Sep 2025 14:03) - *Upvotes: 1*
EC2 Image Builder is purpose-built for creating, patching, testing, and distributing AMIs.

It integrates with AWS Organizations to share AMIs across accounts easily.

It can be scheduled to run automatically (e.g., monthly).

It requires the least operational effort and is fully managed.

---


<br/>

## Question 387

*Date: Sept. 21, 2025, 2:04 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company has an application that uses an Amazon API Gateway REST API, AWS Lambda functions, and an Amazon DynamoDB table. The application currently runs in a single AWS Region. The company wants to make the application highly available across two Regions. User traffic must be routed to the Region that provides the least latency.

Which combination of steps will meet these requirements? (Choose three.)

**Options:**
- A. Create a replica of the DynamoDB table in a second Region.
- B. Create a global secondary index for the DynamoDB table.
- C. Create copies of the REST API and the Lambda functions in a second Region.
- D. Create health checks in Amazon Route 53. Create DNS records that include a failover routing policy.
- E. Create health checks in Amazon Route 53. Create DNS records that include a latency routing policy.
- F. Create DNS records in Amazon Route 53 that include a multivalue answer routing policy.

> **Suggested Answer:** ACE
> **Community Vote:** ACE (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**TuMMM** (Sun 21 Sep 2025 14:04) - *Upvotes: 1*
A: Replicate DynamoDB to the second Region using global tables.

C: Deploy API Gateway and Lambda in the second Region.

E: Use Route 53 latency routing with health checks to direct traffic to the lowest-latency Region.

---


<br/>

## Question 388

*Date: Sept. 19, 2025, 8:37 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company has a web application that is hosted on Amazon EC2 instances. The company is deploying the application into multiple AWS Regions.

The application consists of dynamic content such as WebSocket-based real-time product updates. The company uses Amazon Route 53 to manage all DNS records.

Which solution will provide multi-Region access to the application with the LEAST latency?

**Options:**
- A. Deploy an Application Load Balancer (ALB) in front of the EC2 instances in each Region. Create a Route 53 A record with a latency-based routing policy. Add IP addresses of the ALBs as the value of the record.
- B. Deploy an Application Load Balancer (ALB) in front of the EC2 instances in each Region. Deploy an Amazon CloudFront distribution with an origin group that contains the ALBs as origins. Create a Route 53 alias record that points to the CloudFront distribution's DNS address.
- C. Deploy a Network Load Balancer (NLB) in front of the EC2 instances in each Region. Create a Route 53 A record with a multivalue answer routing policy. Add IP addresses of the NLBs as the value of the record.
- D. Deploy a Network Load Balancer (NLB) in front of the EC2 instances in each Region. Deploy an AWS Global Accelerator standard accelerator with an endpoint group for each NLB. Create a Route 53 alias record that points to the accelerator's DNS address.

> **Suggested Answer:** A
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**TuMMM** (Sun 21 Sep 2025 14:05) - *Upvotes: 1*
Global Accelerator optimizes the network path for traffic, reducing latency for real-time applications.

It works seamlessly with NLBs (which are suitable for TCP traffic like WebSockets).

Route 53 alias record points to the accelerator's DNS, providing a stable endpoint.

---

**figub** (Fri 19 Sep 2025 08:37) - *Upvotes: 1*
AWS Global Accelerator

---


<br/>

## Question 389

*Date: Sept. 21, 2025, 2:06 p.m.
Disclaimers:
- ExamTopics website is not rel*

A company manages its multi-account environment by using AWS Organizations and AWS Control Tower. The company must deploy standardized security controls and compliance policies across all of its AWS accounts and AWS Regions. Any changes to these controls must be automatically applied to all accounts simultaneously.

The company has the required security controls and compliance policies defined in AWS Cloud Development Kit (AWS CDK) as a security controls construct.

Which solution will deploy these controls across all accounts and Regions with the LEAST operational overhead?

**Options:**
- A. Create an AWS CDK app that includes an AWS CloudFormation StackSets construct. Configure the StackSets construct to use the security controls construct as its template. Specify the target accounts and Regions. Create automation to deploy the CDK app to create and manage the CloudFormation stack set.
- B. Create an AWS CDK app that synthesizes an AWS CloudFormation template from the security controls construct. Use Amazon EventBridge to invoke an AWS Lambda function to update a CloudFormation stack set when changes are made to the security controls construct.
- C. Convert the security controls construct to an AWS CloudFormation macro. Create a CloudFormation stack set that references the macro and deploys the macro to all target accounts. Use Organizations to automatically add new accounts to the stack set’s list of target accounts.
- D. Use AWS Control Tower to create a customized landing zone that includes configurations from the security controls construct. Configure AWS Control Tower to automatically enroll new accounts and to apply the landing zone template.

> **Suggested Answer:** A
> **Community Vote:** D (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**TuMMM** (Sun 21 Sep 2025 14:06) - *Upvotes: 1*
Control Tower is purpose-built for standardizing security and compliance across organizations.

It automatically enrolls new accounts and applies policies.

Customizations can be deployed via CFCT, which integrates with CDK constructs.

This requires minimal ongoing management.

---


<br/>

## Question 390

*Date: Sept. 23, 2025, 2:44 a.m.
Disclaimers:
- ExamTopics website is not rel*

A company needs to manage shared libraries for various projects across its development AWS account and production AWS account. The company has configured IAM roles for developers and has defined an AWS CodePipeline pipeline by using the AWS Cloud Development Kit (AWS CDK).

A DevOps engineer must implement a solution to ensure that only developers can access the latest versions of the libraries. The solution must test shared packages independently before the shared packages are consumed by other applications and before they go to production.

Which solution will meet these requirements?

**Options:**
- A. Create a single AWS CodeArtifact repository for development and production in a central account. Use IAM policies for the developer roles to allow only developers to access the shared libraries. Create an Amazon EventBridge role to start an AWS CodeBuild project and to test each package before the package is copied to the production repository.
- B. Create an AWS CodeArtifact repository in the development account. Create another CodeArtifact repository in the production account. For the development repository, add a repository policy that allows only developers to access the shared libraries. Create an Amazon EventBridge rule to start the CodePipeline pipeline and to test each package before the package is copied to the production repository.
- C. Create a single Amazon S3 bucket with versioning enabled for development and production in a central account. Use IAM policies for the developer roles to allow only the developers to access the shared libraries. Create an Amazon EventBridge rule to start an AWS CodeBuild project and to test each package before the package is copied to production.
- D. Create an Amazon S3 bucket with versioning enabled in the development account. Create another S3 bucket with versioning enabled in the production account. For the development S3 bucket, add a bucket policy that allows only developers to access the shared libraries. Create an Amazon EventBridge role to start the CodePipeline pipeline. Configure the role to test each package when the package is copied to production and to revert the changes if the tests fail.

> **Suggested Answer:** B
> **Community Vote:** B (100%), C (25%), B (20%), Other, A (35%), C (25%), B (20%), Other

### Discussions

**Changwha** (Sun 23 Nov 2025 10:52) - *Upvotes: 1*
The best practice for shared library workflows in AWS is to use separate AWS CodeArtifact repositories for development and production.

---

**0ac7838** (Sun 16 Nov 2025 15:47) - *Upvotes: 1*
Why the other options are worse:

A: Proposes a single CodeArtifact repo for dev and prod in a central account. That contradicts the promotion requirement (dev vs prod) and makes enforcing “only developers access latest dev versions” and clean promotion harder.

C / D (S3-based): S3 is not a native package registry. You’d lose package-management features (version semantics, dependency resolution, repository policies) that CodeArtifact provides. That increases operational complexity and risk for package consumption by applications.

---


<br/>

